{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install ü§ó Transformers and ü§ó Datasets. Uncomment the following cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MOsHUjgdIrIW",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:09.436478Z",
     "start_time": "2024-04-10T09:22:06.253438Z"
    }
   },
   "source": "! pip install datasets transformers seqeval",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (2.17.1)\r\n",
      "Requirement already satisfied: transformers in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (4.38.2)\r\n",
      "Requirement already satisfied: seqeval in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (1.2.2)\r\n",
      "Requirement already satisfied: filelock in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from datasets) (3.13.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from datasets) (1.26.4)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from datasets) (15.0.0)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from datasets) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from datasets) (2.2.1)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from datasets) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from datasets) (4.66.2)\r\n",
      "Requirement already satisfied: xxhash in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from datasets) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\r\n",
      "Requirement already satisfied: aiohttp in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from datasets) (3.9.3)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from datasets) (0.20.3)\r\n",
      "Requirement already satisfied: packaging in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from datasets) (23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from datasets) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from transformers) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from transformers) (0.4.2)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from seqeval) (1.4.1.post1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from aiohttp->datasets) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval) (1.12.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval) (3.3.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\r\n"
     ]
    }
   ],
   "execution_count": 249
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQ9oei3PYoUi"
   },
   "source": [
    "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
    "\n",
    "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
    "\n",
    "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your username and password:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zNZJzDgFYoUj",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:09.742491Z",
     "start_time": "2024-04-10T09:22:09.438623Z"
    }
   },
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"your_token\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/nikolaystepanov/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "execution_count": 250
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0kWpldrYoUj"
   },
   "source": [
    "Then you need to install Git-LFS. Uncomment the following instructions:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UogHw1uOYoUj",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:09.755987Z",
     "start_time": "2024-04-10T09:22:09.746984Z"
    }
   },
   "source": [
    "# !apt install git-lfs"
   ],
   "outputs": [],
   "execution_count": 251
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBhatPuGYoUk"
   },
   "source": [
    "Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "48_bU0zVYoUk",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:09.767499Z",
     "start_time": "2024-04-10T09:22:09.762223Z"
    }
   },
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.38.2\n"
     ]
    }
   ],
   "execution_count": 252
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFASsisvIrIb"
   },
   "source": [
    "You can find a script version of this notebook to fine-tune your model in a distributed fashion using multiple GPUs or TPUs [here](https://github.com/huggingface/transformers/tree/master/examples/token-classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0IkEJXkYoUk"
   },
   "source": [
    "We also quickly upload some telemetry - this tells us which examples and software versions are getting used so we know where to prioritize our maintenance efforts. We don't collect (or care about) any personally identifiable information, but if you'd prefer not to be counted, feel free to skip this step or delete this cell entirely."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NO1ubRh8YoUl",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:09.776456Z",
     "start_time": "2024-04-10T09:22:09.768710Z"
    }
   },
   "source": [
    "from transformers.utils import send_example_telemetry\n",
    "\n",
    "send_example_telemetry(\"token_classification_notebook\", framework=\"pytorch\")"
   ],
   "outputs": [],
   "execution_count": 253
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning a model on a token classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcib0_rLYoUl"
   },
   "source": [
    "In this notebook, we will see how to fine-tune one of the [ü§ó Transformers](https://github.com/huggingface/transformers) model to a token classification task, which is the task of predicting a label for each token.\n",
    "\n",
    "![Widget inference representing the NER task](https://github.com/huggingface/notebooks/blob/main/examples/images/token_classification.png?raw=1)\n",
    "\n",
    "The most common token classification tasks are:\n",
    "\n",
    "- NER (Named-entity recognition) Classify the entities in the text (person, organization, location...).\n",
    "- POS (Part-of-speech tagging) Grammatically classify the tokens (noun, verb, adjective...)\n",
    "- Chunk (Chunking) Grammatically classify the tokens and group them into \"chunks\" that go together\n",
    "\n",
    "We will see how to easily load a dataset for these kinds of tasks and use the `Trainer` API to fine-tune a model on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "This notebook is built to run on any token classification task, with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a version with a token classification head and a fast tokenizer (check on [this table](https://huggingface.co/transformers/index.html#bigtable) if this is the case). It might just need some small adjustments if you decide to use a different dataset than the one used here. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zVvslsfMIrIh",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:09.784073Z",
     "start_time": "2024-04-10T09:22:09.779240Z"
    }
   },
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"ai-forever/ruBert-base\"\n",
    "batch_size = 16"
   ],
   "outputs": [],
   "execution_count": 254
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "We will use the [ü§ó Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`.  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IreSlFmlIrIm",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:09.791762Z",
     "start_time": "2024-04-10T09:22:09.787607Z"
    }
   },
   "source": [
    "from datasets import load_dataset, load_metric"
   ],
   "outputs": [],
   "execution_count": 255
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKx2zKs5IrIq"
   },
   "source": [
    "For our example here, we'll use the [CONLL 2003 dataset](https://www.aclweb.org/anthology/W03-0419.pdf). The notebook should work with any token classification dataset provided by the ü§ó Datasets library. If you're using your own dataset defined from a JSON or csv file (see the [Datasets documentation](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files) on how to load them), it might need some adjustments in the names of the columns used."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270,
     "referenced_widgets": [
      "69caab03d6264fef9fc5649bffff5e20",
      "3f74532faa86412293d90d3952f38c4a",
      "50615aa59c7247c4804ca5cbc7945bd7",
      "fe962391292a413ca55dc932c4279fa7",
      "299f4b4c07654e53a25f8192bd1d7bbd",
      "ad04ed1038154081bbb0c1444784dcc2",
      "7c667ad22b5740d5a6319f1b1e3a8097",
      "46c2b043c0f84806978784a45a4e203b",
      "80e2943be35f46eeb24c8ab13faa6578",
      "de5956b5008d4fdba807bae57509c393",
      "931db1f7a42f4b46b7ff8c2e1262b994",
      "6c1db72efff5476e842c1386fadbbdba",
      "ccd2f37647c547abb4c719b75a26f2de",
      "d30a66df5c0145e79693e09789d96b81",
      "5fa26fc336274073abbd1d550542ee33",
      "2b34de08115d49d285def9269a53f484",
      "d426be871b424affb455aeb7db5e822e",
      "160bf88485f44f5cb6eaeecba5e0901f",
      "745c0d47d672477b9bb0dae77b926364",
      "d22ab78269cd4ccfbcf70c707057c31b",
      "d298eb19eeff453cba51c2804629d3f4",
      "a7204ade36314c86907c562e0a2158b8",
      "e35d42b2d352498ca3fc8530393786b2",
      "75103f83538d44abada79b51a1cec09e",
      "f6253931d90543e9b5fd0bb2d615f73a",
      "051aa783ff9e47e28d1f9584043815f5",
      "0984b2a14115454bbb009df71c1cf36f",
      "8ab9dfce29854049912178941ef1b289",
      "c9de740e007141958545e269372780a4",
      "cbea68b25d6d4ba09b2ce0f27b1726d5",
      "5781fc45cf8d486cb06ed68853b2c644",
      "d2a92143a08a4951b55bab9bc0a6d0d3",
      "a14c3e40e5254d61ba146f6ec88eae25",
      "c4ffe6f624ce4e978a0d9b864544941a",
      "1aca01c1d8c940dfadd3e7144bb35718",
      "9fbbaae50e6743f2aa19342152398186",
      "fea27ca6c9504fc896181bc1ff5730e5",
      "940d00556cb849b3a689d56e274041c2",
      "5cdf9ed939fb42d4bf77301c80b8afca",
      "94b39ccfef0b4b08bf2fb61bb0a657c1",
      "9a55087c85b74ea08b3e952ac1d73cbe",
      "2361ab124daf47cc885ff61f2899b2af",
      "1a65887eb37747ddb75dc4a40f7285f2",
      "3c946e2260704e6c98593136bd32d921",
      "50d325cdb9844f62a9ecc98e768cb5af",
      "aa781f0cfe454e9da5b53b93e9baabd8",
      "6bb68d3887ef43809eb23feb467f9723",
      "7e29a8b952cf4f4ea42833c8bf55342f",
      "dd5997d01d8947e4b1c211433969b89b",
      "2ace4dc78e2f4f1492a181bcd63304e7",
      "bbee008c2791443d8610371d1f16b62b",
      "31b1c8a2e3334b72b45b083688c1a20c",
      "7fb7c36adc624f7dbbcb4a831c1e4f63",
      "0b7c8f1939074794b3d9221244b1344d",
      "a71908883b064e1fbdddb547a8c41743",
      "2f5223f26c8541fc87e91d2205c39995"
     ]
    },
    "id": "s_AY1ATSIrIq",
    "outputId": "fd0578d1-8895-443d-b56f-5908de9f1b6b",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:12.238152Z",
     "start_time": "2024-04-10T09:22:09.794070Z"
    }
   },
   "source": "datasets = load_dataset(\"iluvvatar/NEREL\", trust_remote_code=True)",
   "outputs": [],
   "execution_count": 256
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "The `datasets` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GWiVUF0jIrIv",
    "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:12.252429Z",
     "start_time": "2024-04-10T09:22:12.243438Z"
    }
   },
   "source": [
    "datasets"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'entities', 'relations', 'links'],\n",
       "        num_rows: 746\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'entities', 'relations', 'links'],\n",
       "        num_rows: 93\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'text', 'entities', 'relations', 'links'],\n",
       "        num_rows: 94\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 257
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lzzOQLDYoUm"
   },
   "source": [
    "We can see the training, validation and test sets all have a column for the tokens (the input texts split into words) and one column of labels for each kind of task we introduced before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:12.265278Z",
     "start_time": "2024-04-10T09:22:12.254267Z"
    }
   },
   "source": [
    "datasets[\"train\"][0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'text': '–ü—É–ª–µ–º–µ—Ç—ã, –∞–≤—Ç–æ–º–∞—Ç—ã –∏ —Å–Ω–∞–π–ø–µ—Ä—Å–∫–∏–µ –≤–∏–Ω—Ç–æ–≤–∫–∏ –∏–∑—ä—è—Ç—ã –≤ –∞—Ä–µ–Ω–¥—É–µ–º–æ–º –∞–º–µ—Ä–∏–∫–∞–Ω—Ü–∞–º–∏ –¥–æ–º–µ –≤ –ë–∏—à–∫–µ–∫–µ\\n\\n05/08/2008 10:35\\n\\n–ë–ò–®–ö–ï–ö, 5 –∞–≤–≥—É—Å—Ç–∞ /–ù–æ–≤–æ—Å—Ç–∏-–ì—Ä—É–∑–∏—è/. –ü—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã –ö–∏—Ä–≥–∏–∑–∏–∏ –æ–±–Ω–∞—Ä—É–∂–∏–ª–∏ –≤ –¥–æ–º–µ, –∞—Ä–µ–Ω–¥—É–µ–º–æ–º –≥—Ä–∞–∂–¥–∞–Ω–∞–º–∏ –°–®–ê –≤ –ë–∏—à–∫–µ–∫–µ, –ø—É–ª–µ–º–µ—Ç—ã, –∞–≤—Ç–æ–º–∞—Ç—ã –∏ —Å–Ω–∞–π–ø–µ—Ä—Å–∫–∏–µ –≤–∏–Ω—Ç–æ–≤–∫–∏, —Å–æ–æ–±—â–∞–µ—Ç –≤–æ –≤—Ç–æ—Ä–Ω–∏–∫ –ø—Ä–µ—Å—Å-—Å–ª—É–∂–±–∞ –ú–í–î –ö–∏—Ä–≥–∏–∑–∏–∏.\\n\\n\"–í —Ö–æ–¥–µ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ-–ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –ø–æ–¥ –∫–æ–¥–æ–≤—ã–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º \"–ê—Ä—Å–µ–Ω–∞–ª\" –≤ –Ω–æ–≤–æ—Å—Ç—Ä–æ–π–∫–µ –´–Ω—Ç—ã–º–∞–∫, –≤ –¥–æ–º–µ, –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—â–µ–º 66-–ª–µ—Ç–Ω–µ–º—É –≥—Ä–∞–∂–¥–∞–Ω–∏–Ω—É –ö–∏—Ä–≥–∏–∑–∏–∏ –∏ –∞—Ä–µ–Ω–¥—É–µ–º–æ–º –≥—Ä–∞–∂–¥–∞–Ω–∞–º–∏ –°–®–ê, –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏ –∏–∑—ä—è—Ç—ã: —à–µ—Å—Ç—å –∫—Ä—É–ø–Ω–æ–∫–∞–ª–∏–±–µ—Ä–Ω—ã—Ö –ø—É–ª–µ–º–µ—Ç–æ–≤ —Å –æ–ø—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–∏—Ü–µ–ª–æ–º –∏ —Å –ø—Ä–∏–±–æ—Ä–∞–º–∏ –Ω–æ—á–Ω–æ–≥–æ –≤–∏–¥–µ–Ω–∏—è, 26 –∞–≤—Ç–æ–º–∞—Ç–æ–≤ –∫–∞–ª–∏–±—Ä–∞ 5,56 –º–∏–ª–ª–∏–º–µ—Ç—Ä–∞, –¥–≤–∞ –≤–∏–Ω—á–µ—Å—Ç–µ—Ä–∞ –º–∞—Ä–∫–∏ –ú–û–°–í–ï–ì–ê 12-–≥–æ –∫–∞–ª–∏–±—Ä–∞, —á–µ—Ç—ã—Ä–µ —Å—Ç–≤–æ–ª–∞ –æ—Ç –∫—Ä—É–ø–Ω–æ–∫–∞–ª–∏–±–µ—Ä–Ω–æ–≥–æ –ø—É–ª–µ–º–µ—Ç–∞, –¥–≤–∞ –ø–æ–¥—Å—Ç–≤–æ–ª—å–Ω—ã—Ö –≥—Ä–∞–Ω–∞—Ç–æ–º–µ—Ç–∞, —á–µ—Ç—ã—Ä–µ —Å–Ω–∞–π–ø–µ—Ä—Å–∫–∏–µ –≤–∏–Ω—Ç–æ–≤–∫–∏ —Å –æ–ø—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–∏—Ü–µ–ª–æ–º –∑–∞—â–∏—Ç–Ω–æ–≥–æ —Ü–≤–µ—Ç–∞, —à–µ—Å—Ç—å –ø–∏—Å—Ç–æ–ª–µ—Ç–æ–≤ –∫–∞–ª–∏–±—Ä–∞ 9 –º–∏–ª–ª–∏–º–µ—Ç—Ä–æ–≤ –º–∞—Ä–∫–∏ –ë–µ—Ä–µ—Ç—Ç–∞, –æ–¥–Ω–∞ –≤–∏–Ω—Ç–æ–≤–∫–∞\", - –≥–æ–≤–æ—Ä–∏—Ç—Å—è –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏ –ú–í–î.\\n\\n–ü—Ä–µ—Å—Å-—Å–ª—É–∂–±–∞ –æ—Ç–º–µ—á–∞–µ—Ç, —á—Ç–æ –Ω–∞ –º–æ–º–µ–Ω—Ç –æ–±—ã—Å–∫–∞ \"–≤ –¥–æ–º–µ –Ω–∞—Ö–æ–¥–∏–ª–∏—Å—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ –ø–æ—Å–æ–ª—å—Å—Ç–≤–∞ –°–®–ê, –æ–±–ª–∞–¥–∞—é—â–∏—Ö –¥–∏–ø–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –∏–º–º—É–Ω–∏—Ç–µ—Ç–æ–º, –∏ 10 –≤–æ–µ–Ω–Ω–æ—Å–ª—É–∂–∞—â–∏—Ö, —è–∫–æ–±—ã –ø—Ä–∏–±—ã–≤—à–∏—Ö –∏–∑ –°–®–ê –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —Ç—Ä–µ–Ω–∏–Ω–≥–∞ —Å —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º–∏ —Å–ø–µ—Ü–ø–æ–¥—Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –æ–¥–Ω–æ–π –∏–∑ —Å–∏–ª–æ–≤—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä —Ä–µ—Å–ø—É–±–ª–∏–∫–∏, –ª–∏—á–Ω–æ—Å—Ç–∏ –∫–æ—Ç–æ—Ä—ã—Ö –≤ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è\".\\n\\n–°–æ–≥–ª–∞—Å–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏—é, –≤ –¥–æ–º–µ –±—ã–ª–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–æ–µ–ø—Ä–∏–ø–∞—Å–æ–≤. \"–î–≤–∞ –Ω–æ–∂–∞, 2920 —à—Ç—É–∫ –ø–∞—Ç—Ä–æ–Ω–æ–≤ –∫–∞–ª–∏–±—Ä–∞ 5,56 –º–∏–ª–ª–∏–º–µ—Ç—Ä–∞, 10556 —à—Ç—É–∫ –ø–∞—Ç—Ä–æ–Ω–æ–≤ –∫–∞–ª–∏–±—Ä–∞ 9 –º–∏–ª–ª–∏–º–µ—Ç—Ä–æ–≤, –¥–≤–∞ —è—â–∏–∫–∞ –ø–∞—Ç—Ä–æ–Ω–æ–≤ –∫–∞–ª–∏–±—Ä–∞ 50 –º–∏–ª–ª–∏–º–µ—Ç—Ä–æ–≤, –≤ –∫–∞–∂–¥–æ–º 350 —à—Ç—É–∫, –ø–∞—Ç—Ä–æ–Ω—ã –∫–∞–ª–∏–±—Ä–∞ 12 –º–∏–ª–ª–∏–º–µ—Ç—Ä–æ–≤ –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ 478 —à—Ç—É–∫, –º–∞—Ä–∫–∏—Ä–æ–≤–æ—á–Ω—ã–µ (—Ç—Ä–∞—Å—Å–∏—Ä—É—é—â–∏–µ) –ø–∞—Ç—Ä–æ–Ω—ã (–∫—Ä–∞—Å–Ω–æ–≥–æ —Ü–≤–µ—Ç–∞) 1000 —à—Ç—É–∫, 66 —à—Ç—É–∫ –ø—É—Å—Ç—ã—Ö –º–∞–≥–∞–∑–∏–Ω–æ–≤ –æ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ—Ä—É–∂–∏—è, 57 —à—Ç—É–∫ –ø—É—Å—Ç—ã—Ö –º–∞–≥–∞–∑–∏–Ω–æ–≤ –æ—Ç –ø–∏—Å—Ç–æ–ª–µ—Ç–∞ –ë–µ—Ä–µ—Ç—Ç–∞\", - –≥–æ–≤–æ—Ä–∏—Ç—Å—è –≤ –ø—Ä–µ—Å—Å-—Ä–µ–ª–∏–∑–µ.\\n\\n–ü—Ä–µ—Å—Å-—Å–ª—É–∂–±–∞ –ú–í–î —Å–æ–æ–±—â–∏–ª–∞, —á—Ç–æ —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ –¥–∞–Ω–Ω–æ–º—É —Ñ–∞–∫—Ç—É –ø—Ä–æ–≤–æ–¥–∏—Ç –ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞ –ë–∏—à–∫–µ–∫–∞. –°–µ–π—á–∞—Å –≤—ã—è—Å–Ω—è–µ—Ç—Å—è, –∫–æ–º—É –∏–º–µ–Ω–Ω–æ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –∏–∑—ä—è—Ç–æ–µ –æ—Ä—É–∂–∏–µ, –ø–µ—Ä–µ–¥–∞–µ—Ç –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏.\\n\\n–û—Ä—É–∂–∏–µ, –∏–∑—ä—è—Ç–æ–µ —É –≥—Ä–∞–∂–¥–∞–Ω –°–®–ê –ø—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –æ—Ä–≥–∞–Ω–∞–º–∏ –ö–∏—Ä–≥–∏–∑–∏–∏, –Ω–∞—Ö–æ–¥–∏–ª–æ—Å—å –≤ —Ä–µ—Å–ø—É–±–ª–∏–∫–µ —Å –≤–µ–¥–æ–º–∞ –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –ö–∏—Ä–≥–∏–∑–∏–∏, —Å–æ–æ–±—â–∏–ª –≤–æ –≤—Ç–æ—Ä–Ω–∏–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å –ø—Ä–µ—Å—Å-—Å–ª—É–∂–±—ã –ø–æ—Å–æ–ª—å—Å—Ç–≤–∞ –°–®–ê.\\n\\n\"–í—Å–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ –Ω–∞—Ö–æ–¥–∏–ª–æ—Å—å –Ω–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ –ö–∏—Ä–≥–∏–∑–∏–∏ —Å –≤–µ–¥–æ–º–∞ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∫–∏—Ä–≥–∏–∑—Å–∫–∏—Ö –≤–ª–∞—Å—Ç–µ–π\", - —Å–∫–∞–∑–∞–ª —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫ –∞–≥–µ–Ω—Ç—Å—Ç–≤–∞. –í–æ–µ–Ω–Ω–æ—Å–ª—É–∂–∞—â–∏–µ –∏ –æ—Ä—É–∂–∏–µ \"–ø—Ä–∏–±—ã–ª–∏ –≤ —Ä–µ—Å–ø—É–±–ª–∏–∫—É –ø–æ –ø—Ä–∏–≥–ª–∞—à–µ–Ω–∏—é –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ —Å —Ü–µ–ª—å—é –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∞–Ω—Ç–∏—Ç–µ—Ä—Ä–æ—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —É—á–µ–Ω–∏–π –¥–ª—è –º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤\", –∑–∞—è–≤–∏–ª–æ –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–µ –¥–∏–ø–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–µ–¥–æ–º—Å—Ç–≤–æ.\\n\\n\"–î–æ–º –∏ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ –Ω–∞—Ö–æ–¥–∏–ª–∏—Å—å –ø–æ–¥ –∑–∞—â–∏—Ç–æ–π –∫–∏—Ä–≥–∏–∑—Å–∫–∏—Ö –≤–ª–∞—Å—Ç–µ–π\", - –æ—Ç–º–µ—á–∞–µ—Ç –ø—Ä–µ—Å—Å-—Å–ª—É–∂–±–∞.\\n\\n–ü–æ—Å–æ–ª—å—Å—Ç–≤–æ –°–®–ê —Å—á–∏—Ç–∞–µ—Ç —Å–ª—É—á–∏–≤—à–µ–µ—Å—è \"–Ω–µ–ø—Ä–∏—è—Ç–Ω—ã–º –∏–Ω—Ü–∏–¥–µ–Ω—Ç–æ–º\" –∏ –≤—ã—Ä–∞–∂–∞–µ—Ç –Ω–∞–¥–µ–∂–¥—É —á—Ç–æ \"–°–®–ê –∏ –ö–∏—Ä–≥–∏–∑–∏—è –º–æ–≥–ª–∏ –±—ã –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å —É—Å–∏–ª–∏—è –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –∞–Ω—Ç–∏—Ç–µ—Ä—Ä–æ—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ö–∏—Ä–≥–∏–∑–∏–∏\".\\n\\n–ü—Ä–µ—Å—Å-—Å–ª—É–∂–±–∞ –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–π –≤–æ–µ–Ω–Ω–æ–π –±–∞–∑—ã —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–Ω–æ–π –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–º –∞—ç—Ä–æ–ø–æ—Ä—Ç—É \"–ú–∞–Ω–∞—Å\" —Å—Ç–æ–ª–∏—Ü—ã –ö–∏—Ä–≥–∏–∑–∏–∏ –æ—Ç–∫–∞–∑–∞–ª–∞—Å—å –∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–π –∏–Ω—Ü–∏–¥–µ–Ω—Ç —Å —É—á–∞—Å—Ç–∏–µ–º –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö –≤–æ–µ–Ω–Ω—ã—Ö.\\n\\n\"–í—Å–µ–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–º–∏ —Å –¥–∞–Ω–Ω—ã–º —Å–ª—É—á–∞–µ–º, –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è –ø–æ—Å–æ–ª—å—Å—Ç–≤–æ –°–®–ê\", - —Å–æ–æ–±—â–∏–ª–∏ –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏ –≤ –ø—Ä–µ—Å—Å-—Å–ª—É–∂–±–µ –±–∞–∑—ã.',\n",
       " 'entities': ['T1\\tNATIONALITY 62 74\\t–∞–º–µ—Ä–∏–∫–∞–Ω—Ü–∞–º–∏',\n",
       "  'T2\\tCITY 82 89\\t–ë–∏—à–∫–µ–∫–µ',\n",
       "  'T3\\tDATE 117 126\\t5 –∞–≤–≥—É—Å—Ç–∞',\n",
       "  'T4\\tCOUNTRY 136 142\\t–ì—Ä—É–∑–∏—è',\n",
       "  'T5\\tORGANIZATION 145 179\\t–ü—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã –ö–∏—Ä–≥–∏–∑–∏–∏',\n",
       "  'T6\\tCOUNTRY 171 179\\t–ö–∏—Ä–≥–∏–∑–∏–∏',\n",
       "  'T7\\tCOUNTRY 221 224\\t–°–®–ê',\n",
       "  'T8\\tCITY 227 234\\t–ë–∏—à–∫–µ–∫–µ',\n",
       "  'T9\\tDATE 288 298\\t–≤–æ –≤—Ç–æ—Ä–Ω–∏–∫',\n",
       "  'T10\\tORGANIZATION 312 315\\t–ú–í–î',\n",
       "  'T11\\tDATE 91 101\\t05/08/2008',\n",
       "  'T12\\tCITY 433 440\\t–´–Ω—Ç—ã–º–∞–∫',\n",
       "  'T13\\tAGE 464 474\\t66-–ª–µ—Ç–Ω–µ–º—É',\n",
       "  'T14\\tCOUNTRY 486 494\\t–ö–∏—Ä–≥–∏–∑–∏–∏',\n",
       "  'T15\\tCOUNTRY 519 522\\t–°–®–ê',\n",
       "  'T16\\tNUMBER 545 550\\t—à–µ—Å—Ç—å',\n",
       "  'T17\\tNUMBER 631 633\\t26',\n",
       "  'T18\\tNUMBER 652 667\\t5,56 –º–∏–ª–ª–∏–º–µ—Ç—Ä–∞',\n",
       "  'T19\\tNUMBER 669 672\\t–¥–≤–∞',\n",
       "  'T20\\tPRODUCT 690 697\\t–ú–û–°–í–ï–ì–ê',\n",
       "  'T21\\tNUMBER 713 719\\t—á–µ—Ç—ã—Ä–µ',\n",
       "  'T22\\tNUMBER 758 761\\t–¥–≤–∞',\n",
       "  'T23\\tNUMBER 788 794\\t—á–µ—Ç—ã—Ä–µ',\n",
       "  'T24\\tNUMBER 855 860\\t—à–µ—Å—Ç—å',\n",
       "  'T25\\tNUMBER 880 893\\t9 –º–∏–ª–ª–∏–º–µ—Ç—Ä–æ–≤',\n",
       "  'T26\\tPRODUCT 900 907\\t–ë–µ—Ä–µ—Ç—Ç–∞',\n",
       "  'T27\\tNUMBER 909 913\\t–æ–¥–Ω–∞',\n",
       "  'T28\\tORGANIZATION 949 952\\t–ú–í–î',\n",
       "  'T29\\tNUMBER 1098 1100\\t10',\n",
       "  'T30\\tCOUNTRY 1136 1139\\t–°–®–ê',\n",
       "  'T31\\tNUMBER 1372 1375\\t–î–≤–∞',\n",
       "  'T32\\tNUMBER 1382 1386\\t2920',\n",
       "  'T33\\tNUMBER 1409 1424\\t5,56 –º–∏–ª–ª–∏–º–µ—Ç—Ä–∞',\n",
       "  'T34\\tNUMBER 1426 1431\\t10556',\n",
       "  'T35\\tNUMBER 1454 1467\\t9 –º–∏–ª–ª–∏–º–µ—Ç—Ä–æ–≤',\n",
       "  'T36\\tNUMBER 1469 1472\\t–¥–≤–∞',\n",
       "  'T37\\tNUMBER 1496 1510\\t50 –º–∏–ª–ª–∏–º–µ—Ç—Ä–æ–≤',\n",
       "  'T38\\tNUMBER 1521 1524\\t350',\n",
       "  'T39\\tNUMBER 1547 1561\\t12 –º–∏–ª–ª–∏–º–µ—Ç—Ä–æ–≤',\n",
       "  'T40\\tNUMBER 1575 1578\\t478',\n",
       "  'T41\\tNUMBER 1639 1643\\t1000',\n",
       "  'T42\\tORGANIZATION 312 324\\t–ú–í–î –ö–∏—Ä–≥–∏–∑–∏–∏',\n",
       "  'T43\\tNUMBER 1650 1652\\t66',\n",
       "  'T44\\tORGANIZATION 1792 1795\\t–ú–í–î',\n",
       "  'T45\\tNUMBER 1702 1704\\t57',\n",
       "  'T46\\tPRODUCT 1740 1747\\t–ë–µ—Ä–µ—Ç—Ç–∞',\n",
       "  'T47\\tORGANIZATION 1779 1795\\t–ü—Ä–µ—Å—Å-—Å–ª—É–∂–±–∞ –ú–í–î',\n",
       "  'T48\\tORGANIZATION 1850 1861\\t–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞',\n",
       "  'T49\\tORGANIZATION 1939 1950\\t–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏',\n",
       "  'T50\\tCOUNTRY 1979 1982\\t–°–®–ê',\n",
       "  'T51\\tCOUNTRY 2012 2020\\t–ö–∏—Ä–≥–∏–∑–∏–∏',\n",
       "  'T52\\tORGANIZATION 2055 2077\\t–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –ö–∏—Ä–≥–∏–∑–∏–∏',\n",
       "  'T53\\tDATE 2087 2097\\t–≤–æ –≤—Ç–æ—Ä–Ω–∏–∫',\n",
       "  'T54\\tORGANIZATION 2216 2234\\t–∫–∏—Ä–≥–∏–∑—Å–∫–∏—Ö –≤–ª–∞—Å—Ç–µ–π',\n",
       "  'T55\\tCOUNTRY 2185 2193\\t–ö–∏—Ä–≥–∏–∑–∏–∏',\n",
       "  'T56\\tCOUNTRY 2216 2226\\t–∫–∏—Ä–≥–∏–∑—Å–∫–∏—Ö',\n",
       "  'T57\\tCOUNTRY 2417 2429\\t–∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–µ',\n",
       "  'T58\\tCOUNTRY 2501 2511\\t–∫–∏—Ä–≥–∏–∑—Å–∫–∏—Ö',\n",
       "  'T59\\tCITY 2819 2835\\t—Å—Ç–æ–ª–∏—Ü—ã –ö–∏—Ä–≥–∏–∑–∏–∏',\n",
       "  'T60\\tCOUNTRY 2631 2634\\t–°–®–ê',\n",
       "  'T61\\tCOUNTRY 2637 2645\\t–ö–∏—Ä–≥–∏–∑–∏—è',\n",
       "  'T62\\tCOUNTRY 2720 2728\\t–ö–∏—Ä–≥–∏–∑–∏–∏',\n",
       "  'T63\\tNUMBER 698 711\\t12-–≥–æ –∫–∞–ª–∏–±—Ä–∞',\n",
       "  'T64\\tFACILITY 2812 2817\\t–ú–∞–Ω–∞—Å',\n",
       "  'T65\\tCOUNTRY 2827 2835\\t–ö–∏—Ä–≥–∏–∑–∏–∏',\n",
       "  'T66\\tCOUNTRY 2889 2901\\t–∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö',\n",
       "  'T67\\tCOUNTRY 2745 2757\\t–∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–π',\n",
       "  'T68\\tORGANIZATION 2998 3009\\t–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏',\n",
       "  'T69\\tTIME 91 107\\t05/08/2008 10:35',\n",
       "  'T70\\tEVENT 410 417\\t–ê—Ä—Å–µ–Ω–∞–ª',\n",
       "  'T71\\tCITY 109 115\\t–ë–ò–®–ö–ï–ö',\n",
       "  'T72\\tORGANIZATION 128 142\\t–ù–æ–≤–æ—Å—Ç–∏-–ì—Ä—É–∑–∏—è',\n",
       "  'T73\\tCOUNTRY 316 324\\t–ö–∏—Ä–≥–∏–∑–∏–∏',\n",
       "  'T74\\tORGANIZATION 299 324\\t–ø—Ä–µ—Å—Å-—Å–ª—É–∂–±–∞ –ú–í–î –ö–∏—Ä–≥–∏–∑–∏–∏',\n",
       "  'T75\\tCOUNTRY 1051 1054\\t–°–®–ê',\n",
       "  'T76\\tORGANIZATION 1040 1054\\t–ø–æ—Å–æ–ª—å—Å—Ç–≤–∞ –°–®–ê',\n",
       "  'T77\\tPROFESSION 1028 1054\\t—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ –ø–æ—Å–æ–ª—å—Å—Ç–≤–∞ –°–®–ê',\n",
       "  'T78\\tPROFESSION 1101 1115\\t–≤–æ–µ–Ω–Ω–æ—Å–ª—É–∂–∞—â–∏—Ö',\n",
       "  'T79\\tNATIONALITY 1971 1982\\t–≥—Ä–∞–∂–¥–∞–Ω –°–®–ê',\n",
       "  'T80\\tPROFESSION 1166 1196\\t—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º–∏ —Å–ø–µ—Ü–ø–æ–¥—Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è',\n",
       "  'T81\\tCOUNTRY 2136 2139\\t–°–®–ê',\n",
       "  'T82\\tORGANIZATION 1850 1869\\t–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞ –ë–∏—à–∫–µ–∫–∞',\n",
       "  'T83\\tCITY 1862 1869\\t–ë–∏—à–∫–µ–∫–∞',\n",
       "  'T84\\tTIME 1871 1877\\t–°–µ–π—á–∞—Å',\n",
       "  'T85\\tORGANIZATION 1983 2020\\t–ø—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –æ—Ä–≥–∞–Ω–∞–º–∏ –ö–∏—Ä–≥–∏–∑–∏–∏',\n",
       "  'T86\\tCOUNTRY 2069 2077\\t–ö–∏—Ä–≥–∏–∑–∏–∏',\n",
       "  'T87\\tORGANIZATION 2125 2139\\t–ø–æ—Å–æ–ª—å—Å—Ç–≤–∞ –°–®–ê',\n",
       "  'T88\\tPROFESSION 2098 2139\\t–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å –ø—Ä–µ—Å—Å-—Å–ª—É–∂–±—ã –ø–æ—Å–æ–ª—å—Å—Ç–≤–∞ –°–®–ê',\n",
       "  'T89\\tORGANIZATION 2112 2139\\t–ø—Ä–µ—Å—Å-—Å–ª—É–∂–±—ã –ø–æ—Å–æ–ª—å—Å—Ç–≤–∞ –°–®–ê',\n",
       "  'T90\\tPROFESSION 2268 2282\\t–í–æ–µ–Ω–Ω–æ—Å–ª—É–∂–∞—â–∏–µ',\n",
       "  'T91\\tORGANIZATION 2395 2406\\t–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤',\n",
       "  'T92\\tORGANIZATION 2329 2342\\t–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞',\n",
       "  'T93\\tORGANIZATION 2417 2455\\t–∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–µ –¥–∏–ø–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–µ–¥–æ–º—Å—Ç–≤–æ',\n",
       "  'T94\\tORGANIZATION 2430 2455\\t–¥–∏–ø–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–µ–¥–æ–º—Å—Ç–≤–æ',\n",
       "  'T95\\tORGANIZATION 2533 2545\\t–ø—Ä–µ—Å—Å-—Å–ª—É–∂–±–∞',\n",
       "  'T96\\tCOUNTRY 2559 2562\\t–°–®–ê',\n",
       "  'T97\\tORGANIZATION 2548 2562\\t–ü–æ—Å–æ–ª—å—Å—Ç–≤–æ –°–®–ê',\n",
       "  'T98\\tORGANIZATION 2732 2770\\t–ü—Ä–µ—Å—Å-—Å–ª—É–∂–±–∞ –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–π –≤–æ–µ–Ω–Ω–æ–π –±–∞–∑—ã',\n",
       "  'T100\\tORGANIZATION 2745 2770\\t–∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–π –≤–æ–µ–Ω–Ω–æ–π –±–∞–∑—ã',\n",
       "  'T101\\tNATIONALITY 475 494\\t–≥—Ä–∞–∂–¥–∞–Ω–∏–Ω—É –ö–∏—Ä–≥–∏–∑–∏–∏',\n",
       "  'T102\\tPROFESSION 2902 2909\\t–≤–æ–µ–Ω–Ω—ã—Ö',\n",
       "  'T103\\tORGANIZATION 2970 2984\\t–ø–æ—Å–æ–ª—å—Å—Ç–≤–æ –°–®–ê',\n",
       "  'T104\\tNATIONALITY 210 224\\t–≥—Ä–∞–∂–¥–∞–Ω–∞–º–∏ –°–®–ê',\n",
       "  'T105\\tNATIONALITY 508 522\\t–≥—Ä–∞–∂–¥–∞–Ω–∞–º–∏ –°–®–ê',\n",
       "  'T106\\tEVENT 524 543\\t–æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏ –∏–∑—ä—è—Ç—ã',\n",
       "  'T107\\tORGANIZATION 2501 2519\\t–∫–∏—Ä–≥–∏–∑—Å–∫–∏—Ö –≤–ª–∞—Å—Ç–µ–π',\n",
       "  'T108\\tCOUNTRY 2981 2984\\t–°–®–ê'],\n",
       " 'relations': ['R1\\tTAKES_PLACE_IN Arg1:T70 Arg2:T12',\n",
       "  'R2\\tAGE_IS Arg1:T101 Arg2:T13',\n",
       "  'R3\\tHEADQUARTERED_IN Arg1:T72 Arg2:T4',\n",
       "  'R4\\tHEADQUARTERED_IN Arg1:T5 Arg2:T6',\n",
       "  'R5\\tHEADQUARTERED_IN Arg1:T42 Arg2:T73',\n",
       "  'R6\\tORGANIZES Arg1:T5 Arg2:T70',\n",
       "  'R7\\tLOCATED_IN Arg1:T12 Arg2:T8',\n",
       "  'R8\\tLOCATED_IN Arg1:T8 Arg2:T6',\n",
       "  'R9\\tOWNER_OF Arg1:T101 Arg2:T12',\n",
       "  'R10\\tSUBEVENT_OF Arg1:T106 Arg2:T70',\n",
       "  'R11\\tHEADQUARTERED_IN Arg1:T82 Arg2:T83',\n",
       "  'R12\\tWORKPLACE Arg1:T88 Arg2:T89',\n",
       "  'R13\\tHEADQUARTERED_IN Arg1:T54 Arg2:T56',\n",
       "  'R14\\tALTERNATIVE_NAME Arg1:T56 Arg2:T55',\n",
       "  'R15\\tALTERNATIVE_NAME Arg1:T87 Arg2:T93',\n",
       "  'R16\\tALTERNATIVE_NAME Arg1:T81 Arg2:T57',\n",
       "  'R17\\tALTERNATIVE_NAME Arg1:T93 Arg2:T97',\n",
       "  'R18\\tALTERNATIVE_NAME Arg1:T57 Arg2:T96',\n",
       "  'R19\\tLOCATED_IN Arg1:T64 Arg2:T59',\n",
       "  'R20\\tORIGINS_FROM Arg1:T102 Arg2:T66',\n",
       "  'R21\\tOWNER_OF Arg1:T108 Arg2:T103',\n",
       "  'R22\\tORIGINS_FROM Arg1:T104 Arg2:T7',\n",
       "  'R23\\tALTERNATIVE_NAME Arg1:T1 Arg2:T104',\n",
       "  'R24\\tORIGINS_FROM Arg1:T105 Arg2:T15',\n",
       "  'R25\\tORIGINS_FROM Arg1:T101 Arg2:T14',\n",
       "  'R26\\tWORKPLACE Arg1:T77 Arg2:T76',\n",
       "  'R27\\tPART_OF Arg1:T74 Arg2:T42',\n",
       "  'R28\\tOWNER_OF Arg1:T96 Arg2:T97',\n",
       "  'R29\\tLOCATED_IN Arg1:T59 Arg2:T65',\n",
       "  'R30\\tPART_OF Arg1:T98 Arg2:T100',\n",
       "  'R31\\tOWNER_OF Arg1:T67 Arg2:T100',\n",
       "  'R32\\tOWNER_OF Arg1:T75 Arg2:T76',\n",
       "  'R33\\tPART_OF Arg1:T47 Arg2:T44',\n",
       "  'R34\\tORIGINS_FROM Arg1:T79 Arg2:T50',\n",
       "  'R35\\tHEADQUARTERED_IN Arg1:T93 Arg2:T57',\n",
       "  'R36\\tHEADQUARTERED_IN Arg1:T107 Arg2:T58',\n",
       "  'R37\\tHEADQUARTERED_IN Arg1:T85 Arg2:T51',\n",
       "  'R38\\tHEADQUARTERED_IN Arg1:T52 Arg2:T86'],\n",
       " 'links': ['N1\\tReference T5 Wikidata:Q813\\t–ö–∏—Ä–≥–∏–∑–∏—è',\n",
       "  'N2\\tReference T10 Wikidata:Q6589202\\t–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–µ–ª',\n",
       "  'N3\\tReference T12 Wikidata:Q13648730\\t–´–Ω—Ç—ã–º–∞–∫',\n",
       "  'N4\\tReference T20 Wikidata:NULL\\t',\n",
       "  'N5\\tReference T46 Wikidata:Q324782\\tBeretta',\n",
       "  'N6\\tReference T47 Wikidata:NULL\\t',\n",
       "  'N7\\tReference T48 Wikidata:Q1092499\\t–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞',\n",
       "  'N8\\tReference T49 Wikidata:Q821172\\t–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏',\n",
       "  'N9\\tReference T52 Wikidata:NULL\\t',\n",
       "  'N10\\tReference T62 Wikidata:Q813\\t–ö–∏—Ä–≥–∏–∑–∏—è',\n",
       "  'N11\\tReference T64 Wikidata:Q32997\\t–ú–∞–Ω–∞—Å',\n",
       "  'N12\\tReference T71 Wikidata:Q9361\\t–ë–∏—à–∫–µ–∫',\n",
       "  'N13\\tReference T72 Wikidata:NULL\\t',\n",
       "  'N14\\tReference T4 Wikidata:Q230\\t–ì—Ä—É–∑–∏—è',\n",
       "  'N15\\tReference T74 Wikidata:NULL\\t',\n",
       "  'N16\\tReference T75 Wikidata:Q30\\t–°–®–ê',\n",
       "  'N17\\tReference T76 Wikidata:NULL\\t',\n",
       "  'N18\\tReference T77 Wikidata:NULL\\t',\n",
       "  'N19\\tReference T78 Wikidata:Q47064\\t–≤–æ–µ–Ω–Ω–æ—Å–ª—É–∂–∞—â–∏–π',\n",
       "  'N20\\tReference T80 Wikidata:NULL\\t',\n",
       "  'N21\\tReference T82 Wikidata:NULL\\t',\n",
       "  'N22\\tReference T88 Wikidata:NULL\\t',\n",
       "  'N23\\tReference T89 Wikidata:NULL\\t',\n",
       "  'N24\\tReference T91 Wikidata:Q192350\\t–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ',\n",
       "  'N25\\tReference T92 Wikidata:Q7188\\t–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ',\n",
       "  'N26\\tReference T94 Wikidata:NULL\\t',\n",
       "  'N27\\tReference T95 Wikidata:NULL\\t',\n",
       "  'N28\\tReference T98 Wikidata:NULL\\t',\n",
       "  'N29\\tReference T100 Wikidata:Q245016\\t–≤–æ–µ–Ω–Ω–∞—è –±–∞–∑–∞',\n",
       "  'N30\\tReference T102 Wikidata:Q47064\\t–≤–æ–µ–Ω–Ω–æ—Å–ª—É–∂–∞—â–∏–π',\n",
       "  'N31\\tReference T101 Wikidata:NULL\\t',\n",
       "  'N32\\tReference T105 Wikidata:Q846570\\t',\n",
       "  'N33\\tReference T42 Wikidata:NULL\\t',\n",
       "  'N34\\tReference T59 Wikidata:Q9361\\t–ë–∏—à–∫–µ–∫',\n",
       "  'N35\\tReference T107 Wikidata:NULL\\t',\n",
       "  'N36\\tReference T85 Wikidata:Q813\\t–ö–∏—Ä–≥–∏–∑–∏—è',\n",
       "  'N37\\tReference T44 Wikidata:Q6589202\\t–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–µ–ª',\n",
       "  'N38\\tReference T28 Wikidata:Q6589202\\t–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–µ–ª',\n",
       "  'N39\\tReference T26 Wikidata:Q324782\\tBeretta',\n",
       "  'N40\\tReference T68 Wikidata:Q821172\\t–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏',\n",
       "  'N41\\tReference T51 Wikidata:Q813\\t–ö–∏—Ä–≥–∏–∑–∏—è',\n",
       "  'N42\\tReference T61 Wikidata:Q813\\t–ö–∏—Ä–≥–∏–∑–∏—è',\n",
       "  'N43\\tReference T56 Wikidata:Q813\\t–ö–∏—Ä–≥–∏–∑–∏—è',\n",
       "  'N44\\tReference T14 Wikidata:Q813\\t–ö–∏—Ä–≥–∏–∑–∏—è',\n",
       "  'N45\\tReference T86 Wikidata:Q813\\t–ö–∏—Ä–≥–∏–∑–∏—è',\n",
       "  'N46\\tReference T65 Wikidata:Q813\\t–ö–∏—Ä–≥–∏–∑–∏—è',\n",
       "  'N47\\tReference T73 Wikidata:Q813\\t–ö–∏—Ä–≥–∏–∑–∏—è',\n",
       "  'N48\\tReference T55 Wikidata:Q813\\t–ö–∏—Ä–≥–∏–∑–∏—è',\n",
       "  'N49\\tReference T58 Wikidata:Q813\\t–ö–∏—Ä–≥–∏–∑–∏—è',\n",
       "  'N50\\tReference T6 Wikidata:Q813\\t–ö–∏—Ä–≥–∏–∑–∏—è',\n",
       "  'N51\\tReference T8 Wikidata:Q9361\\t–ë–∏—à–∫–µ–∫',\n",
       "  'N52\\tReference T83 Wikidata:Q9361\\t–ë–∏—à–∫–µ–∫',\n",
       "  'N53\\tReference T2 Wikidata:Q9361\\t–ë–∏—à–∫–µ–∫',\n",
       "  'N54\\tReference T57 Wikidata:Q30\\t–°–®–ê',\n",
       "  'N55\\tReference T96 Wikidata:Q30\\t–°–®–ê',\n",
       "  'N56\\tReference T15 Wikidata:Q30\\t–°–®–ê',\n",
       "  'N57\\tReference T108 Wikidata:Q30\\t–°–®–ê',\n",
       "  'N58\\tReference T7 Wikidata:Q30\\t–°–®–ê',\n",
       "  'N59\\tReference T81 Wikidata:Q30\\t–°–®–ê',\n",
       "  'N60\\tReference T30 Wikidata:Q30\\t–°–®–ê',\n",
       "  'N61\\tReference T67 Wikidata:Q30\\t–°–®–ê',\n",
       "  'N62\\tReference T50 Wikidata:Q30\\t–°–®–ê',\n",
       "  'N63\\tReference T66 Wikidata:Q30\\t–°–®–ê',\n",
       "  'N64\\tReference T60 Wikidata:Q30\\t–°–®–ê',\n",
       "  'N65\\tReference T90 Wikidata:Q47064\\t–≤–æ–µ–Ω–Ω–æ—Å–ª—É–∂–∞—â–∏–π',\n",
       "  'N66\\tReference T79 Wikidata:Q846570\\t',\n",
       "  'N67\\tReference T1 Wikidata:Q846570\\t',\n",
       "  'N68\\tReference T104 Wikidata:Q846570\\t',\n",
       "  'N69\\tReference T103 Wikidata:NULL\\t',\n",
       "  'N70\\tReference T87 Wikidata:NULL\\t',\n",
       "  'N71\\tReference T93 Wikidata:NULL\\t',\n",
       "  'N72\\tReference T97 Wikidata:NULL\\t',\n",
       "  'N73\\tReference T54 Wikidata:NULL\\t']}"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 258
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVA333xvYoUn"
   },
   "source": [
    "The labels are already coded as integer ids to be easily usable by our model, but the correspondence with the actual categories is stored in the `features` of the dataset:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:12.273169Z",
     "start_time": "2024-04-10T09:22:12.266987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def transform_entities(entities):\n",
    "    markup = []\n",
    "    for i in entities:\n",
    "        splited = re.split('\\t', i)\n",
    "        data = re.split(' ', splited[1])\n",
    "        if \";\" in data[2]:\n",
    "            data[2] = data[2].split(\";\")[0]\n",
    "        markup.append({'id': splited[0],\n",
    "               'type': data[0],\n",
    "               'start': int(data[1]),\n",
    "               'stop': int(data[2]),\n",
    "               'text': splited[2]})\n",
    "    markup.sort(key=lambda x: x[\"start\"])\n",
    "    return markup"
   ],
   "outputs": [],
   "execution_count": 259
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:12.293707Z",
     "start_time": "2024-04-10T09:22:12.274460Z"
    }
   },
   "cell_type": "code",
   "source": "transform_entities(datasets[\"train\"][0]['entities'])",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'T1',\n",
       "  'type': 'NATIONALITY',\n",
       "  'start': 62,\n",
       "  'stop': 74,\n",
       "  'text': '–∞–º–µ—Ä–∏–∫–∞–Ω—Ü–∞–º–∏'},\n",
       " {'id': 'T2', 'type': 'CITY', 'start': 82, 'stop': 89, 'text': '–ë–∏—à–∫–µ–∫–µ'},\n",
       " {'id': 'T11', 'type': 'DATE', 'start': 91, 'stop': 101, 'text': '05/08/2008'},\n",
       " {'id': 'T69',\n",
       "  'type': 'TIME',\n",
       "  'start': 91,\n",
       "  'stop': 107,\n",
       "  'text': '05/08/2008 10:35'},\n",
       " {'id': 'T71', 'type': 'CITY', 'start': 109, 'stop': 115, 'text': '–ë–ò–®–ö–ï–ö'},\n",
       " {'id': 'T3', 'type': 'DATE', 'start': 117, 'stop': 126, 'text': '5 –∞–≤–≥—É—Å—Ç–∞'},\n",
       " {'id': 'T72',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 128,\n",
       "  'stop': 142,\n",
       "  'text': '–ù–æ–≤–æ—Å—Ç–∏-–ì—Ä—É–∑–∏—è'},\n",
       " {'id': 'T4', 'type': 'COUNTRY', 'start': 136, 'stop': 142, 'text': '–ì—Ä—É–∑–∏—è'},\n",
       " {'id': 'T5',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 145,\n",
       "  'stop': 179,\n",
       "  'text': '–ü—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã –ö–∏—Ä–≥–∏–∑–∏–∏'},\n",
       " {'id': 'T6',\n",
       "  'type': 'COUNTRY',\n",
       "  'start': 171,\n",
       "  'stop': 179,\n",
       "  'text': '–ö–∏—Ä–≥–∏–∑–∏–∏'},\n",
       " {'id': 'T104',\n",
       "  'type': 'NATIONALITY',\n",
       "  'start': 210,\n",
       "  'stop': 224,\n",
       "  'text': '–≥—Ä–∞–∂–¥–∞–Ω–∞–º–∏ –°–®–ê'},\n",
       " {'id': 'T7', 'type': 'COUNTRY', 'start': 221, 'stop': 224, 'text': '–°–®–ê'},\n",
       " {'id': 'T8', 'type': 'CITY', 'start': 227, 'stop': 234, 'text': '–ë–∏—à–∫–µ–∫–µ'},\n",
       " {'id': 'T9', 'type': 'DATE', 'start': 288, 'stop': 298, 'text': '–≤–æ –≤—Ç–æ—Ä–Ω–∏–∫'},\n",
       " {'id': 'T74',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 299,\n",
       "  'stop': 324,\n",
       "  'text': '–ø—Ä–µ—Å—Å-—Å–ª—É–∂–±–∞ –ú–í–î –ö–∏—Ä–≥–∏–∑–∏–∏'},\n",
       " {'id': 'T10',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 312,\n",
       "  'stop': 315,\n",
       "  'text': '–ú–í–î'},\n",
       " {'id': 'T42',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 312,\n",
       "  'stop': 324,\n",
       "  'text': '–ú–í–î –ö–∏—Ä–≥–∏–∑–∏–∏'},\n",
       " {'id': 'T73',\n",
       "  'type': 'COUNTRY',\n",
       "  'start': 316,\n",
       "  'stop': 324,\n",
       "  'text': '–ö–∏—Ä–≥–∏–∑–∏–∏'},\n",
       " {'id': 'T70', 'type': 'EVENT', 'start': 410, 'stop': 417, 'text': '–ê—Ä—Å–µ–Ω–∞–ª'},\n",
       " {'id': 'T12', 'type': 'CITY', 'start': 433, 'stop': 440, 'text': '–´–Ω—Ç—ã–º–∞–∫'},\n",
       " {'id': 'T13', 'type': 'AGE', 'start': 464, 'stop': 474, 'text': '66-–ª–µ—Ç–Ω–µ–º—É'},\n",
       " {'id': 'T101',\n",
       "  'type': 'NATIONALITY',\n",
       "  'start': 475,\n",
       "  'stop': 494,\n",
       "  'text': '–≥—Ä–∞–∂–¥–∞–Ω–∏–Ω—É –ö–∏—Ä–≥–∏–∑–∏–∏'},\n",
       " {'id': 'T14',\n",
       "  'type': 'COUNTRY',\n",
       "  'start': 486,\n",
       "  'stop': 494,\n",
       "  'text': '–ö–∏—Ä–≥–∏–∑–∏–∏'},\n",
       " {'id': 'T105',\n",
       "  'type': 'NATIONALITY',\n",
       "  'start': 508,\n",
       "  'stop': 522,\n",
       "  'text': '–≥—Ä–∞–∂–¥–∞–Ω–∞–º–∏ –°–®–ê'},\n",
       " {'id': 'T15', 'type': 'COUNTRY', 'start': 519, 'stop': 522, 'text': '–°–®–ê'},\n",
       " {'id': 'T106',\n",
       "  'type': 'EVENT',\n",
       "  'start': 524,\n",
       "  'stop': 543,\n",
       "  'text': '–æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏ –∏–∑—ä—è—Ç—ã'},\n",
       " {'id': 'T16', 'type': 'NUMBER', 'start': 545, 'stop': 550, 'text': '—à–µ—Å—Ç—å'},\n",
       " {'id': 'T17', 'type': 'NUMBER', 'start': 631, 'stop': 633, 'text': '26'},\n",
       " {'id': 'T18',\n",
       "  'type': 'NUMBER',\n",
       "  'start': 652,\n",
       "  'stop': 667,\n",
       "  'text': '5,56 –º–∏–ª–ª–∏–º–µ—Ç—Ä–∞'},\n",
       " {'id': 'T19', 'type': 'NUMBER', 'start': 669, 'stop': 672, 'text': '–¥–≤–∞'},\n",
       " {'id': 'T20',\n",
       "  'type': 'PRODUCT',\n",
       "  'start': 690,\n",
       "  'stop': 697,\n",
       "  'text': '–ú–û–°–í–ï–ì–ê'},\n",
       " {'id': 'T63',\n",
       "  'type': 'NUMBER',\n",
       "  'start': 698,\n",
       "  'stop': 711,\n",
       "  'text': '12-–≥–æ –∫–∞–ª–∏–±—Ä–∞'},\n",
       " {'id': 'T21', 'type': 'NUMBER', 'start': 713, 'stop': 719, 'text': '—á–µ—Ç—ã—Ä–µ'},\n",
       " {'id': 'T22', 'type': 'NUMBER', 'start': 758, 'stop': 761, 'text': '–¥–≤–∞'},\n",
       " {'id': 'T23', 'type': 'NUMBER', 'start': 788, 'stop': 794, 'text': '—á–µ—Ç—ã—Ä–µ'},\n",
       " {'id': 'T24', 'type': 'NUMBER', 'start': 855, 'stop': 860, 'text': '—à–µ—Å—Ç—å'},\n",
       " {'id': 'T25',\n",
       "  'type': 'NUMBER',\n",
       "  'start': 880,\n",
       "  'stop': 893,\n",
       "  'text': '9 –º–∏–ª–ª–∏–º–µ—Ç—Ä–æ–≤'},\n",
       " {'id': 'T26',\n",
       "  'type': 'PRODUCT',\n",
       "  'start': 900,\n",
       "  'stop': 907,\n",
       "  'text': '–ë–µ—Ä–µ—Ç—Ç–∞'},\n",
       " {'id': 'T27', 'type': 'NUMBER', 'start': 909, 'stop': 913, 'text': '–æ–¥–Ω–∞'},\n",
       " {'id': 'T28',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 949,\n",
       "  'stop': 952,\n",
       "  'text': '–ú–í–î'},\n",
       " {'id': 'T77',\n",
       "  'type': 'PROFESSION',\n",
       "  'start': 1028,\n",
       "  'stop': 1054,\n",
       "  'text': '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ –ø–æ—Å–æ–ª—å—Å—Ç–≤–∞ –°–®–ê'},\n",
       " {'id': 'T76',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 1040,\n",
       "  'stop': 1054,\n",
       "  'text': '–ø–æ—Å–æ–ª—å—Å—Ç–≤–∞ –°–®–ê'},\n",
       " {'id': 'T75', 'type': 'COUNTRY', 'start': 1051, 'stop': 1054, 'text': '–°–®–ê'},\n",
       " {'id': 'T29', 'type': 'NUMBER', 'start': 1098, 'stop': 1100, 'text': '10'},\n",
       " {'id': 'T78',\n",
       "  'type': 'PROFESSION',\n",
       "  'start': 1101,\n",
       "  'stop': 1115,\n",
       "  'text': '–≤–æ–µ–Ω–Ω–æ—Å–ª—É–∂–∞—â–∏—Ö'},\n",
       " {'id': 'T30', 'type': 'COUNTRY', 'start': 1136, 'stop': 1139, 'text': '–°–®–ê'},\n",
       " {'id': 'T80',\n",
       "  'type': 'PROFESSION',\n",
       "  'start': 1166,\n",
       "  'stop': 1196,\n",
       "  'text': '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º–∏ —Å–ø–µ—Ü–ø–æ–¥—Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è'},\n",
       " {'id': 'T31', 'type': 'NUMBER', 'start': 1372, 'stop': 1375, 'text': '–î–≤–∞'},\n",
       " {'id': 'T32', 'type': 'NUMBER', 'start': 1382, 'stop': 1386, 'text': '2920'},\n",
       " {'id': 'T33',\n",
       "  'type': 'NUMBER',\n",
       "  'start': 1409,\n",
       "  'stop': 1424,\n",
       "  'text': '5,56 –º–∏–ª–ª–∏–º–µ—Ç—Ä–∞'},\n",
       " {'id': 'T34', 'type': 'NUMBER', 'start': 1426, 'stop': 1431, 'text': '10556'},\n",
       " {'id': 'T35',\n",
       "  'type': 'NUMBER',\n",
       "  'start': 1454,\n",
       "  'stop': 1467,\n",
       "  'text': '9 –º–∏–ª–ª–∏–º–µ—Ç—Ä–æ–≤'},\n",
       " {'id': 'T36', 'type': 'NUMBER', 'start': 1469, 'stop': 1472, 'text': '–¥–≤–∞'},\n",
       " {'id': 'T37',\n",
       "  'type': 'NUMBER',\n",
       "  'start': 1496,\n",
       "  'stop': 1510,\n",
       "  'text': '50 –º–∏–ª–ª–∏–º–µ—Ç—Ä–æ–≤'},\n",
       " {'id': 'T38', 'type': 'NUMBER', 'start': 1521, 'stop': 1524, 'text': '350'},\n",
       " {'id': 'T39',\n",
       "  'type': 'NUMBER',\n",
       "  'start': 1547,\n",
       "  'stop': 1561,\n",
       "  'text': '12 –º–∏–ª–ª–∏–º–µ—Ç—Ä–æ–≤'},\n",
       " {'id': 'T40', 'type': 'NUMBER', 'start': 1575, 'stop': 1578, 'text': '478'},\n",
       " {'id': 'T41', 'type': 'NUMBER', 'start': 1639, 'stop': 1643, 'text': '1000'},\n",
       " {'id': 'T43', 'type': 'NUMBER', 'start': 1650, 'stop': 1652, 'text': '66'},\n",
       " {'id': 'T45', 'type': 'NUMBER', 'start': 1702, 'stop': 1704, 'text': '57'},\n",
       " {'id': 'T46',\n",
       "  'type': 'PRODUCT',\n",
       "  'start': 1740,\n",
       "  'stop': 1747,\n",
       "  'text': '–ë–µ—Ä–µ—Ç—Ç–∞'},\n",
       " {'id': 'T47',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 1779,\n",
       "  'stop': 1795,\n",
       "  'text': '–ü—Ä–µ—Å—Å-—Å–ª—É–∂–±–∞ –ú–í–î'},\n",
       " {'id': 'T44',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 1792,\n",
       "  'stop': 1795,\n",
       "  'text': '–ú–í–î'},\n",
       " {'id': 'T48',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 1850,\n",
       "  'stop': 1861,\n",
       "  'text': '–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞'},\n",
       " {'id': 'T82',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 1850,\n",
       "  'stop': 1869,\n",
       "  'text': '–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞ –ë–∏—à–∫–µ–∫–∞'},\n",
       " {'id': 'T83', 'type': 'CITY', 'start': 1862, 'stop': 1869, 'text': '–ë–∏—à–∫–µ–∫–∞'},\n",
       " {'id': 'T84', 'type': 'TIME', 'start': 1871, 'stop': 1877, 'text': '–°–µ–π—á–∞—Å'},\n",
       " {'id': 'T49',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 1939,\n",
       "  'stop': 1950,\n",
       "  'text': '–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏'},\n",
       " {'id': 'T79',\n",
       "  'type': 'NATIONALITY',\n",
       "  'start': 1971,\n",
       "  'stop': 1982,\n",
       "  'text': '–≥—Ä–∞–∂–¥–∞–Ω –°–®–ê'},\n",
       " {'id': 'T50', 'type': 'COUNTRY', 'start': 1979, 'stop': 1982, 'text': '–°–®–ê'},\n",
       " {'id': 'T85',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 1983,\n",
       "  'stop': 2020,\n",
       "  'text': '–ø—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –æ—Ä–≥–∞–Ω–∞–º–∏ –ö–∏—Ä–≥–∏–∑–∏–∏'},\n",
       " {'id': 'T51',\n",
       "  'type': 'COUNTRY',\n",
       "  'start': 2012,\n",
       "  'stop': 2020,\n",
       "  'text': '–ö–∏—Ä–≥–∏–∑–∏–∏'},\n",
       " {'id': 'T52',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 2055,\n",
       "  'stop': 2077,\n",
       "  'text': '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –ö–∏—Ä–≥–∏–∑–∏–∏'},\n",
       " {'id': 'T86',\n",
       "  'type': 'COUNTRY',\n",
       "  'start': 2069,\n",
       "  'stop': 2077,\n",
       "  'text': '–ö–∏—Ä–≥–∏–∑–∏–∏'},\n",
       " {'id': 'T53',\n",
       "  'type': 'DATE',\n",
       "  'start': 2087,\n",
       "  'stop': 2097,\n",
       "  'text': '–≤–æ –≤—Ç–æ—Ä–Ω–∏–∫'},\n",
       " {'id': 'T88',\n",
       "  'type': 'PROFESSION',\n",
       "  'start': 2098,\n",
       "  'stop': 2139,\n",
       "  'text': '–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å –ø—Ä–µ—Å—Å-—Å–ª—É–∂–±—ã –ø–æ—Å–æ–ª—å—Å—Ç–≤–∞ –°–®–ê'},\n",
       " {'id': 'T89',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 2112,\n",
       "  'stop': 2139,\n",
       "  'text': '–ø—Ä–µ—Å—Å-—Å–ª—É–∂–±—ã –ø–æ—Å–æ–ª—å—Å—Ç–≤–∞ –°–®–ê'},\n",
       " {'id': 'T87',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 2125,\n",
       "  'stop': 2139,\n",
       "  'text': '–ø–æ—Å–æ–ª—å—Å—Ç–≤–∞ –°–®–ê'},\n",
       " {'id': 'T81', 'type': 'COUNTRY', 'start': 2136, 'stop': 2139, 'text': '–°–®–ê'},\n",
       " {'id': 'T55',\n",
       "  'type': 'COUNTRY',\n",
       "  'start': 2185,\n",
       "  'stop': 2193,\n",
       "  'text': '–ö–∏—Ä–≥–∏–∑–∏–∏'},\n",
       " {'id': 'T54',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 2216,\n",
       "  'stop': 2234,\n",
       "  'text': '–∫–∏—Ä–≥–∏–∑—Å–∫–∏—Ö –≤–ª–∞—Å—Ç–µ–π'},\n",
       " {'id': 'T56',\n",
       "  'type': 'COUNTRY',\n",
       "  'start': 2216,\n",
       "  'stop': 2226,\n",
       "  'text': '–∫–∏—Ä–≥–∏–∑—Å–∫–∏—Ö'},\n",
       " {'id': 'T90',\n",
       "  'type': 'PROFESSION',\n",
       "  'start': 2268,\n",
       "  'stop': 2282,\n",
       "  'text': '–í–æ–µ–Ω–Ω–æ—Å–ª—É–∂–∞—â–∏–µ'},\n",
       " {'id': 'T92',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 2329,\n",
       "  'stop': 2342,\n",
       "  'text': '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞'},\n",
       " {'id': 'T91',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 2395,\n",
       "  'stop': 2406,\n",
       "  'text': '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤'},\n",
       " {'id': 'T57',\n",
       "  'type': 'COUNTRY',\n",
       "  'start': 2417,\n",
       "  'stop': 2429,\n",
       "  'text': '–∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–µ'},\n",
       " {'id': 'T93',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 2417,\n",
       "  'stop': 2455,\n",
       "  'text': '–∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–µ –¥–∏–ø–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–µ–¥–æ–º—Å—Ç–≤–æ'},\n",
       " {'id': 'T94',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 2430,\n",
       "  'stop': 2455,\n",
       "  'text': '–¥–∏–ø–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–µ–¥–æ–º—Å—Ç–≤–æ'},\n",
       " {'id': 'T58',\n",
       "  'type': 'COUNTRY',\n",
       "  'start': 2501,\n",
       "  'stop': 2511,\n",
       "  'text': '–∫–∏—Ä–≥–∏–∑—Å–∫–∏—Ö'},\n",
       " {'id': 'T107',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 2501,\n",
       "  'stop': 2519,\n",
       "  'text': '–∫–∏—Ä–≥–∏–∑—Å–∫–∏—Ö –≤–ª–∞—Å—Ç–µ–π'},\n",
       " {'id': 'T95',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 2533,\n",
       "  'stop': 2545,\n",
       "  'text': '–ø—Ä–µ—Å—Å-—Å–ª—É–∂–±–∞'},\n",
       " {'id': 'T97',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 2548,\n",
       "  'stop': 2562,\n",
       "  'text': '–ü–æ—Å–æ–ª—å—Å—Ç–≤–æ –°–®–ê'},\n",
       " {'id': 'T96', 'type': 'COUNTRY', 'start': 2559, 'stop': 2562, 'text': '–°–®–ê'},\n",
       " {'id': 'T60', 'type': 'COUNTRY', 'start': 2631, 'stop': 2634, 'text': '–°–®–ê'},\n",
       " {'id': 'T61',\n",
       "  'type': 'COUNTRY',\n",
       "  'start': 2637,\n",
       "  'stop': 2645,\n",
       "  'text': '–ö–∏—Ä–≥–∏–∑–∏—è'},\n",
       " {'id': 'T62',\n",
       "  'type': 'COUNTRY',\n",
       "  'start': 2720,\n",
       "  'stop': 2728,\n",
       "  'text': '–ö–∏—Ä–≥–∏–∑–∏–∏'},\n",
       " {'id': 'T98',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 2732,\n",
       "  'stop': 2770,\n",
       "  'text': '–ü—Ä–µ—Å—Å-—Å–ª—É–∂–±–∞ –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–π –≤–æ–µ–Ω–Ω–æ–π –±–∞–∑—ã'},\n",
       " {'id': 'T67',\n",
       "  'type': 'COUNTRY',\n",
       "  'start': 2745,\n",
       "  'stop': 2757,\n",
       "  'text': '–∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–π'},\n",
       " {'id': 'T100',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 2745,\n",
       "  'stop': 2770,\n",
       "  'text': '–∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–π –≤–æ–µ–Ω–Ω–æ–π –±–∞–∑—ã'},\n",
       " {'id': 'T64',\n",
       "  'type': 'FACILITY',\n",
       "  'start': 2812,\n",
       "  'stop': 2817,\n",
       "  'text': '–ú–∞–Ω–∞—Å'},\n",
       " {'id': 'T59',\n",
       "  'type': 'CITY',\n",
       "  'start': 2819,\n",
       "  'stop': 2835,\n",
       "  'text': '—Å—Ç–æ–ª–∏—Ü—ã –ö–∏—Ä–≥–∏–∑–∏–∏'},\n",
       " {'id': 'T65',\n",
       "  'type': 'COUNTRY',\n",
       "  'start': 2827,\n",
       "  'stop': 2835,\n",
       "  'text': '–ö–∏—Ä–≥–∏–∑–∏–∏'},\n",
       " {'id': 'T66',\n",
       "  'type': 'COUNTRY',\n",
       "  'start': 2889,\n",
       "  'stop': 2901,\n",
       "  'text': '–∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏—Ö'},\n",
       " {'id': 'T102',\n",
       "  'type': 'PROFESSION',\n",
       "  'start': 2902,\n",
       "  'stop': 2909,\n",
       "  'text': '–≤–æ–µ–Ω–Ω—ã—Ö'},\n",
       " {'id': 'T103',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 2970,\n",
       "  'stop': 2984,\n",
       "  'text': '–ø–æ—Å–æ–ª—å—Å—Ç–≤–æ –°–®–ê'},\n",
       " {'id': 'T108', 'type': 'COUNTRY', 'start': 2981, 'stop': 2984, 'text': '–°–®–ê'},\n",
       " {'id': 'T68',\n",
       "  'type': 'ORGANIZATION',\n",
       "  'start': 2998,\n",
       "  'stop': 3009,\n",
       "  'text': '–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏'}]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 260
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:12.300969Z",
     "start_time": "2024-04-10T09:22:12.295097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def transform(text, markup): \n",
    "    tokens = [text[0:markup[0]['start']]]\n",
    "    tags = ['O']\n",
    "    \n",
    "    for i in range(len(markup[:-1])):\n",
    "        tokens.append(text[markup[i]['start']:markup[i]['stop']])\n",
    "        tags.append(markup[i]['type'])\n",
    "        tokens.append(text[markup[i]['stop']:markup[i + 1]['start']])\n",
    "        tags.append('O')\n",
    "\n",
    "    tokens.append(text[markup[-1]['start']:markup[-1]['stop']])\n",
    "    tags.append(markup[-1]['type'])\n",
    "    tokens.append(text[markup[-1]['stop']:])\n",
    "    tags.append('O')\n",
    "    \n",
    "    final_tokens = []\n",
    "    final_tags = []\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        size = len(tokens[i].split())\n",
    "        final_tokens += tokens[i].split()\n",
    "        if tags[i] != \"O\":\n",
    "            final_tags.append(\"B-\" + tags[i])\n",
    "            final_tags += [\"I-\" + tags[i]] * (size - 1)\n",
    "        else:\n",
    "            final_tags += [tags[i]] * size\n",
    "        \n",
    "    return final_tokens, final_tags"
   ],
   "outputs": [],
   "execution_count": 261
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:12.307535Z",
     "start_time": "2024-04-10T09:22:12.301974Z"
    }
   },
   "cell_type": "code",
   "source": "tokens, entities = transform(datasets[\"train\"][0]['text'], transform_entities(datasets[\"train\"][0]['entities']))",
   "outputs": [],
   "execution_count": 262
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:12.312957Z",
     "start_time": "2024-04-10T09:22:12.308735Z"
    }
   },
   "cell_type": "code",
   "source": "len(tokens), len(entities)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445, 445)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 263
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RddCu-oqYoUn",
    "outputId": "a4aa7160-f27b-476f-d1cc-074562f8e944",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:12.321979Z",
     "start_time": "2024-04-10T09:22:12.314660Z"
    }
   },
   "source": "entities",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NATIONALITY',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-CITY',\n",
       " 'B-DATE',\n",
       " 'B-TIME',\n",
       " 'I-TIME',\n",
       " 'B-CITY',\n",
       " 'O',\n",
       " 'B-DATE',\n",
       " 'I-DATE',\n",
       " 'O',\n",
       " 'B-ORGANIZATION',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NATIONALITY',\n",
       " 'I-NATIONALITY',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'B-CITY',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-DATE',\n",
       " 'I-DATE',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-ORGANIZATION',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-EVENT',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-CITY',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-AGE',\n",
       " 'B-NATIONALITY',\n",
       " 'I-NATIONALITY',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NATIONALITY',\n",
       " 'I-NATIONALITY',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'B-EVENT',\n",
       " 'I-EVENT',\n",
       " 'I-EVENT',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'I-NUMBER',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PRODUCT',\n",
       " 'B-NUMBER',\n",
       " 'I-NUMBER',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'I-NUMBER',\n",
       " 'O',\n",
       " 'B-PRODUCT',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORGANIZATION',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PROFESSION',\n",
       " 'I-PROFESSION',\n",
       " 'I-PROFESSION',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'B-PROFESSION',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PROFESSION',\n",
       " 'I-PROFESSION',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'I-NUMBER',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'I-NUMBER',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'I-NUMBER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'I-NUMBER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NUMBER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PRODUCT',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-ORGANIZATION',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORGANIZATION',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-CITY',\n",
       " 'O',\n",
       " 'B-TIME',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-NATIONALITY',\n",
       " 'I-NATIONALITY',\n",
       " 'B-COUNTRY',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-DATE',\n",
       " 'I-DATE',\n",
       " 'B-PROFESSION',\n",
       " 'I-PROFESSION',\n",
       " 'I-PROFESSION',\n",
       " 'I-PROFESSION',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PROFESSION',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORGANIZATION',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORGANIZATION',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-COUNTRY',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-COUNTRY',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORGANIZATION',\n",
       " 'O',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-COUNTRY',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-FACILITY',\n",
       " 'O',\n",
       " 'B-CITY',\n",
       " 'I-CITY',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-COUNTRY',\n",
       " 'B-PROFESSION',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'B-COUNTRY',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORGANIZATION',\n",
       " 'I-ORGANIZATION',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 264
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z30lbCjLYoUn"
   },
   "source": [
    "So for the NER tags, 0 corresponds to 'O', 1 to 'B-PER' etc... On top of the 'O' (which means no special entity), there are four labels for NER here, each prefixed with 'B-' (for beginning) or 'I-' (for intermediate), that indicate if the token is the first one for the current group with the label or not:\n",
    "- 'PER' for person\n",
    "- 'ORG' for organization\n",
    "- 'LOC' for location\n",
    "- 'MISC' for miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYqXmDFdYoUn"
   },
   "source": [
    "Since the labels are lists of `ClassLabel`, the actual names of the labels are nested in the `feature` attribute of the object above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset (automatically decoding the labels in passing)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:12.869969Z",
     "start_time": "2024-04-10T09:22:12.863739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class OwnDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        text = item['text']\n",
    "        tokens, entities = transform(item['text'], transform_entities(item['entities']))\n",
    "        return {\"text\": text,\n",
    "                \"tokens\": tokens,\n",
    "                \"ner_tags\": entities}"
   ],
   "outputs": [],
   "execution_count": 265
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:13.105133Z",
     "start_time": "2024-04-10T09:22:13.101263Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset = OwnDataset(datasets['train'])",
   "outputs": [],
   "execution_count": 266
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i3j8APAoIrI3",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:13.384790Z",
     "start_time": "2024-04-10T09:22:13.377002Z"
    }
   },
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(columns=[\"text\", \"tokens\", \"ner_tags\"])\n",
    "    for idx in range(num_examples):\n",
    "        df.at[idx, \"text\"] = dataset[picks[idx]][\"text\"]\n",
    "        df.at[idx, \"tokens\"] = dataset[picks[idx]][\"tokens\"]\n",
    "        df.at[idx, \"ner_tags\"] = dataset[picks[idx]][\"ner_tags\"]\n",
    "    display(HTML(df.to_html()))"
   ],
   "outputs": [],
   "execution_count": 267
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:13.729672Z",
     "start_time": "2024-04-10T09:22:13.641417Z"
    }
   },
   "source": "show_random_elements(train_dataset)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–î–≤–µ –ª–µ—Å–±–∏—è–Ω–∫–∏ —Å–æ –≤–∑—Ä–æ—Å–ª—ã–º —Å—ã–Ω–æ–º –∏–∑ –†–æ—Å—Å–∏–∏ –ø–æ–∂–µ–Ω–∏–ª–∏—Å—å –∏ –ø–æ–ø—Ä–æ—Å–∏–ª–∏ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–±–µ–∂–∏—â–∞ –≤ –ê—Ä–≥–µ–Ω—Ç–∏–Ω–µ\\n–í –¥–µ–Ω—å —á–µ—Ç—ã—Ä—ë—Ö–ª–µ—Ç–∏—è –ø—Ä–∏–Ω—è—Ç–∏—è –∑–∞–∫–æ–Ω–∞ –æ–± –æ–¥–Ω–æ–ø–æ–ª–æ–º –±—Ä–∞–∫–µ –≤ –ê—Ä–≥–µ–Ω—Ç–∏–Ω–µ ‚Äî 15 –∏—é–ª—è 2014 –≥–æ–¥–∞, –≤ –ë—É—ç–Ω–æ—Å-–ê–π—Ä–µ—Å–µ –¥–≤–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –ª–µ—Å–±–∏—è–Ω–∫–∏ ‚Äî –ú–∞—Ä–∏–Ω–∞ –ú–∏—Ä–æ–Ω–æ–≤–∞ –∏ –û–∫—Å–∞–Ω–∞ –¢–∏–º–æ—Ñ–µ–µ–≤–∞, –≤–æ—Å–ø–∏—Ç—ã–≤–∞—é—â–∏–µ 16-–ª–µ—Ç–Ω–æ–µ–≥–æ —Å—ã–Ω–∞, –ø–æ–∂–µ–Ω–∏–ª–∏—Å—å –∏ –ø—Ä–æ–ø—Ä–æ—Å–∏–ª–∏ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–±–µ–∂–∏—â–∞ –≤ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–µ.\\n–û–± —ç—Ç–æ–º –æ–æ–±—â–∞–µ—Ç –∞–≥–µ–Ω—Ç—Å—Ç–≤–æ EFE —Å–æ —Å—Å—ã–ª–∫–æ–π –Ω–∞ –§–µ–¥–µ—Ä–∞—Ü–∏—é –ª–µ—Å–±–∏—è–Ω–æ–∫, –≥–µ–µ–≤, –±–∏—Å–µ–∫—Å—É–∞–ª–æ–≤ –∏ —Ç—Ä–∞–Ω—Å—Å–µ–∫—Å—É–∞–ª–æ–≤ –ê—Ä–≥–µ–Ω—Ç–∏–Ω—ã (FALGBT).\\n\\n–ß–µ—Ä–µ–∑ —á–µ—Ç—ã—Ä–µ –≥–æ–¥–∞ –ø–æ—Å–ª–µ –ø—Ä–∏–Ω—è—Ç–∏—è –∑–∞–∫–æ–Ω–∞ –æ–± –æ–¥–Ω–æ–ø–æ–ª–æ–º –±—Ä–∞–∫–µ –≤ –ê—Ä–≥–µ–Ω—Ç–∏–Ω–µ, –¥–≤–µ —Ä—É—Å—Å–∫–∏–µ –∂–µ–Ω—â–∏–Ω—ã –∂–µ–Ω–∏–ª–∏—Å—å –≤ –ë—É—ç–Ω–æ—Å-–ê–π—Ä–µ—Å–µ, 15 –∏—é–ª—è 2014 –≥–æ–¥–∞. –ò—Å—Ç–æ—á–Ω–∏–∫ –≤–∏–¥–µ–æ.\\n\\n–ñ–µ–Ω—â–∏–Ω—ã –∑–∞—è–≤–∏–ª–∏, —á—Ç–æ –¥–∞—Ç–∞ —Å–æ–≤–ø–∞–ª–∞ —Å–ª—É—á–∞–π–Ω–æ, –∞ –∏—Ö –≤—ã–±–æ—Ä –ø–∞–ª –Ω–∞ –ê—Ä–≥–µ–Ω—Ç–∏–Ω—É, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ —Ö–æ—Ç—è—Ç –∂–∏—Ç—å ¬´—Å–≤–æ–±–æ–¥–Ω–æ –∏ –≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏¬ª, –∏ —ç—Ç–∞ —Å—Ç—Ä–∞–Ω–∞ –º–æ–∂–µ—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –∏–º —Ç–∞–∫—É—é –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å.\\n\\n–í–∏—Ü–µ-–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç FALGBT –ö–ª–∞—É–¥–∏—è –ö–∞—Å—Ç—Ä–æ—Å–∏–Ω –í–µ—Ä–¥—É, —Å—Ç–∞–≤—à–∞—è —Å–≤–∏–¥–µ—Ç–µ–ª–µ–º –Ω–∞ —Å–≤–∞–¥—å–±–µ, –ø–æ—è—Å–Ω–∏–ª–∞, —á—Ç–æ –ø–∞—Ä–∞ –∏—Å–∫–∞–ª–∞ –ø–æ–º–æ—â–∏ —É –∞—Ä–≥–µ–Ω—Ç–∏–Ω—Å–∫–æ–π —Ñ–µ–¥–µ—Ä–∞—Ü–∏–∏, —Å—Ç—Ä–∞–¥–∞—è –æ—Ç –ø—Ä–µ—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≥–æ–º–æ—Å–µ–∫—Å—É–∞–ª–∏—Å—Ç–æ–≤ –≤ –†–æ—Å—Å–∏–∏, –Ω–∞—á–∞–≤—à–∏—Ö—Å—è –≤ –∫–æ–Ω—Ü–µ 2012 –≥–æ–¥–∞, –ø–æ—Å–ª–µ –ø—Ä–∏–Ω—è—Ç–∏—è –∑–∞–∫–æ–Ω–∞ –æ –∑–∞–ø—Ä–µ—Ç–µ –ø—Ä–æ–ø–∞–≥–∞–Ω–¥—ã –≥–æ–º–æ—Å–µ–∫—Å—É–∞–ª–∏–∑–º–∞ —Å—Ä–µ–¥–∏ –Ω–µ—Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–ª–µ—Ç–Ω–∏—Ö:\\n–£ –Ω–∏—Ö –µ—Å—Ç—å 16-–ª–µ—Ç–Ω–∏–π —Å—ã–Ω, –∫–æ—Ç–æ—Ä–æ–≥–æ –æ–Ω–∏ –Ω–µ —Ö–æ—Ç—è—Ç –ø–æ—Ç–µ—Ä—è—Ç—å.\\n\\n–ö–∞—Å—Ç—Ä–æ—Å–∏–Ω –í–µ—Ä–¥—É —Ç–∞–∫–∂–µ –≤—ã—Ä–∞–∑–∏–ª–∞ –æ–±–µ—Å–ø–æ–∫–æ–µ–Ω–Ω–æ—Å—Ç—å —É—â–µ–º–ª–µ–Ω–∏–µ–º –≤ –†–æ—Å—Å–∏–∏ –ø—Ä–∞–≤ —Å–µ–∫—Å—É–∞–ª—å–Ω—ã—Ö –º–µ–Ω—å—à–∏–Ω—Å—Ç–≤.\\n\\n–ñ–µ–Ω—â–∏–Ω—ã 38 –∏ 36 –ª–µ—Ç –ø—Ä–∏–µ—Ö–∞–ª–∏ –≤ –ê—Ä–≥–µ–Ω—Ç–∏–Ω—É –º–µ—Å—è—Ü –Ω–∞–∑–∞–¥ –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ —É–∑–Ω–∞–ª–∏ –æ —è–∫–æ–±—ã –≥–æ—Ç–æ–≤—è—â–µ–º—Å—è –∑–∞–∫–æ–Ω–µ, –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É —É –æ–¥–Ω–æ–ø–æ–ª—ã—Ö –ø–∞—Ä —Å–º–æ–≥—É—Ç –æ—Ç–±–∏—Ä–∞—Ç—å –¥–µ—Ç–µ–π.\\n–°–ú–ò –æ—Ç–º–µ—á–∞—é—Ç, —á—Ç–æ –æ–Ω–∏ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –±–µ–∂–∞–ª–∏ –∏–∑ –†–æ—Å—Å–∏–∏ –≤ –ê—Ä–≥–µ–Ω—Ç–∏–Ω—É, –Ω–µ –∑–Ω–∞—è –≤ –¥–æ–ª–∂–Ω–æ–π –º–µ—Ä–µ –Ω–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∏ –∏—Å–ø–∞–Ω—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.\\n\\n–ú–∞—Ä–∏–Ω–∞ –ú–∏—Ä–æ–Ω–æ–≤–∞ (–º–∞—Ç—å 16-–ª–µ—Ç–Ω–µ–≥–æ –ø–∞—Ä–Ω—è) —Ç–∞–∫–∂–µ —Å–æ–æ–±—â–∏–ª–∞ –∏–∑–¥–∞–Ω–∏—é T√©lam, —á—Ç–æ –≤ –†–æ—Å—Å–∏–∏ –æ–Ω–∞ —Ä–∞–±–æ—Ç–∞–ª–∞ —É—á–∏—Ç–µ–ª–µ–º –∏ –±—ã–ª–∞ —É–≤–æ–ª–µ–Ω–∞, –ø–æ—Å–ª–µ —Ç–æ–≥–æ –∫–∞–∫ –≤ —à–∫–æ–ª–µ —É–∑–Ω–∞–ª–∏ –æ —Ç–æ–º, —á—Ç–æ –æ–Ω–∞ –∂–∏–≤—ë—Ç —Å –∂–µ–Ω—â–∏–Ω–æ–π.\\n\\n–° –∫–æ–Ω—Ü–∞ 2012 –≥–æ–¥–∞, –∫–æ–≥–¥–∞ –≤ –†–æ—Å—Å–∏–∏ –±—ã–ª –ø—Ä–∏–Ω—è—Ç ¬´–∞–Ω—Ç–∏–≥–µ–π—Å–∫–∏–π¬ª –∑–∞–∫–æ–Ω, –¥–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ –ª–∏—Ü ¬´–Ω–µ—Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–π¬ª –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ —Ç–æ–ª—å–∫–æ —É—Å–∏–ª–∏–≤–∞–µ—Ç—Å—è.\\n\\n–ò–º –∑–∞–ø—Ä–µ—â–∞—é—Ç –ø—Ä–æ–≤–æ–¥–∏—Ç—å –∞–∫—Ü–∏–∏ –∏ —à–µ—Å—Ç–≤–∏—è –≤ –∑–∞—â–∏—Ç—É —Å–≤–æ–∏—Ö –ø—Ä–∞–≤.\\n\\n–î–∏–∫—Ç–æ—Ä—ã —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ —Ç–µ–ª–µ–≤–∏–¥–µ–Ω–∏—è –¥–æ–ø—É—Å–∫–∞—é—Ç –æ—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –ø—Ä–æ–≤–æ–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏—è –≤ –∞–¥—Ä–µ—Å –õ–ì–ë–¢-—Å–æ–æ–±—â–µ—Å—Ç–≤–∞, –ø—Ä–µ–¥–ª–∞–≥–∞—è —Å–∂–∏–≥–∞—Ç—å –≤ –ø–µ—á–∞—Ö ¬´—Å–µ—Ä–¥—Ü–∞ –≥–µ–µ–≤¬ª –∏ –∏—Ö —Å–∞–º–∏—Ö.\\n\\n–í —Å–µ–Ω—Ç—è–±—Ä–µ 2013 –≥–æ–¥–∞ –¥–µ–ø—É—Ç–∞—Ç –ì–æ—Å–¥—É–º—ã –†–§ –æ—Ç –ï–¥–∏–Ω–æ–π –†–æ—Å—Å–∏–∏, –ø—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—å –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–π –ø–∞—Ä—Ç–∏–∏ ¬´–†–æ–¥–∏–Ω–∞¬ª –ê–ª–µ–∫—Å–µ–π –ñ—É—Ä–∞–≤–ª—ë–≤ –≤—ã—Å—Ç—É–ø–∏–ª –∏–Ω–∏—Ü–∏–∞—Ç–æ—Ä–æ–º –∑–∞–∫–æ–Ω–æ–ø—Ä–æ–µ–∫—Ç–∞ –æ –ª–∏—à–µ–Ω–∏–∏ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–∞–≤ –ª–∏—Ü –Ω–µ—Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–π —Å–µ–∫—Å—É–∞–ª—å–Ω–æ–π –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏.\\n–î–æ–∫–ª–∞–¥—á–∏–∫ –ü–ê–°–ï –ø–æ –ø—Ä–∞–≤–∞–º –õ–ì–ë–¢ –•. –•–∞—É–≥–ª–∏ —Ä–∞—Å—Ü–µ–Ω–∏–ª —ç—Ç—É –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤—É –∫–∞–∫ –ø—Ä–æ—è–≤–ª–µ–Ω–∏–µ –≥–æ–º–æ—Ñ–æ–±–Ω–æ–π –ø—Ä–æ–ø–∞–≥–∞–Ω–¥—ã, –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∞—â–µ–µ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–º –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º –†–æ—Å—Å–∏–∏.\\n</td>\n",
       "      <td>[–î–≤–µ, –ª–µ—Å–±–∏—è–Ω–∫–∏, —Å–æ, –≤–∑—Ä–æ—Å–ª—ã–º, —Å—ã–Ω–æ–º, –∏–∑, –†–æ—Å—Å–∏–∏, –ø–æ–∂–µ–Ω–∏–ª–∏—Å—å, –∏, –ø–æ–ø—Ä–æ—Å–∏–ª–∏, –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ, —É–±–µ–∂–∏—â–∞, –≤, –ê—Ä–≥–µ–Ω—Ç–∏–Ω–µ, –í, –¥–µ–Ω—å, —á–µ—Ç—ã—Ä—ë—Ö–ª–µ—Ç–∏—è, –ø—Ä–∏–Ω—è—Ç–∏—è, –∑–∞–∫–æ–Ω–∞, –∑–∞–∫–æ–Ω–∞, –æ–±, –æ–¥–Ω–æ–ø–æ–ª–æ–º, –±—Ä–∞–∫–µ, –≤, –ê—Ä–≥–µ–Ω—Ç–∏–Ω–µ, ‚Äî, 15, –∏—é–ª—è, 2014, –≥–æ–¥–∞, ,, –≤, –ë—É—ç–Ω–æ—Å-–ê–π—Ä–µ—Å–µ, –¥–≤–µ, —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ, –ª–µ—Å–±–∏—è–Ω–∫–∏, ‚Äî, –ú–∞—Ä–∏–Ω–∞, –ú–∏—Ä–æ–Ω–æ–≤–∞, –∏, –û–∫—Å–∞–Ω–∞, –¢–∏–º–æ—Ñ–µ–µ–≤–∞, ,, –≤–æ—Å–ø–∏—Ç—ã–≤–∞—é—â–∏–µ, 16-–ª–µ—Ç–Ω–æ–µ–≥–æ, —Å—ã–Ω–∞,, –ø–æ–∂–µ–Ω–∏–ª–∏—Å—å, –∏, –ø—Ä–æ–ø—Ä–æ—Å–∏–ª–∏, –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ, —É–±–µ–∂–∏—â–∞, –≤, —ç—Ç–æ–π, —Å—Ç—Ä–∞–Ω–µ., –û–±, —ç—Ç–æ–º, –æ–æ–±—â–∞–µ—Ç, –∞–≥–µ–Ω—Ç—Å—Ç–≤–æ, EFE, —Å–æ, —Å—Å—ã–ª–∫–æ–π, –Ω–∞, –§–µ–¥–µ—Ä–∞—Ü–∏—é, –ª–µ—Å–±–∏—è–Ω–æ–∫,, –≥–µ–µ–≤,, –±–∏—Å–µ–∫—Å—É–∞–ª–æ–≤, –∏, —Ç—Ä–∞–Ω—Å—Å–µ–∫—Å—É–∞–ª–æ–≤, –ê—Ä–≥–µ–Ω—Ç–∏–Ω—ã, –ê—Ä–≥–µ–Ω—Ç–∏–Ω—ã, (, FALGBT, )., –ß–µ—Ä–µ–∑, —á–µ—Ç—ã—Ä–µ, –≥–æ–¥–∞, –ø–æ—Å–ª–µ, –ø—Ä–∏–Ω—è—Ç–∏—è, –∑–∞–∫–æ–Ω–∞, –æ–±, –æ–¥–Ω–æ–ø–æ–ª–æ–º, –±—Ä–∞–∫–µ, –∑–∞–∫–æ–Ω–∞, –æ–±, –æ–¥–Ω–æ–ø–æ–ª–æ–º, –±—Ä–∞–∫–µ, –≤, –ê—Ä–≥–µ–Ω—Ç–∏–Ω–µ, ,, –¥–≤–µ, —Ä—É—Å—Å–∫–∏–µ, –∂–µ–Ω—â–∏–Ω—ã, –∂–µ–Ω–∏–ª–∏—Å—å, –≤, –ë—É—ç–Ω–æ—Å-–ê–π—Ä–µ—Å–µ, ,, 15, –∏—é–ª—è, 2014, –≥–æ–¥–∞, ...]</td>\n",
       "      <td>[B-NUMBER, B-EVENT, O, O, O, O, B-COUNTRY, O, O, O, O, O, O, B-COUNTRY, B-DATE, I-DATE, B-AGE, B-EVENT, I-EVENT, B-LAW, I-LAW, I-LAW, I-LAW, O, B-COUNTRY, O, B-DATE, I-DATE, I-DATE, I-DATE, O, O, B-CITY, B-NUMBER, B-NATIONALITY, O, O, B-PERSON, I-PERSON, O, B-PERSON, I-PERSON, O, O, B-AGE, O, B-EVENT, O, B-EVENT, I-EVENT, I-EVENT, O, O, O, O, O, O, O, B-ORGANIZATION, O, O, O, B-ORGANIZATION, I-ORGANIZATION, I-ORGANIZATION, I-ORGANIZATION, I-ORGANIZATION, I-ORGANIZATION, I-ORGANIZATION, B-COUNTRY, O, B-ORGANIZATION, O, B-DATE, I-DATE, I-DATE, O, B-EVENT, I-EVENT, I-EVENT, I-EVENT, I-EVENT, B-LAW, I-LAW, I-LAW, I-LAW, O, B-COUNTRY, O, B-NUMBER, B-NATIONALITY, O, B-EVENT, O, B-CITY, O, B-DATE, I-DATE, I-DATE, I-DATE, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–í –ü–æ–ª—å—à–µ –Ω–∞–π–¥–µ–Ω –º—ë—Ä—Ç–≤—ã–º —Å—ã–Ω —ç–∫—Å-–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –õ–µ—Ö–∞ –í–∞–ª–µ–Ω—Å—ã\\n–ü–æ–ª–∏—Ü–µ–π—Å–∫–∏–π –∞–≤—Ç–æ–º–æ–±–∏–ª—å –≤ –ì–¥–∞–Ω—å—Å–∫–µ\\n–í –ì–¥–∞–Ω—å—Å–∫–µ –Ω–∞–π–¥–µ–Ω –º—ë—Ä—Ç–≤—ã–º –≤ —Å–≤–æ–µ–π –∫–≤–∞—Ä—Ç–∏—Ä–µ —Å—ã–Ω —ç–∫—Å-–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –ü–æ–ª—å—à–∏ –õ–µ—Ö–∞ –í–∞–ª–µ–Ω—Å—ã.\\n\\n–ü–æ –¥–∞–Ω–Ω—ã–º –ø–æ–ª–∏—Ü–∏–∏, —Ç–µ–ª–æ 43-–ª–µ—Ç–Ω–µ–≥–æ –ü—à–µ–º—ã—Å–ª–∞–≤–∞ –í–∞–ª–µ–Ω—Å—ã –æ–±–Ω–∞—Ä—É–∂–∏–ª –µ–≥–æ —Å—ã–Ω –≤ –∫–≤–∞—Ä—Ç–∏—Ä–µ, –≥–¥–µ –∂–∏–ª —Å–∫–æ–Ω—á–∞–≤—à–∏–π—Å—è.\\n–û–Ω –∂–µ –∏ –≤—ã–∑–≤–∞–ª —Å–∫–æ—Ä—É—é –ø–æ–º–æ—â—å.\\n–°–µ–π—á–∞—Å —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –ø—Ä–∏—á–∏–Ω–∞ —Å–º–µ—Ä—Ç–∏ –º—É–∂—á–∏–Ω—ã.\\n\\n–ü–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–º –¥–∞–Ω–Ω—ã–º, —Ç–µ–ª–µ—Å–Ω—ã—Ö –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –Ω–∞ –µ–≥–æ —Ç–µ–ª–µ –Ω–µ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–æ.\\n–ò–∑–≤–µ—Å—Ç–Ω–æ, —á—Ç–æ —Ä–∞–Ω–µ–µ —É–º–µ—Ä—à–∏–π —Å—ã–Ω –±—ã–≤—à–µ–≥–æ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –Ω–µ–æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω–æ —Å—Ç–∞–Ω–æ–≤–∏–ª—Å—è –≤–∏–Ω–æ–≤–Ω–∏–∫–æ–º –î–¢–ü –∏–∑-–∑–∞ –≤–æ–∂–¥–µ–Ω–∏—è –≤ –Ω–µ—Ç—Ä–µ–∑–≤–æ–º –≤–∏–¥–µ (–≤ 1993 –∏ 2003 –≥–æ–¥–∞—Ö).\\n\\n–í—Å–µ–≥–æ —É –õ–µ—Ö–∞ –í–∞–ª–µ–Ω—Å—ã –∏ –µ–≥–æ –∂–µ–Ω—ã –≤–æ—Å–µ–º—å –¥–µ—Ç–µ–π ‚Äî —á–µ—Ç–≤–µ—Ä–æ —Å—ã–Ω–æ–≤–µ–π –∏ —á–µ—Ç—ã—Ä–µ –¥–æ—á–µ—Ä–∏.\\n–ü—à–µ–º—ã—Å–ª–∞–≤ –±—ã–ª —Ç—Ä–µ—Ç—å–∏–º —Ä–µ–±–µ–Ω–∫–æ–º –≤ —Å–µ–º—å–µ, –æ–Ω —Ä–æ–¥–∏–ª—Å—è –≤ 1974 –≥–æ–¥—É.\\n–£ –Ω–µ–≥–æ —É —Å–∞–º–æ–≥–æ –¥–≤–∞ —Å—ã–Ω–∞.\\n–û–¥–∏–Ω –∏–∑ –Ω–∏—Ö ‚Äî –î–æ–º–∏–Ω–∏–∫ ‚Äî –ø–æ–ø–∞–¥–∞–ª –≤ —Å–∫–∞–Ω–¥–∞–ª: —Ä–∞–Ω–∏–ª –Ω–æ–∂–æ–º —Å–≤–æ—é –¥–µ–≤—É—à–∫—É.\\n</td>\n",
       "      <td>[–í, –ü–æ–ª—å—à–µ, –Ω–∞–π–¥–µ–Ω, –º—ë—Ä—Ç–≤—ã–º, —Å—ã–Ω, —ç–∫—Å-, –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞, –õ–µ—Ö–∞, –í–∞–ª–µ–Ω—Å—ã, –ü–æ–ª–∏—Ü–µ–π—Å–∫–∏–π, –∞–≤—Ç–æ–º–æ–±–∏–ª—å, –≤, –ì–¥–∞–Ω—å—Å–∫–µ, –í, –ì–¥–∞–Ω—å—Å–∫–µ, –Ω–∞–π–¥–µ–Ω, –º—ë—Ä—Ç–≤—ã–º, –≤, —Å–≤–æ–µ–π, –∫–≤–∞—Ä—Ç–∏—Ä–µ, —Å—ã–Ω, —ç–∫—Å-, –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞, –ü–æ–ª—å—à–∏, –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞, –ü–æ–ª—å—à–∏, –õ–µ—Ö–∞, –í–∞–ª–µ–Ω—Å—ã, ., –ü–æ, –¥–∞–Ω–Ω—ã–º, –ø–æ–ª–∏—Ü–∏–∏,, —Ç–µ–ª–æ, 43-–ª–µ—Ç–Ω–µ–≥–æ, –ü—à–µ–º—ã—Å–ª–∞–≤–∞, –í–∞–ª–µ–Ω—Å—ã, –æ–±–Ω–∞—Ä—É–∂–∏–ª, –µ–≥–æ, —Å—ã–Ω, –≤, –∫–≤–∞—Ä—Ç–∏—Ä–µ,, –≥–¥–µ, –∂–∏–ª, —Å–∫–æ–Ω—á–∞–≤—à–∏–π—Å—è., –û–Ω, –∂–µ, –∏, –≤—ã–∑–≤–∞–ª, —Å–∫–æ—Ä—É—é, –ø–æ–º–æ—â—å., –°–µ–π—á–∞—Å, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è, –ø—Ä–∏—á–∏–Ω–∞, —Å–º–µ—Ä—Ç–∏, –º—É–∂—á–∏–Ω—ã., –ü–æ, –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–º, –¥–∞–Ω–Ω—ã–º,, —Ç–µ–ª–µ—Å–Ω—ã—Ö, –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π, –Ω–∞, –µ–≥–æ, —Ç–µ–ª–µ, –Ω–µ, –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–æ., –ò–∑–≤–µ—Å—Ç–Ω–æ,, —á—Ç–æ, —Ä–∞–Ω–µ–µ, —É–º–µ—Ä—à–∏–π, —Å—ã–Ω, –±—ã–≤—à–µ–≥–æ, –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞, –Ω–µ–æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω–æ, —Å—Ç–∞–Ω–æ–≤–∏–ª—Å—è, –≤–∏–Ω–æ–≤–Ω–∏–∫–æ–º, –î–¢–ü, –∏–∑-–∑–∞, –≤–æ–∂–¥–µ–Ω–∏—è, –≤, –Ω–µ—Ç—Ä–µ–∑–≤–æ–º, –≤–∏–¥–µ, (, –≤, 1993, –∏, 2003, –≥–æ–¥–∞—Ö, )., –í—Å–µ–≥–æ, —É, –õ–µ—Ö–∞, –í–∞–ª–µ–Ω—Å—ã, –∏, –µ–≥–æ, –∂–µ–Ω—ã, –≤–æ—Å–µ–º—å, –¥–µ—Ç–µ–π, ‚Äî, —á–µ—Ç–≤–µ—Ä–æ, —Å—ã–Ω–æ–≤–µ–π, ...]</td>\n",
       "      <td>[O, B-COUNTRY, B-EVENT, I-EVENT, O, O, B-PROFESSION, B-PERSON, I-PERSON, O, O, O, B-CITY, O, B-CITY, B-EVENT, I-EVENT, O, O, O, O, O, B-PROFESSION, I-PROFESSION, B-PROFESSION, B-COUNTRY, B-PERSON, I-PERSON, O, O, O, O, O, B-AGE, B-PERSON, I-PERSON, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-PROFESSION, O, O, O, O, O, O, O, O, O, O, B-DATE, I-DATE, O, B-DATE, I-DATE, O, O, O, B-PERSON, I-PERSON, O, O, O, B-NUMBER, O, O, B-NUMBER, O, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–° –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∞–ª—å–±–æ–º–∞ Rammstein —Å–Ω—è—Ç—ã –≤—Å–µ –∑–∞–ø—Ä–µ—Ç—ã\\n\\n\\n–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π —Å—É–¥ –ö—ë–ª—å–Ω–∞ —Å–Ω—è–ª —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∞–ª—å–±–æ–º–∞ –Ω–µ–º–µ—Ü–∫–æ–π –∏–Ω–¥–∞—Å—Ç—Ä–∏–∞–ª-–º–µ—Ç–∞–ª –≥—Ä—É–ø–ø—ã Rammstein ¬´Liebe ist f√ºr alle da¬ª –≤—Å–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é. –°—É–¥ —Å—á—ë–ª –Ω–µ–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –∑–∞–ø—Ä–µ—Ç –Ω–∞ –ø—Ä–æ–¥–∞–∂—É –∞–ª—å–±–æ–º–∞ –ª–∏—Ü–∞–º –¥–æ –≤–æ—Å–µ–º–Ω–∞–¥—Ü–∞—Ç–∏ –ª–µ—Ç –∏ —Ü–µ–Ω–∑—É—Ä—É –æ–±–ª–æ–∂–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –≤–≤–µ–¥–µ–Ω—ã –æ—Å–µ–Ω—å—é –ø—Ä–æ—à–ª–æ–≥–æ –≥–æ–¥–∞.\\n–¢–∏–ª–ª—å –õ–∏–Ω–¥–µ–º–∞–Ω–Ω —Å –æ–≥–Ω–µ–º—ë—Ç–æ–º –Ω–∞ —Å—Ü–µ–Ω–µ.\\n\\n¬´Liebe ist f√ºr alle da¬ª –ø–æ–¥–≤–µ—Ä–≥—Å—è —Ü–µ–Ω–∑—É—Ä–µ –≤ –Ω–æ—è–±—Ä–µ 2009 –≥–æ–¥–∞, –∫–æ–≥–¥–∞ –ù–µ–º–µ—Ü–∫–∏–º –§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–º –ö–æ–º–∏—Ç–µ—Ç–æ–º –ø–æ –û—Ü–µ–Ω–∫–µ –°–ú–ò –æ–±–ª–æ–∂–∫–∞ –¥–∏—Å–∫–∞ –±—ã–ª–∞ –Ω–∞–∑–≤–∞–Ω–∞ –ø—Ä–æ–ø–∞–≥–∞–Ω–¥–∏—Ä—É—é—â–µ–π —Å–∞–¥–æ–º–∞–∑–æ—Ö–∏–∑–º. –ö —Ç–æ–º—É –∂–µ –Ω–µ–ø—Ä–∏–ª–∏—á–Ω—ã–º –±—ã–ª –æ–±—ä—è–≤–ª–µ–Ω —Ç–µ–∫—Å—Ç –ø–µ—Å–Ω–∏ ¬´Ich tu dir weh¬ª, –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —á–µ–≥–æ –∫–æ–º–ø–æ–∑–∏—Ü–∏—é –∑–∞–ø—Ä–µ—Ç–∏–ª–∏ –¥–ª—è –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–∞ –ø—É–±–ª–∏–∫–µ.\\n\\n–°—É–¥ –ö—ë–ª—å–Ω–∞ –≤—ã–Ω–æ—Å–∏–ª –≤–µ—Ä–¥–∏–∫—Ç, —Å—Å—ã–ª–∞—è—Å—å –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≤ –ø–µ—Å–Ω—è—Ö –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π –Ω–∞—Å–∏–ª—å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞. –¢–∞–∫–∂–µ –¥–æ–≤–æ–¥–æ–º –≤ –ø–æ–ª—å–∑—É —Å–Ω—è—Ç–∏—è –∑–∞–ø—Ä–µ—Ç–Ω—ã—Ö –º–µ—Ä —Å—Ç–∞–ª–æ —Ç–æ, —á—Ç–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Ä–µ—à–µ–Ω–∏–µ –Ω–µ –ø–æ—è—Å–Ω—è–ª–æ –ø—Ä–∏—á–∏–Ω –ø–∞–≥—É–±–Ω–æ–≥–æ –≤–ª–∏—è–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏—è —Å–∞–¥–æ–º–∞–∑–æ—Ö–∏–∑–º–∞ –Ω–∞ –º–æ–ª–æ–¥—ë–∂—å. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—É–¥–∞ —Ä–∞–∑—Ä–µ—à–∞–µ—Ç —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø–ª–∞—Å—Ç–∏–Ω–∫–∏ –≤ –ø—Ä–æ–∑—Ä–∞—á–Ω–æ–π —É–ø–∞–∫–æ–≤–∫–µ –∏ –¥–∞—ë—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–æ–∫—É–ø–∞—Ç—å –µ—ë –¥–µ—Ç—è–º –º–ª–∞–¥—à–µ –≤–æ—Å–µ–º–Ω–∞–¥—Ü–∞—Ç–∏ –ª–µ—Ç.\\n\\n–¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –≤–µ—Ä–¥–∏–∫—Ç –Ω–æ—Å–∏—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä. –≠—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å —Ç–µ–º, —á—Ç–æ –ö–æ–º–∏—Ç–µ—Ç –ø–æ –û—Ü–µ–Ω–∫–µ –°–ú–ò —Ä–µ—à–∏–ª –æ–ø—Ä–æ—Ç–µ—Å—Ç–æ–≤–∞—Ç—å —Ä–µ—à–µ–Ω–∏–µ –ö—ë–ª—å–Ω—Å–∫–æ–≥–æ —Å—É–¥–∞. –ü–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–º –æ—Ü–µ–Ω–∫–∞–º –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å –º–æ–∂–µ—Ç —É–π—Ç–∏ –Ω–µ –º–µ–Ω–µ–µ –ø–æ–ª—É–≥–æ–¥–∞. –í —ç—Ç–æ—Ç –ø–µ—Ä–∏–æ–¥ –ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è –ø—Ä–æ–¥–∞–≤–∞—Ç—å –∞–ª—å–±–æ–º –≤ –æ–±—ã—á–Ω–æ–º —Ä–µ–∂–∏–º–µ, —Ç.–µ –±–µ–∑ –∫–∞–∫–∏—Ö –±—ã —Ç–æ –Ω–∏ –±—ã–ª–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π.\\n\\n–ö—Ä–æ–º–µ —Ç–æ–≥–æ, —Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –≤ —Ñ–µ–≤—Ä–∞–ª–µ 2010 –≥–æ–¥–∞ Rammstein —Å—Ç–∞–ª–∏ –æ–±—ä–µ–∫—Ç–æ–º –∂—ë—Å—Ç–∫–æ–π –∫—Ä–∏—Ç–∏–∫–∏ —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã –±–µ–ª–æ—Ä—É—Å—Å–∫–∏—Ö –≤–ª–∞—Å—Ç–µ–π. –¢–æ–≥–¥–∞ –≤ –ø—Ä–µ–¥–¥–≤–µ—Ä–∏–∏ –º–∏–Ω—Å–∫–æ–≥–æ –∫–æ–Ω—Ü–µ—Ä—Ç–∞ –≥—Ä—É–ø–ø—ã –û–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Å–æ–≤–µ—Ç –ø–æ –Ω—Ä–∞–≤—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∫—É–ª—å—Ç—É—Ä—ã –ë–µ–ª–æ—Ä—É—Å—Å–∏–∏ –∑–∞–ø–æ–¥–æ–∑—Ä–∏–ª–∏ –Ω–µ–º–µ—Ü–∫–∏—Ö –º–µ—Ç–∞–ª–ª–∏—Å—Ç–æ–≤ –≤ –ø—Ä–æ–ø–∞–≥–∞–Ω–¥–µ –Ω–∞—Å–∏–ª–∏—è, —Å–µ–∫—Å—É–∞–ª—å–Ω—ã—Ö –∏–∑–≤—Ä–∞—â–µ–Ω–∏–π –∏ –Ω–∞—Ü–∏–∑–º–∞.\\n\\n–¢–µ–º –Ω–µ –º–µ–Ω–µ–µ —Ä–∞–Ω–Ω–µ–µ —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ Rammstein –Ω–µ –≤—ã–∑—ã–≤–∞–ª–æ —Å—Ç–æ–ª—å –∫—Ä–∏—Ç–∏—á–Ω–æ–≥–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—è —Å–æ —Å—Ç–æ—Ä–æ–Ω—ã –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–æ–≤, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ –≤ –Ω—ë–º —Ç–µ–º—ã —Å–µ–∫—Å—É–∞–ª—å–Ω–æ–≥–æ –Ω–∞—Å–∏–ª–∏—è.\\n</td>\n",
       "      <td>[–°, –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ, –∞–ª—å–±–æ–º–∞, Rammstein, —Å–Ω—è—Ç—ã, –≤—Å–µ, –∑–∞–ø—Ä–µ—Ç—ã, –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–π, —Å—É–¥, –ö—ë–ª—å–Ω–∞, –ö—ë–ª—å–Ω–∞, —Å–Ω—è–ª, —Å, –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ, –∞–ª—å–±–æ–º–∞, –Ω–µ–º–µ—Ü–∫–æ–π, –∏–Ω–¥–∞—Å—Ç—Ä–∏–∞–ª-–º–µ—Ç–∞–ª, –≥—Ä—É–ø–ø—ã, Rammstein, ¬´, Liebe, ist, f√ºr, alle, da, ¬ª, –≤—Å–µ, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è, –Ω–∞, —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é, ., –°—É–¥, —Å—á—ë–ª, –Ω–µ–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º–∏, –∑–∞–ø—Ä–µ—Ç, –Ω–∞, –ø—Ä–æ–¥–∞–∂—É, –∞–ª—å–±–æ–º–∞, –ª–∏—Ü–∞–º, –¥–æ, –≤–æ—Å–µ–º–Ω–∞–¥—Ü–∞—Ç–∏, –ª–µ—Ç, –∏, —Ü–µ–Ω–∑—É—Ä—É, –æ–±–ª–æ–∂–∫–∏,, –∫–æ—Ç–æ—Ä—ã–µ, –±—ã–ª–∏, –≤–≤–µ–¥–µ–Ω—ã, –æ—Å–µ–Ω—å—é, –ø—Ä–æ—à–ª–æ–≥–æ, –≥–æ–¥–∞, ., –¢–∏–ª–ª—å, –õ–∏–Ω–¥–µ–º–∞–Ω–Ω, —Å, –æ–≥–Ω–µ–º—ë—Ç–æ–º, –Ω–∞, —Å—Ü–µ–Ω–µ., ¬´, Liebe, ist, f√ºr, alle, da, ¬ª, –ø–æ–¥–≤–µ—Ä–≥—Å—è, —Ü–µ–Ω–∑—É—Ä–µ, –≤, –Ω–æ—è–±—Ä–µ, 2009, –≥–æ–¥–∞, ,, –∫–æ–≥–¥–∞, –ù–µ–º–µ—Ü–∫–∏–º, –§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–º, –ö–æ–º–∏—Ç–µ—Ç–æ–º, –ø–æ, –û—Ü–µ–Ω–∫–µ, –°–ú–ò, –ù–µ–º–µ—Ü–∫–∏–º, –§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–º, –ö–æ–º–∏—Ç–µ—Ç–æ–º, –ø–æ, –û—Ü–µ–Ω–∫–µ, –°–ú–ò, –æ–±–ª–æ–∂–∫–∞, –¥–∏—Å–∫–∞, –±—ã–ª–∞, –Ω–∞–∑–≤–∞–Ω–∞, –ø—Ä–æ–ø–∞–≥–∞–Ω–¥–∏—Ä—É—é—â–µ–π, —Å–∞–¥–æ–º–∞–∑–æ—Ö–∏–∑–º, —Å–∞–¥–æ–º–∞–∑–æ—Ö–∏–∑–º, ., –ö, —Ç–æ–º—É, –∂–µ, –Ω–µ–ø—Ä–∏–ª–∏—á–Ω—ã–º, –±—ã–ª, –æ–±—ä—è–≤–ª–µ–Ω, —Ç–µ–∫—Å—Ç, ...]</td>\n",
       "      <td>[O, O, O, B-ORGANIZATION, O, O, O, B-ORGANIZATION, I-ORGANIZATION, I-ORGANIZATION, B-CITY, O, O, O, O, B-COUNTRY, O, O, B-ORGANIZATION, O, B-WORK_OF_ART, I-WORK_OF_ART, I-WORK_OF_ART, I-WORK_OF_ART, I-WORK_OF_ART, O, O, B-EVENT, I-EVENT, I-EVENT, O, O, O, O, O, O, O, O, O, B-AGE, I-AGE, I-AGE, O, O, O, O, O, O, B-DATE, I-DATE, I-DATE, O, B-PERSON, I-PERSON, O, O, O, O, O, B-WORK_OF_ART, I-WORK_OF_ART, I-WORK_OF_ART, I-WORK_OF_ART, I-WORK_OF_ART, O, B-EVENT, I-EVENT, B-DATE, I-DATE, I-DATE, I-DATE, O, O, B-ORGANIZATION, I-ORGANIZATION, I-ORGANIZATION, I-ORGANIZATION, I-ORGANIZATION, I-ORGANIZATION, B-COUNTRY, O, O, O, O, O, O, O, O, O, B-CRIME, I-CRIME, B-IDEOLOGY, O, O, O, O, O, O, O, O, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–≠–∫—Å-–≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä –ú–∏–Ω–Ω–µ—Å–æ—Ç—ã –º–µ—Ç–∏—Ç –≤ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç—ã\\n\\n''–£ –ë–∞—Ä–∞–∫–∞ –û–±–∞–º—ã –ø–æ—è–≤–∏–ª—Å—è –µ—â–µ –æ–¥–∏–Ω —Å–æ–ø–µ—Ä–Ω–∏–∫ –Ω–∞ –ø—Ä–µ–¥—Å—Ç–æ—è—â–∏—Ö –≤—ã–±–æ—Ä–∞—Ö –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –°–æ–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö —à—Ç–∞—Ç–æ–≤''\\n\\n–ë—ã–≤—à–∏–π –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä —à—Ç–∞—Ç–∞ –ú–∏–Ω–Ω–µ—Å–æ—Ç–∞ —á–ª–µ–Ω –†–µ—Å–ø—É–±–ª–∏–∫–∞–Ω—Å–∫–æ–π –ø–∞—Ä—Ç–∏–∏ –°–®–ê –¢–∏–º –ü–æ–ª–µ–Ω—Ç–∏, –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ –∑–∞—è–≤–∏–ª –æ —Å–≤–æ–µ–º –≤—Å—Ç—É–ø–ª–µ–Ω–∏–∏ –≤ –±–æ—Ä—å–±—É –∑–∞ —Ç–æ, —á—Ç–æ–±—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å –†–µ—Å–ø—É–±–ª–∏–∫–∞–Ω—Å–∫—É—é –ø–∞—Ä—Ç–∏—é –Ω–∞ –ø—Ä–µ–¥—Å—Ç–æ—è—â–∏—Ö –≤—ã–±–æ—Ä–∞—Ö –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –°–®–ê.\\n\\n¬´–ù–∞—à–µ–π —Å—Ç—Ä–∞–Ω–µ –Ω—É–∂–Ω–æ –Ω–æ–≤–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ. –ù–∞–º –Ω—É–∂–Ω–æ –≤–Ω–æ–≤—å –∑–∞—Å—Ç–∞–≤–∏—Ç—å —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞—à—É —ç–∫–æ–Ω–æ–º–∏–∫—É. –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —É –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –û–±–∞–º—ã –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –º—É–∂–µ—Å—Ç–≤–∞ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ø—Ä–æ—Å—Ç—ã–º –∞–º–µ—Ä–∏–∫–∞–Ω—Ü–∞–º –≤ –≥–ª–∞–∑–∞ –∏ —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å –∏–º –≥–æ—Ä—å–∫—É—é –ø—Ä–∞–≤–¥—É –æ —Ç–æ–º, —á—Ç–æ –º—ã –¥–æ–ª–∂–Ω—ã –±—É–¥–µ–º —Å–¥–µ–ª–∞—Ç—å –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –æ–±—É–∑–¥–∞—Ç—å –Ω–∞—à–∏ —Ä–∞—Å—Ö–æ–¥—ã. –ê —è —ç—Ç–æ —Å–¥–µ–ª–∞—é¬ª, ‚Äî –∑–∞—è–≤–∏–ª –¢–∏–º –ü–æ–ª–µ–Ω—Ç–∏.\\n\\n–ü–æ–ª–µ–Ω—Ç–∏ 50 –ª–µ—Ç, –≤ –ø–µ—Ä–∏–æ–¥ —Å 2003 –ø–æ 2011 –≥–æ–¥—ã –æ–Ω –∑–∞–Ω–∏–º–∞–ª –ø–æ—Å—Ç –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä–∞ –ú–∏–Ω–Ω–µ—Å–æ—Ç—ã, –æ–Ω –Ω–µ –∏–º–µ–µ—Ç –±–æ–ª—å—à–æ–π –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç–∏ –≤ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–π —Å—Ä–µ–¥–µ. –ù–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø–æ–ª–∞–≥–∞—é—Ç —á—Ç–æ —É –ü–æ–ª–µ–Ω—Ç–∏ –µ—Å—Ç—å –≤—Å–µ —à–∞–Ω—Å—ã —Å—Ç–∞—Ç—å –µ–¥–∏–Ω—ã–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–º –æ—Ç –†–µ—Å–ø—É–±–ª–∏–∫–∞–Ω—Å–∫–æ–π –ø–∞—Ä—Ç–∏–∏. –°–µ–π—á–∞—Å —Ñ–∞–≤–æ—Ä–∏—Ç–æ–º –ø—Ä–µ–¥–≤—ã–±–æ—Ä–Ω–æ–π –≥–æ–Ω–∫–∏ –æ—Ç –µ–≥–æ –ø–∞—Ä—Ç–∏–∏ –æ—Å—Ç–∞–µ—Ç—Å—è –ú–∏—Ç—Ç –†–æ–º–Ω–∏.\\n\\n–û –ø–µ—Ä–≤–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –ü–æ–ª–µ–Ω—Ç–∏ —Ä–∞—Å—Å–∫–∞–∑–∞–ª –Ω–∞ –Ω–∞ —Ç–µ–ª–µ–∫–∞–Ω–∞–ª–µ NBC –≤ –ø—Ä–æ–≥—Ä–∞–º–º–µ ¬´–°–µ–≥–æ–¥–Ω—è¬ª\\n</td>\n",
       "      <td>[–≠–∫—Å-, –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä, –ú–∏–Ω–Ω–µ—Å–æ—Ç—ã, –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä, –ú–∏–Ω–Ω–µ—Å–æ—Ç—ã, –º–µ—Ç–∏—Ç, –≤, –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç—ã, ''–£, –ë–∞—Ä–∞–∫–∞, –û–±–∞–º—ã, –ø–æ—è–≤–∏–ª—Å—è, –µ—â–µ, –æ–¥–∏–Ω, —Å–æ–ø–µ—Ä–Ω–∏–∫, –Ω–∞, –ø—Ä–µ–¥—Å—Ç–æ—è—â–∏—Ö, –≤—ã–±–æ—Ä–∞—Ö, –≤—ã–±–æ—Ä–∞—Ö, –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞, –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞, –°–æ–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö, —à—Ç–∞—Ç–æ–≤, –°–æ–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö, —à—Ç–∞—Ç–æ–≤, '', –ë—ã–≤—à–∏–π, –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä, —à—Ç–∞—Ç–∞, –ú–∏–Ω–Ω–µ—Å–æ—Ç–∞, –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä, —à—Ç–∞—Ç–∞, –ú–∏–Ω–Ω–µ—Å–æ—Ç–∞, –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä, —à—Ç–∞—Ç–∞, –ú–∏–Ω–Ω–µ—Å–æ—Ç–∞, —á–ª–µ–Ω, –†–µ—Å–ø—É–±–ª–∏–∫–∞–Ω—Å–∫–æ–π, –ø–∞—Ä—Ç–∏–∏, –°–®–ê, –†–µ—Å–ø—É–±–ª–∏–∫–∞–Ω—Å–∫–æ–π, –ø–∞—Ä—Ç–∏–∏, –°–®–ê, –¢–∏–º, –ü–æ–ª–µ–Ω—Ç–∏, ,, –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ, –∑–∞—è–≤–∏–ª, –æ, —Å–≤–æ–µ–º, –≤—Å—Ç—É–ø–ª–µ–Ω–∏–∏, –≤, –±–æ—Ä—å–±—É, –∑–∞, —Ç–æ,, —á—Ç–æ–±—ã, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å, –†–µ—Å–ø—É–±–ª–∏–∫–∞–Ω—Å–∫—É—é, –ø–∞—Ä—Ç–∏—é, –†–µ—Å–ø—É–±–ª–∏–∫–∞–Ω—Å–∫—É—é, –ø–∞—Ä—Ç–∏—é, –Ω–∞, –ø—Ä–µ–¥—Å—Ç–æ—è—â–∏—Ö, –≤—ã–±–æ—Ä–∞—Ö, –≤—ã–±–æ—Ä–∞—Ö, –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞, –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞, –°–®–ê, –°–®–ê, ., ¬´–ù–∞—à–µ–π, —Å—Ç—Ä–∞–Ω–µ, –Ω—É–∂–Ω–æ, –Ω–æ–≤–æ–µ, —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ., –ù–∞–º, –Ω—É–∂–Ω–æ, –≤–Ω–æ–≤—å, –∑–∞—Å—Ç–∞–≤–∏—Ç—å, —Ä–∞–±–æ—Ç–∞—Ç—å, –Ω–∞—à—É, —ç–∫–æ–Ω–æ–º–∏–∫—É., –ö, —Å–æ–∂–∞–ª–µ–Ω–∏—é,, —É, –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞, –û–±–∞–º—ã, –Ω–µ, —Ö–≤–∞—Ç–∞–µ—Ç, –º—É–∂–µ—Å—Ç–≤–∞, –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å, –ø—Ä–æ—Å—Ç—ã–º, –∞–º–µ—Ä–∏–∫–∞–Ω—Ü–∞–º, –≤, –≥–ª–∞–∑–∞, –∏, —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å, –∏–º, –≥–æ—Ä—å–∫—É—é, –ø—Ä–∞–≤–¥—É, ...]</td>\n",
       "      <td>[O, B-PROFESSION, I-PROFESSION, B-PROFESSION, B-STATE_OR_PROVINCE, O, O, B-PROFESSION, O, B-PERSON, I-PERSON, O, O, O, O, O, O, B-EVENT, B-EVENT, I-EVENT, B-PROFESSION, I-PROFESSION, I-PROFESSION, B-COUNTRY, I-COUNTRY, O, O, B-PROFESSION, I-PROFESSION, I-PROFESSION, B-PROFESSION, I-PROFESSION, I-PROFESSION, B-PROFESSION, O, B-STATE_OR_PROVINCE, O, B-ORGANIZATION, I-ORGANIZATION, I-ORGANIZATION, B-IDEOLOGY, O, B-COUNTRY, B-PERSON, I-PERSON, O, O, O, O, O, O, O, O, O, O, O, O, B-ORGANIZATION, I-ORGANIZATION, B-IDEOLOGY, O, O, O, B-EVENT, B-EVENT, I-EVENT, B-PROFESSION, I-PROFESSION, B-COUNTRY, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-PROFESSION, B-PERSON, O, O, O, O, O, B-NATIONALITY, O, O, O, O, O, O, O, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–§–ò–§–ê: –†–æ–Ω–∞–ª–¥—É ‚Äî –ª—É—á—à–∏–π —Ñ—É—Ç–±–æ–ª–∏—Å—Ç, –ó–∏–¥–∞–Ω ‚Äî —Ç—Ä–µ–Ω–µ—Ä, –ë—É—Ñ—Ñ–æ–Ω ‚Äî –≤—Ä–∞—Ç–∞—Ä—å\\n–ö—Ä–∏—à—Ç–∏–∞–Ω—É –†–æ–Ω–∞–ª–¥—É\\n–í –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫, 23 –æ–∫—Ç—è–±—Ä—è 2017 –≥–æ–¥–∞, –≤ –ª–æ–Ω–¥–æ–Ω—Å–∫–æ–º —ç—Å—Ç—Ä–∞–¥–Ω–æ–º —Ç–µ–∞—Ç—Ä–µ ¬´–ü–∞–ª–ª–∞–¥–∏—É–º¬ª –ø—Ä–æ—à–ª–∞ —Ü–µ—Ä–µ–º–æ–Ω–∏—è –≤—Ä—É—á–µ–Ω–∏—è –Ω–∞–≥—Ä–∞–¥ –§–ò–§–ê.\\n\\n–ù–∞–ø–∞–¥–∞—é—â–∏–π ¬´–†–µ–∞–ª–∞¬ª –∏ —Å–±–æ—Ä–Ω–æ–π –ü–æ—Ä—Ç—É–≥–∞–ª–∏–∏ –ö—Ä–∏—à—Ç–∏–∞–Ω—É –†–æ–Ω–∞–ª–¥—É –ø—Ä–∏–∑–Ω–∞–Ω –ª—É—á—à–∏–º —Ñ—É—Ç–±–æ–ª–∏—Å—Ç–æ–º 2017 –≥–æ–¥–∞. –û–Ω –æ–ø–µ—Ä–µ–¥–∏–ª –≤ —Å–ø–æ—Ä–µ –∑–∞ —ç—Ç–æ –∑–≤–∞–Ω–∏–µ –õ–∏–æ–Ω–µ–ª—è –ú–µ—Å—Å–∏ (¬´–ë–∞—Ä—Å–µ–ª–æ–Ω–∞¬ª) –∏ –ù–µ–π–º–∞—Ä–∞ (¬´–ë–∞—Ä—Å–µ–ª–æ–Ω–∞¬ª/¬´–ü–°–ñ¬ª).\\n\\n–†–æ–Ω–∞–ª–¥—É —è–≤–ª—è–µ—Ç—Å—è –¥–µ–π—Å—Ç–≤—É—é—â–∏–º –æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–º –Ω–∞–≥—Ä–∞–¥—ã. –í 2017 –≥–æ–¥—É –≤ —Å–æ—Å—Ç–∞–≤–µ ¬´–†–µ–∞–ª–∞¬ª –æ–Ω –≤—ã–∏–≥—Ä–∞–ª —á–µ–º–ø–∏–æ–Ω–∞—Ç –ò—Å–ø–∞–Ω–∏–∏ –∏ –õ–∏–≥—É —á–µ–º–ø–∏–æ–Ω–æ–≤, —Å—Ç–∞–≤ –ª—É—á—à–∏–º –±–æ–º–±–∞—Ä–¥–∏—Ä–æ–º —Ç—É—Ä–Ω–∏—Ä–∞.\\n\\n–ó–∏–Ω–µ–¥–∏–Ω –ó–∏–¥–∞–Ω\\n–ì–ª–∞–≤–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –º–∞–¥—Ä–∏–¥—Å–∫–æ–≥–æ ¬´–†–µ–∞–ª–∞¬ª –ó–∏–Ω–µ–¥–∏–Ω –ó–∏–¥–∞–Ω —Å—Ç–∞–ª –ª—É—á—à–∏–º —Ç—Ä–µ–Ω–µ—Ä–æ–º 2017 –≥–æ–¥–∞. –í —Å–ø–æ—Ä–µ –∑–∞ —ç—Ç–æ –∑–≤–∞–Ω–∏–µ –æ–Ω –æ–ø–µ—Ä–µ–¥–∏–ª –≥–ª–∞–≤–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞ ¬´–ß–µ–ª—Å–∏¬ª –ê–Ω—Ç–æ–Ω–∏–æ –ö–æ–Ω—Ç–µ –∏ —Ä—É–ª–µ–≤–æ–≥–æ ¬´–Æ–≤–µ–Ω—Ç—É—Å–∞¬ª –ú–∞—Å—Å–∏–º–∏–ª–∏–∞–Ω–æ –ê–ª–ª–µ–≥—Ä–∏.\\n\\n–î–∂–∞–Ω–ª—É–∏–¥–∂–∏ –ë—É—Ñ—Ñ–æ–Ω\\n–ò—Ç–∞–ª—å—è–Ω—Å–∫–∏–π –≥–æ–ª–∫–∏–ø–µ—Ä –î–∂–∞–Ω–ª—É–∏–¥–∂–∏ –ë—É—Ñ—Ñ–æ–Ω (–Æ–≤–µ–Ω—Ç—É—Å) –ø—Ä–∏–∑–Ω–∞–Ω –§–ò–§–ê –ª—É—á—à–∏–º –≤—Ä–∞—Ç–∞—Ä—ë–º –≥–æ–¥–∞ –∏ —Å—Ç–∞–ª –ø–µ—Ä–≤—ã–º –æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–º —ç—Ç–æ–π –Ω–∞–≥—Ä–∞–¥—ã. –ü–æ–ª—É—á–∏–≤ –ø—Ä–∏–∑, –≥–æ–ª–∫–∏–ø–µ—Ä, –∫–æ—Ç–æ—Ä–æ–º—É –≤ —è–Ω–≤–∞—Ä–µ –∏—Å–ø–æ–ª–Ω–∏—Ç—Å—è 40 –ª–µ—Ç, —Å–∫–∞–∑–∞–ª, —á—Ç–æ –æ–Ω –ø–æ–ª—å—â–µ–Ω –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–æ–ª—å –ø–æ—á–µ—Ç–Ω–æ–µ –ø—Ä–∏–∑–Ω–∞–Ω–∏–µ –≤ —Ç–∞–∫–æ–º –ø–æ—á—Ç–µ–Ω–Ω–æ–º –≤–æ–∑—Ä–∞—Å—Ç–µ.\\n\\n–õ–∞—É—Ä–µ–∞—Ç–æ–º –ø—Ä–∏–∑–∞ –∑–∞ —á–µ—Å—Ç–Ω—É—é –∏–≥—Ä—É –æ—Ç FIFA —Å—Ç–∞–ª –Ω–∞–ø–∞–¥–∞—é—â–∏–π —á–µ—à—Å–∫–æ–≥–æ –∫–ª—É–±–∞ ¬´–°–ª–æ–≤–∞—Ü–∫–æ¬ª (1. FC Slov√°cko) –§—Ä–∞–Ω—Å–∏—Å –ö–æ–Ω–µ (Francis Kon√©), –∫–æ—Ç–æ—Ä—ã–π –≤–æ –≤—Ä–µ–º—è –º–∞—Ç—á–∞ —Å ¬´–ë–æ–≥–µ–º–∏–∞–Ω—Å–æ–º¬ª —Å–ø–∞—Å –∂–∏–∑–Ω—å –≤—Ä–∞—Ç–∞—Ä—é —Å–æ–ø–µ—Ä–Ω–∏–∫–∞ –ú–∞—Ä—Ç–∏–Ω—É –ë–µ—Ä–∫–æ–≤–µ—Ü—É (Martin Berkovec). –ì–æ–ª–∫–∏–ø–µ—Ä –≤—Ä–µ–∑–∞–ª—Å—è –≤ —Å–≤–æ–µ–≥–æ –∑–∞—â–∏—Ç–Ω–∏–∫–∞ –∏ –ø–æ—Ç–µ—Ä—è–ª —Å–æ–∑–Ω–∞–Ω–∏–µ. –£ –ë–µ—Ä–∫–æ–≤–µ—Ü–∞ –∑–∞–ø–∞–ª —è–∑—ã–∫, –∏ –ö–æ–Ω–µ –æ–∫–∞–∑–∞–ª –µ–º—É –ø–µ—Ä–≤—É—é –ø–æ–º–æ—â—å.\\n\\n–ü–æ–ª—É–∑–∞—â–∏—Ç–Ω–∏–∫ –∏—Å–ø–∞–Ω—Å–∫–æ–π ¬´–ë–∞—Ä—Å–µ–ª–æ–Ω—ã¬ª –∏ —Å–±–æ—Ä–Ω–æ–π –ù–∏–¥–µ—Ä–ª–∞–Ω–¥–æ–≤ –õ–∏–∫–µ –ú–∞—Ä—Ç–µ–Ω—Å –ø—Ä–∏–∑–Ω–∞–Ω–∞ –ª—É—á—à–µ–π —Ñ—É—Ç–±–æ–ª–∏—Å—Ç–∫–æ–π –≥–æ–¥–∞. –í —Å–æ—Å—Ç–∞–≤–µ —Å–±–æ—Ä–Ω–æ–π –ù–∏–¥–µ—Ä–ª–∞–Ω–¥–æ–≤ –æ–Ω–∞ —Å—Ç–∞–ª–∞ —á–µ–º–ø–∏–æ–Ω–∫–æ–π –ï–≤—Ä–æ–ø—ã 2017 –≥–æ–¥–∞. –ú–∞—Ä—Ç–µ–Ω—Å –∑–∞–±–∏–ª–∞ —Ç—Ä–∏ –º—è—á–∞, —Å–¥–µ–ª–∞–ª–∞ –¥–≤–µ –≥–æ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–¥–∞—á–∏ –∏ –ø–æ–ª—É—á–∏–ª–∞ –∑–≤–∞–Ω–∏–µ –ª—É—á—à–µ–≥–æ –∏–≥—Ä–æ–∫–∞ —Ç—É—Ä–Ω–∏—Ä–∞.\\n\\n–õ—É—á—à–∏–º–∏ –±–æ–ª–µ–ª—å—â–∏–∫–∞–º–∏ –≥–æ–¥–∞ —Å—Ç–∞–ª–∏ —Ñ–∞–Ω–∞—Ç—ã ¬´–°–µ–ª—Ç–∏–∫–∞¬ª.\\n\\n–ù–∞ –ø—Ä–æ—Ç—è–∂–µ–Ω–∏–∏ —à–µ—Å—Ç–∏ –ª–µ—Ç –§–ò–§–ê –≤—Ä—É—á–∞–ª–∞ –ª—É—á—à–µ–º—É –∏–≥—Ä–æ–∫—É –ø–ª–∞–Ω–µ—Ç—ã ¬´–ó–æ–ª–æ—Ç–æ–π –º—è—á¬ª —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å —Ä–µ–¥–∞–∫—Ü–∏–µ–π –∂—É—Ä–Ω–∞–ª–∞ ''France Football'', –æ–¥–Ω–∞–∫–æ —Å 2016 –≥–æ–¥–∞ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–æ–µ –∏–∑–¥–∞–Ω–∏–µ, —É—á—Ä–µ–¥–∏–≤—à–µ–µ —Å–≤–æ–π –ø—Ä–∏–∑ –≤ 1956 –≥–æ–¥—É, –∏ –§–ò–§–ê –≤–Ω–æ–≤—å –≤—Ä—É—á–∞—é—Ç –Ω–∞–≥—Ä–∞–¥—ã –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏.\\n</td>\n",
       "      <td>[–§–ò–§–ê, :, –†–æ–Ω–∞–ª–¥—É, ‚Äî, –ª—É—á—à–∏–π, —Ñ—É—Ç–±–æ–ª–∏—Å—Ç, —Ñ—É—Ç–±–æ–ª–∏—Å—Ç, ,, –ó–∏–¥–∞–Ω, ‚Äî, —Ç—Ä–µ–Ω–µ—Ä, ,, –ë—É—Ñ—Ñ–æ–Ω, ‚Äî, –≤—Ä–∞—Ç–∞—Ä—å, –ö—Ä–∏—à—Ç–∏–∞–Ω—É, –†–æ–Ω–∞–ª–¥—É, –í, –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫,, 23, –æ–∫—Ç—è–±—Ä—è, 2017, –≥–æ–¥–∞, ,, –≤, –ª–æ–Ω–¥–æ–Ω—Å–∫–æ–º, —ç—Å—Ç—Ä–∞–¥–Ω–æ–º, —Ç–µ–∞—Ç—Ä–µ, ¬´, –ü–∞–ª–ª–∞–¥–∏—É–º, ¬ª, –ø—Ä–æ—à–ª–∞, —Ü–µ—Ä–µ–º–æ–Ω–∏—è, –≤—Ä—É—á–µ–Ω–∏—è, –Ω–∞–≥—Ä–∞–¥, –§–ò–§–ê, ., –ù–∞–ø–∞–¥–∞—é—â–∏–π, –ù–∞–ø–∞–¥–∞—é—â–∏–π, ¬´–†–µ–∞–ª–∞, –†–µ–∞–ª–∞, ¬ª, –∏, —Å–±–æ—Ä–Ω–æ–π, –ü–æ—Ä—Ç—É–≥–∞–ª–∏–∏, –ü–æ—Ä—Ç—É–≥–∞–ª–∏–∏, –ö—Ä–∏—à—Ç–∏–∞–Ω—É, –†–æ–Ω–∞–ª–¥—É, –ø—Ä–∏–∑–Ω–∞–Ω, –ª—É—á—à–∏–º, —Ñ—É—Ç–±–æ–ª–∏—Å—Ç–æ–º, –ª—É—á—à–∏–º, —Ñ—É—Ç–±–æ–ª–∏—Å—Ç–æ–º, 2017, –≥–æ–¥–∞, —Ñ—É—Ç–±–æ–ª–∏—Å—Ç–æ–º, 2017, –≥–æ–¥–∞, ., –û–Ω, –æ–ø–µ—Ä–µ–¥–∏–ª, –≤, —Å–ø–æ—Ä–µ, –∑–∞, —ç—Ç–æ, –∑–≤–∞–Ω–∏–µ, –õ–∏–æ–Ω–µ–ª—è, –ú–µ—Å—Å–∏, (¬´, –ë–∞—Ä—Å–µ–ª–æ–Ω–∞, ¬ª), –∏, –ù–µ–π–º–∞—Ä–∞, (¬´, –ë–∞—Ä—Å–µ–ª–æ–Ω–∞, ¬ª/¬´, –ü–°–ñ, ¬ª)., –†–æ–Ω–∞–ª–¥—É, —è–≤–ª—è–µ—Ç—Å—è, –¥–µ–π—Å—Ç–≤—É—é—â–∏–º, –æ–±–ª–∞–¥–∞—Ç–µ–ª–µ–º, –Ω–∞–≥—Ä–∞–¥—ã., –í, 2017, –≥–æ–¥—É, –≤, —Å–æ—Å—Ç–∞–≤–µ, ¬´, –†–µ–∞–ª–∞, ¬ª, –æ–Ω, –≤—ã–∏–≥—Ä–∞–ª, —á–µ–º–ø–∏–æ–Ω–∞—Ç, –ò—Å–ø–∞–Ω–∏–∏, –ò—Å–ø–∞–Ω–∏–∏, –∏, –õ–∏–≥—É, —á–µ–º–ø–∏–æ–Ω–æ–≤, ,, ...]</td>\n",
       "      <td>[B-ORGANIZATION, O, B-PERSON, O, B-AWARD, I-AWARD, B-PROFESSION, O, B-PERSON, O, B-PROFESSION, O, B-PERSON, O, B-PROFESSION, B-PERSON, I-PERSON, B-DATE, I-DATE, I-DATE, I-DATE, I-DATE, I-DATE, O, O, B-CITY, O, O, O, B-FACILITY, O, O, B-EVENT, I-EVENT, I-EVENT, B-ORGANIZATION, O, B-PROFESSION, B-PROFESSION, I-PROFESSION, B-ORGANIZATION, O, O, B-ORGANIZATION, I-ORGANIZATION, B-COUNTRY, B-PERSON, I-PERSON, O, B-AWARD, I-AWARD, B-AWARD, I-AWARD, I-AWARD, I-AWARD, B-PROFESSION, B-DATE, I-DATE, O, O, O, O, O, O, O, O, B-PERSON, I-PERSON, O, B-ORGANIZATION, O, O, B-PERSON, O, B-ORGANIZATION, O, B-ORGANIZATION, O, B-PERSON, O, O, O, O, B-DATE, I-DATE, I-DATE, O, O, O, B-ORGANIZATION, O, O, O, B-EVENT, I-EVENT, B-COUNTRY, O, B-EVENT, I-EVENT, O, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>–í–æ –≤—Ä–µ–º—è –≤–∑—Ä—ã–≤–∞ –∞–≤—Ç–æ–º–æ–±–∏–ª—è –≤ –ö–∏–µ–≤–µ –ø–æ—Å—Ç—Ä–∞–¥–∞–ª–∞ –º–æ–¥–µ–ª—å Dior\\n–ú–∞–≥–∞–∑–∏–Ω Christian Dior\\n–í –∞–≤—Ç–æ–º–æ–±–∏–ª–µ, –∫–æ—Ç–æ—Ä—ã–π –≤–∑–æ—Ä–≤–∞–ª—Å—è –≤—á–µ—Ä–∞, 8 —Å–µ–Ω—Ç—è–±—Ä—è 2017 –≥–æ–¥–∞, –≤ —Ä–∞–π–æ–Ω–µ –ë–µ—Å—Å–∞—Ä–∞–±—Å–∫–æ–π –ø–ª–æ—â–∞–¥–∏ –≤ –ö–∏–µ–≤–µ –Ω–∞—Ö–æ–¥–∏–ª–∞—Å—å –º–æ–¥–µ–ª—å Christian Dior –ù–∞—Ç–∞–ª—å—è –ö–æ—à–µ–ª—å, –µ–π –æ—Ç–æ—Ä–≤–∞–ª–æ –Ω–æ–≥—É, —Ç–∞–∫–∂–µ –æ–Ω–∞ –ø–æ–ª—É—á–∏–ª–∞ —Ç—Ä–∞–≤–º—ã –≥–ª–∞–∑, —Å–æ–æ–±—â–∞—é—Ç –°–ú–ò.\\n\\n–†–∞–Ω–µ–µ –ø–æ–¥—Ä—É–≥–∞ –¥–µ–≤—É—à–∫–∏ –û–∫—Å–∞–Ω–∞ –õ–∞–∑–µ–±–Ω–∏–∫ —Å–æ–æ–±—â–∏–ª–∞ –°–ú–ò, —á—Ç–æ –≤ –º–æ–º–µ–Ω—Ç –≤–∑—Ä—ã–≤–∞ –≤ –º–∞—à–∏–Ω–µ –Ω–∞—Ö–æ–¥–∏–ª–∞—Å—å ¬´–≤—Å–µ–º–∏—Ä–Ω–æ –∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è–µ—Ç—Å—è –ª–∏—Ü–æ–º –º–∞—Ä–∫–∏ Dior¬ª.\\n\\n–í –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è –∑–∞ –µ—ë –∂–∏–∑–Ω—å –±–æ—Ä—é—Ç—Å—è –≤—Ä–∞—á–∏ 17-–π –±–æ–ª—å–Ω–∏—Ü—ã —É–∫—Ä–∞–∏–Ω—Å–∫–æ–π —Å—Ç–æ–ª–∏—Ü—ã.\\n–ó–∞–≤–µ–¥—É—é—â–∏–π –æ—Ç–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ–ª–∏—Ç—Ä–∞–≤–º –∫–∏–µ–≤—Å–∫–æ–π –±–æ–ª—å–Ω–∏—Ü—ã ‚Ññ 17 –î–º–∏—Ç—Ä–∏–π –ú—è—Å–Ω–∏–∫–æ–≤ –∑–∞—è–≤–∏–ª, —á—Ç–æ –∂–µ–Ω—â–∏–Ω—É –æ–ø–µ—Ä–∏—Ä—É—é—Ç —Ç—Ä–∏ –±—Ä–∏–≥–∞–¥—ã —Ö–∏—Ä—É—Ä–≥–æ–≤.\\n\\n¬´–£ –ø–∞—Ü–∏–µ–Ω—Ç–∫–∏ –µ—Å—Ç—å —Ç–µ—Ä–º–∏—á–µ—Å–∫–∞—è —Ç—Ä–∞–≤–º–∞, –æ–∂–æ–≥–∏, –ø–æ—Ä–∞–∂–µ–Ω–∏—è –æ—Ä–≥–∞–Ω–æ–≤, —Ç—Ä–∞–≤–º–∞ –∫–æ—Å—Ç–µ–π –∏ —Ç—Ä–∞–≤–º–∞ –º—è–≥–∫–∏—Ö —Ç–∫–∞–Ω–µ–π¬ª, ‚Äî —Å–∫–∞–∑–∞–ª –≤—Ä–∞—á.\\n\\n–ú–æ–¥–µ–ª—å –Ω–∞—Ö–æ–¥–∏–ª–∞—Å—å –≤ –º–∞—à–∏–Ω–µ –≤–º–µ—Å—Ç–µ —Å —à–µ—Å—Ç–∏–ª–µ—Ç–Ω–∏–º –º–∞–ª—å—á–∏–∫–æ–º –ø–æ –∏–º–µ–Ω–∏ –ê–Ω—Ç–æ–Ω. –†–µ–±—ë–Ω–∫–∞ –¥–æ—Å—Ç–∞–≤–∏–ª–∏ –≤ –æ–∂–æ–≥–æ–≤—ã–π —Ü–µ–Ω—Ç—Ä. –û –µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Å–æ–æ–±—â–∞–µ—Ç—Å—è.\\n\\n–ò–∑-–∑–∞ –≤–∑—Ä—ã–≤–∞ –Ω–∞ –º–µ—Å—Ç–µ –ø—Ä–æ–∏—Å—à–µ—Å—Ç–≤–∏—è –ø–æ–≥–∏–± –Ω–∞—Ö–æ–¥–∏–≤—à–∏–π—Å—è –≤ –º–∞—à–∏–Ω–µ –≥—Ä–∞–∂–¥–∞–Ω–∏–Ω –ì—Ä—É–∑–∏–∏ –±–æ–µ—Ü —á–µ—á–µ–Ω—Å–∫–æ–≥–æ –±–∞—Ç–∞–ª—å–æ–Ω–∞ –∏–º–µ–Ω–∏ —à–µ–π—Ö–∞ –ú–∞–Ω—Å—É—Ä–∞ —á–µ—á–µ–Ω–µ—Ü –¢–∏–º—É—Ä –ú–∞—Ö–∞—É—Ä–∏.\\n\\n–ò–Ω—Ü–∏–¥–µ–Ω—Ç –ø—Ä–æ–∏–∑–æ—à–µ–ª –≤ –ø—è—Ç–Ω–∏—Ü—É –æ–∫–æ–ª–æ 18:00 –º–µ–∂–¥—É —É–ª. –ë–∞—Å—Å–µ–π–Ω–æ–π –∏ –ë–æ–ª—å—à–æ–π –í–∞—Å–∏–ª—å–∫–æ–≤—Å–∫–æ–π. –í–∑–æ—Ä–≤–∞–≤—à–∏–π—Å—è –∞–≤—Ç–æ–º–æ–±–∏–ª—å ¬´–¢–æ–π–æ—Ç–∞ –ö–∞–º—Ä–∏¬ª –ø–æ–ª–Ω–æ—Å—Ç—å—é —É–Ω–∏—á—Ç–æ–∂–µ–Ω.\\n\\n–î–∏—Ä–µ–∫—Ç–æ—Ä –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –ú–í–î –£–∫—Ä–∞–∏–Ω—ã –ê—Ä—Ç—ë–º –®–µ–≤—á–µ–Ω–∫–æ —Å–æ–æ–±—â–∞–ª –°–ú–ò:\\n–°–µ–≥–æ–¥–Ω—è –ø—Ä–æ–∏–∑–æ—à–µ–ª –≤–∑—Ä—ã–≤ –∞–≤—Ç–æ–º–æ–±–∏–ª—è, –≤ –∫–æ—Ç–æ—Ä–æ–º –Ω–∞—Ö–æ–¥–∏–ª–∏—Å—å —Ç—Ä–∏ —á–µ–ª–æ–≤–µ–∫–∞. –≠—Ç–æ —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–≥–∏–±, –∂–µ–Ω—â–∏–Ω–∞, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–ª—É—á–∏–ª–∞ —Å–µ—Ä—å–µ–∑–Ω—ã–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è –∏ —Å–µ–π—á–∞—Å –∑–∞ –µ–µ –∂–∏–∑–Ω—å –±–æ—Ä—é—Ç—Å—è –º–µ–¥–∏–∫–∏, –∏ —Ä–µ–±–µ–Ω–æ–∫, –≤—ã–∂–∏–≤—à–∏–π –∏ –∂–∏–∑–Ω–∏ –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–∏—á–µ–≥–æ –Ω–µ —É–≥—Ä–æ–∂–∞–µ—Ç.\\n\\n–ü–æ —Å–ª–æ–≤–∞–º –®–µ–≤—á–µ–Ω–∫–æ, –≤ 2017 –≥–æ–¥—É –ú–∞—Ö–∞—É—Ä–∏ ‚Äî ¬´–ª–∏—Ü–æ, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–∑–≤–µ—Å—Ç–Ω–æ–µ –≤ –∫—Ä–∏–º–∏–Ω–∞–ª—å–Ω—ã—Ö –∫—Ä—É–≥–∞—Ö, –∫–æ—Ç–æ—Ä–æ–µ –∏–º–µ–ª–æ —É—Å—Ç–æ–π—á–∏–≤—ã–µ —Å–≤—è–∑–∏ —Å —Ä–∞–∑–Ω–æ–≥–æ —Ä–æ–¥–∞ —á–µ—á–µ–Ω—Å–∫–∏–º–∏ –∫—Ä—É–≥–∞–º–∏¬ª, –±—ã–ª –∑–∞–¥–µ—Ä–∂–∞–Ω –∑–∞ –Ω–µ–∑–∞–∫–æ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ä—É–∂–∏—è. –ü–æ–∑–∂–µ –æ–Ω –∑–∞–∫–ª—é—á–∏–ª —Å–¥–µ–ª–∫—É —Å–æ —Å–ª–µ–¥—Å—Ç–≤–∏–µ–º –∏ –ø–æ–ª—É—á–∏–ª —É—Å–ª–æ–≤–Ω—ã–π —Å—Ä–æ–∫.\\n\\n–í–æ–∑–±—É–∂–¥–µ–Ω–æ —É–≥–æ–ª–æ–≤–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –ø–æ —Å—Ç. 115 —á. 2 ¬´–£–º—ã—à–ª–µ–Ω–Ω–æ–µ —É–±–∏–π—Å—Ç–≤–æ, —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ–ø–∞—Å–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º¬ª. –°–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–º –¥–∞–Ω–Ω—ã–º, –≤ –∞–≤—Ç–æ–º–æ–±–∏–ª–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ –≤–∑—Ä—ã–≤–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ.\\n\\n–ü–æ–ª–∏—Ü–∏—è –∏ –ù–∞—Ü–≥–≤–∞—Ä–¥–∏—è —É—Å–∏–ª–∏–ª–∏ –ø–∞—Ç—Ä—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–Ω—Ç—Ä–∞ –ö–∏–µ–≤–∞ –∏ –º–µ—Ç—Ä–æ –ø–æ—Å–ª–µ –≤–∑—Ä—ã–≤–∞ –Ω–∞ –ë–µ—Å—Å–∞—Ä–∞–±–∫–µ.\\n</td>\n",
       "      <td>[–í–æ, –≤—Ä–µ–º—è, –≤–∑—Ä—ã–≤–∞, –≤–∑—Ä—ã–≤–∞, –∞–≤—Ç–æ–º–æ–±–∏–ª—è, –≤, –ö–∏–µ–≤–µ, –ø–æ—Å—Ç—Ä–∞–¥–∞–ª–∞, –º–æ–¥–µ–ª—å, –º–æ–¥–µ–ª—å, Dior, Dior, –ú–∞–≥–∞–∑–∏–Ω, Christian, Dior, –í, –∞–≤—Ç–æ–º–æ–±–∏–ª–µ,, –∫–æ—Ç–æ—Ä—ã–π, –≤–∑–æ—Ä–≤–∞–ª—Å—è, –≤—á–µ—Ä–∞,, 8, —Å–µ–Ω—Ç—è–±—Ä—è, 2017, –≥–æ–¥–∞, ,, –≤, —Ä–∞–π–æ–Ω–µ, –ë–µ—Å—Å–∞—Ä–∞–±—Å–∫–æ–π, –ø–ª–æ—â–∞–¥–∏, –≤, –ö–∏–µ–≤–µ, –Ω–∞—Ö–æ–¥–∏–ª–∞—Å—å, –º–æ–¥–µ–ª—å, –º–æ–¥–µ–ª—å, Christian, Dior, Christian, Dior, –ù–∞—Ç–∞–ª—å—è, –ö–æ—à–µ–ª—å, ,, –µ–π, –æ—Ç–æ—Ä–≤–∞–ª–æ, –Ω–æ–≥—É, ,, —Ç–∞–∫–∂–µ, –æ–Ω–∞, –ø–æ–ª—É—á–∏–ª–∞, —Ç—Ä–∞–≤–º—ã, –≥–ª–∞–∑, ,, —Å–æ–æ–±—â–∞—é—Ç, –°–ú–ò., –†–∞–Ω–µ–µ, –ø–æ–¥—Ä—É–≥–∞, –¥–µ–≤—É—à–∫–∏, –û–∫—Å–∞–Ω–∞, –õ–∞–∑–µ–±–Ω–∏–∫, —Å–æ–æ–±—â–∏–ª–∞, –°–ú–ò,, —á—Ç–æ, –≤, –º–æ–º–µ–Ω—Ç, –≤–∑—Ä—ã–≤–∞, –≤, –º–∞—à–∏–Ω–µ, –Ω–∞—Ö–æ–¥–∏–ª–∞—Å—å, ¬´–≤—Å–µ–º–∏—Ä–Ω–æ, –∏–∑–≤–µ—Å—Ç–Ω–∞—è, –º–æ–¥–µ–ª—å, ,, –∫–æ—Ç–æ—Ä–∞—è, —è–≤–ª—è–µ—Ç—Å—è, –ª–∏—Ü–æ–º, –º–∞—Ä–∫–∏, Dior, ¬ª., –í, –Ω–∞—Å—Ç–æ—è—â–µ–µ, –≤—Ä–µ–º—è, –∑–∞, –µ—ë, –∂–∏–∑–Ω—å, –±–æ—Ä—é—Ç—Å—è, –≤—Ä–∞—á–∏, 17-–π, –±–æ–ª—å–Ω–∏—Ü—ã, —É–∫—Ä–∞–∏–Ω—Å–∫–æ–π, —É–∫—Ä–∞–∏–Ω—Å–∫–æ–π, —Å—Ç–æ–ª–∏—Ü—ã, ., –ó–∞–≤–µ–¥—É—é—â–∏–π, –æ—Ç–¥–µ–ª–µ–Ω–∏–µ–º, –ø–æ–ª–∏—Ç—Ä–∞–≤–º, –∫–∏–µ–≤—Å–∫–æ–π, –±–æ–ª—å–Ω–∏—Ü—ã, ‚Ññ, 17, –ó–∞–≤–µ–¥—É—é—â–∏–π, –æ—Ç–¥–µ–ª–µ–Ω–∏–µ–º, ...]</td>\n",
       "      <td>[O, O, B-EVENT, B-EVENT, I-EVENT, O, B-CITY, O, B-PROFESSION, B-PROFESSION, I-PROFESSION, B-ORGANIZATION, O, B-ORGANIZATION, I-ORGANIZATION, O, O, O, B-EVENT, B-DATE, I-DATE, I-DATE, I-DATE, I-DATE, O, O, O, B-FACILITY, I-FACILITY, O, B-CITY, O, B-PROFESSION, B-PROFESSION, I-PROFESSION, I-PROFESSION, B-ORGANIZATION, I-ORGANIZATION, B-PERSON, I-PERSON, O, O, B-DISEASE, I-DISEASE, O, O, O, O, B-DISEASE, I-DISEASE, O, O, O, O, O, O, B-PERSON, I-PERSON, O, O, O, O, O, B-EVENT, O, O, O, O, O, B-PROFESSION, O, O, O, O, O, B-ORGANIZATION, O, O, O, O, O, O, O, O, O, B-FACILITY, I-FACILITY, B-COUNTRY, B-CITY, I-CITY, O, B-PROFESSION, I-PROFESSION, I-PROFESSION, I-PROFESSION, I-PROFESSION, I-PROFESSION, I-PROFESSION, B-PROFESSION, I-PROFESSION, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>–≠–∫—Å-—Å–æ–≤–µ—Ç–Ω–∏–∫ –¢—Ä–∞–º–ø–∞ –ø–æ–ª—É—á–∏–ª –æ—Ç —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π $68 000\\n–ú–∞–π–∫–ª –§–ª–∏–Ω–Ω\\n–û—Ç—Å—Ç–∞–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞–ª –ê—Ä–º–∏–∏ –°–®–ê –ú–∞–π–∫–ª –§–ª–∏–Ω–Ω, –∫–æ—Ç–æ—Ä–æ–≥–æ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç –î–æ–Ω–∞–ª—å–¥ –¢—Ä–∞–º–ø –Ω–∞–∑–Ω–∞—á–∏–ª, –∞ –∑–∞—Ç–µ–º –ø–æ–¥ –¥–∞–≤–ª–µ–Ω–∏–µ–º –æ–±—â–µ—Å—Ç–≤–∞ —É–≤–æ–ª–∏–ª —Å –ø–æ—Å—Ç–∞ —Å–æ–≤–µ—Ç–Ω–∏–∫–∞ –ø–æ –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –≤ 2015 –≥–æ–¥—É –ø–æ–ª—É—á–∏–ª –ø–æ—á—Ç–∏ 68 000 –¥–æ–ª–ª–∞—Ä–æ–≤ –æ—Ç —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π.\\n\\n–û–± —ç—Ç–æ–º —Å—Ç–∞–ª–æ –∏–∑–≤–µ—Å—Ç–Ω–æ –∏–∑ –Ω–µ–¥–∞–≤–Ω–æ –æ–±–Ω–∞—Ä–æ–¥–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.\\n\\n–ü–æ–ª—É—á–µ–Ω–Ω–∞—è –§–ª–∏–Ω–Ω–æ–º —Å—É–º–º–∞, –æ–∫–∞–∑–∞–≤—à–∞—è—Å—è –≤—ã—à–µ, —á–µ–º –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–ª–æ—Å—å —Ä–∞–Ω–µ–µ, –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è –≤ —Ä–∞–∑–º–µ—Ä–µ 45 386 –¥–æ–ª–ª–∞—Ä–æ–≤, –∫–æ—Ç–æ—Ä–æ–µ –µ–º—É –≤—ã–ø–ª–∞—Ç–∏–ª —Ä–æ—Å—Å–∏–π—Å–∫–∏–π –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–µ–ª–µ–∫–∞–Ω–∞–ª RT.\\n\\n–§–ª–∏–Ω–Ω –ø—Ä–∏–µ–∑–∂–∞–ª –≤ –ú–æ—Å–∫–≤—É –ø–æ —Å–ª—É—á–∞—é 10-–ª–µ—Ç–Ω–µ–≥–æ —é–±–∏–ª–µ—è –∫–∞–Ω–∞–ª–∞ –∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞–ª –Ω–∞ —Ç–æ—Ä–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –ø—Ä–∏—ë–º–µ, –≥–¥–µ —Å–∏–¥–µ–ª –∑–∞ –æ–¥–Ω–∏–º —Å—Ç–æ–ª–æ–º —Å —Ä–æ—Å—Å–∏–π—Å–∫–∏–º –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–æ–º –í–ª–∞–¥–∏–º–∏—Ä–æ–º –ü—É—Ç–∏–Ω—ã–º.\\n\\n–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –§–ª–∏–Ω–Ω –ø–æ–ª—É—á–∏–ª –¥–≤–∞ –ø–ª–∞—Ç–µ–∂–∞ –ø–æ 11 250 –¥–æ–ª–ª–∞—Ä–æ–≤: –æ–¥–∏–Ω ‚Äî –æ—Ç –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–≥–æ —Ñ–∏–ª–∏–∞–ª–∞ —Ä–æ—Å—Å–∏–π—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –≤ –æ–±–ª–∞—Å—Ç–∏ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –¥—Ä—É–≥–æ–π ‚Äî –æ—Ç —á–∞—Ä—Ç–µ—Ä–Ω—ã—Ö –∞–≤–∏–∞–ª–∏–Ω–∏–π ¬´–í–æ–ª–≥–∞-–î–Ω–µ–ø—Ä¬ª.\\n\\n–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –æ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –§–ª–∏–Ω–Ω–æ–º –ø–ª–∞—Ç–µ–∂–∞—Ö –ø—Ä–∏–≤—ë–ª –∫–æ–Ω–≥—Ä–µ—Å—Å–º–µ–Ω –≠–ª–∞–π–¥–∂–∞ –ö–∞–º–º–∏–Ω–≥—Å (Elijah Cummings), –≥–ª–∞–≤–Ω—ã–π –¥–µ–º–æ–∫—Ä–∞—Ç –≤ –∫–æ–º–∏—Ç–µ—Ç–µ –ø–æ –Ω–∞–¥–∑–æ—Ä—É –ü–∞–ª–∞—Ç—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–µ–π.\\n\\n–í –ø–∏—Å—å–º–µ, –æ–±—Ä–∞—â—ë–Ω–Ω–æ–º –∫ –¢—Ä–∞–º–ø—É, –º–∏–Ω–∏—Å—Ç—Ä—É –æ–±–æ—Ä–æ–Ω—ã –î–∂–∏–º—É –ú—ç—Ç—Ç–∏—Å—É –∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä—É –§–ë–† –î–∂–µ–π–º—Å—É –ö–æ–º–∏, –ö–∞–º–º–∏–Ω–≥—Å —Å–ø—Ä–æ—Å–∏–ª, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª –ª–∏ –§–ª–∏–Ω–Ω –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤—É –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–≤–æ–∏—Ö —Å–≤—è–∑—è—Ö —Å –†–æ—Å—Å–∏–µ–π –∏ –¢—É—Ä—Ü–∏–µ–π, –ø—Ä–µ–∂–¥–µ —á–µ–º –ø—Ä–æ—à—ë–ª –ø—Ä–æ–≤–µ—Ä–∫—É –±–ª–∞–≥–æ–Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏ –∏ –±—ã–ª –Ω–∞–∑–Ω–∞—á–µ–Ω –Ω–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç—å –≤ –ë–µ–ª–æ–º –¥–æ–º–µ.\\n</td>\n",
       "      <td>[–≠–∫—Å-, —Å–æ–≤–µ—Ç–Ω–∏–∫, –¢—Ä–∞–º–ø–∞, –¢—Ä–∞–º–ø–∞, –ø–æ–ª—É—á–∏–ª, –æ—Ç, —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö, –∫–æ–º–ø–∞–Ω–∏–π, $68, 000, –ú–∞–π–∫–ª, –§–ª–∏–Ω–Ω, –û—Ç—Å—Ç–∞–≤–Ω–æ–π, –≥–µ–Ω–µ—Ä–∞–ª, –≥–µ–Ω–µ—Ä–∞–ª, –ê—Ä–º–∏–∏, –°–®–ê, –ê—Ä–º–∏–∏, –°–®–ê, –°–®–ê, –ú–∞–π–∫–ª, –§–ª–∏–Ω–Ω, ,, –∫–æ—Ç–æ—Ä–æ–≥–æ, –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç, –î–æ–Ω–∞–ª—å–¥, –¢—Ä–∞–º–ø, –Ω–∞–∑–Ω–∞—á–∏–ª,, –∞, –∑–∞—Ç–µ–º, –ø–æ–¥, –¥–∞–≤–ª–µ–Ω–∏–µ–º, –æ–±—â–µ—Å—Ç–≤–∞, —É–≤–æ–ª–∏–ª, —Å, –ø–æ—Å—Ç–∞, —Å–æ–≤–µ—Ç–Ω–∏–∫–∞, –ø–æ, –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, ,, –≤, 2015, –≥–æ–¥—É, –ø–æ–ª—É—á–∏–ª, –ø–æ—á—Ç–∏, 68, 000, –¥–æ–ª–ª–∞—Ä–æ–≤, –æ—Ç, —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö, –∫–æ–º–ø–∞–Ω–∏–π., –û–±, —ç—Ç–æ–º, —Å—Ç–∞–ª–æ, –∏–∑–≤–µ—Å—Ç–Ω–æ, –∏–∑, –Ω–µ–¥–∞–≤–Ω–æ, –æ–±–Ω–∞—Ä–æ–¥–æ–≤–∞–Ω–Ω—ã—Ö, –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤., –ü–æ–ª—É—á–µ–Ω–Ω–∞—è, –§–ª–∏–Ω–Ω–æ–º, —Å—É–º–º–∞,, –æ–∫–∞–∑–∞–≤—à–∞—è—Å—è, –≤—ã—à–µ,, —á–µ–º, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–ª–æ—Å—å, —Ä–∞–Ω–µ–µ,, –≤, –æ—Å–Ω–æ–≤–Ω–æ–º, —Å–æ—Å—Ç–æ–∏—Ç, –∏–∑, –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è, –≤, —Ä–∞–∑–º–µ—Ä–µ, 45, 386, –¥–æ–ª–ª–∞—Ä–æ–≤, ,, –∫–æ—Ç–æ—Ä–æ–µ, –µ–º—É, –≤—ã–ø–ª–∞—Ç–∏–ª, —Ä–æ—Å—Å–∏–π—Å–∫–∏–π, –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π, —Ç–µ–ª–µ–∫–∞–Ω–∞–ª, RT, ., –§–ª–∏–Ω–Ω, –ø—Ä–∏–µ–∑–∂–∞–ª, –≤, –ú–æ—Å–∫–≤—É, –ø–æ, —Å–ª—É—á–∞—é, 10-–ª–µ—Ç–Ω–µ–≥–æ, —é–±–∏–ª–µ—è, –∫–∞–Ω–∞–ª–∞, –∏, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞–ª, –Ω–∞, —Ç–æ—Ä–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–º, ...]</td>\n",
       "      <td>[O, B-PROFESSION, I-PROFESSION, B-PERSON, O, O, B-COUNTRY, O, B-MONEY, I-MONEY, B-PERSON, I-PERSON, O, B-PROFESSION, B-PROFESSION, I-PROFESSION, I-PROFESSION, B-ORGANIZATION, I-ORGANIZATION, B-COUNTRY, B-PERSON, I-PERSON, O, O, B-PROFESSION, B-PERSON, I-PERSON, O, O, O, O, O, O, O, O, O, B-PROFESSION, I-PROFESSION, I-PROFESSION, I-PROFESSION, O, B-DATE, I-DATE, I-DATE, O, B-MONEY, I-MONEY, I-MONEY, I-MONEY, O, B-COUNTRY, O, O, O, O, O, O, O, O, O, O, B-PERSON, O, O, O, O, O, O, O, O, O, O, O, O, O, B-MONEY, I-MONEY, I-MONEY, O, O, O, O, B-COUNTRY, O, O, B-ORGANIZATION, O, B-PERSON, O, O, B-CITY, O, O, B-AGE, O, O, O, O, O, B-EVENT, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>–ú–∞–∫—Å–∏–º –û—Ä–µ—à–∫–∏–Ω –Ω–∞–∑–Ω–∞—á–µ–Ω –º–∏–Ω–∏—Å—Ç—Ä–æ–º —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è\\n–ú–∞–∫—Å–∏–º –û—Ä–µ—à–∫–∏–Ω\\n30 –Ω–æ—è–±—Ä—è 2016 –≥–æ–¥–∞ –Ω–æ–≤—ã–º –º–∏–Ω–∏—Å—Ç—Ä–æ–º —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –Ω–∞–∑–Ω–∞—á–µ–Ω 34-–ª–µ—Ç–Ω–∏–π –ú–∞–∫—Å–∏–º –û—Ä–µ—à–∫–∏–Ω, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–Ω–µ–µ –∑–∞–Ω–∏–º–∞–ª –ø–æ—Å—Ç –∑–∞–º–µ—Å—Ç–∏—Ç–µ–ª—è –º–∏–Ω–∏—Å—Ç—Ä–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤.\\n\\n–ü—Ä–µ–∑–∏–¥–µ–Ω—Ç –†–§ –í–ª–∞–¥–∏–º–∏—Ä –ü—É—Ç–∏–Ω —Å–ø—Ä–æ—Å–∏–ª —É –û—Ä–µ—à–∫–∏–Ω–∞, —á—Ç–æ –ø–æ –µ–≥–æ –º–Ω–µ–Ω–∏—é —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ –≤ —Ä–∞–±–æ—Ç–µ –≤–µ–¥–æ–º—Å—Ç–≤–∞, –Ω–∞ —á—Ç–æ —Ç–æ—Ç –æ—Ç–≤–µ—Ç–∏–ª:\\n–°–∞–º–æ–µ –ø–ª–æ—Ö–æ–µ —É–∂–µ –ø–æ–∑–∞–¥–∏ –∏ —Å–µ–π—á–∞—Å –≤–∞–∂–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—Ç —Å–Ω—è—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–µ–≥—Ä–∞–¥—ã –¥–ª—è —Ä–æ—Å—Ç–∞ —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —ç–∫–æ–Ω–æ–º–∏–∫–∏.\\n\\n–ü–æ—Å–ª–µ —á–µ–≥–æ –ü—É—Ç–∏–Ω –æ—Ç–≤–µ—Ç–∏–ª:\\n–ú–∞–∫—Å–∏–º –°—Ç–∞–Ω–∏—Å–ª–∞–≤–æ–≤–∏—á, –≤—ã —á–µ–ª–æ–≤–µ–∫ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–æ–ª–æ–¥–æ–π, –Ω–æ –º–æ–ª–æ–¥—ã–º —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–º –≤–∞—Å —É–∂–µ –Ω–µ –Ω–∞–∑–æ–≤–µ—à—å. –í—ã —á–µ–ª–æ–≤–µ–∫ –≥—Ä–∞–º–æ—Ç–Ω—ã–π –∏ –∑—Ä–µ–ª—ã–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç, –æ–ø—ã—Ç–Ω—ã–π. –Ø –∂–µ–ª–∞—é –≤–∞–º —É–¥–∞—á–∏.\\n\\n–ú–∞–∫—Å–∏–º –û—Ä–µ—à–∫–∏–Ω —Ä–æ–¥–∏–ª—Å—è –≤ 1982 –≥–æ–¥—É. –°–≤–æ—é —é–Ω–æ—Å—Ç—å –ø—Ä–æ–≤—ë–ª –≤ –ú–æ—Å–∫–≤–µ, –•–æ–≤—Ä–∏–Ω–æ. –û—Ä–µ—à–∫–∏–Ω –∏–º–µ–µ—Ç –¥–µ—Å—è—Ç–∏–ª–µ—Ç–Ω–∏–π —Å—Ç–∞–∂ –≤ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–π —Å—Ñ–µ—Ä–µ, –ø–æ—Ä–∞–±–æ—Ç–∞–≤ –≤ ¬´–†–æ—Å–±–∞–Ω–∫¬ª, ¬´–í–¢–ë-–ö–∞–ø–∏—Ç–∞–ª¬ª, —Ç–∞–∫–∂–µ –∑–∞–Ω–∏–º–∞–ª –ø–æ—Å—Ç –≤–µ–¥—É—â–µ–≥–æ —ç–∫–æ–Ω–æ–º–∏—Å—Ç–∞ –≤ –¶–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫–µ –∏ —É—Å–ø–µ–ª –ø–æ—Ä–∞–±–æ—Ç–∞—Ç—å –≤ –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞, –∑–∞—Ç–µ–º –∑–∞–º–µ—Å—Ç–∏—Ç–µ–ª–µ–º –º–∏–Ω–∏—Å—Ç—Ä–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤.\\n</td>\n",
       "      <td>[–ú–∞–∫—Å–∏–º, –û—Ä–µ—à–∫–∏–Ω, –Ω–∞–∑–Ω–∞—á–µ–Ω, –º–∏–Ω–∏—Å—Ç—Ä–æ–º, —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–≥–æ, —Ä–∞–∑–≤–∏—Ç–∏—è, –ú–∞–∫—Å–∏–º, –û—Ä–µ—à–∫–∏–Ω, 30, –Ω–æ—è–±—Ä—è, 2016, –≥–æ–¥–∞, –Ω–æ–≤—ã–º, –º–∏–Ω–∏—Å—Ç—Ä–æ–º, —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–≥–æ, —Ä–∞–∑–≤–∏—Ç–∏—è, –º–∏–Ω–∏—Å—Ç—Ä–æ–º, —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–≥–æ, —Ä–∞–∑–≤–∏—Ç–∏—è, –Ω–∞–∑–Ω–∞—á–µ–Ω, 34-–ª–µ—Ç–Ω–∏–π, –ú–∞–∫—Å–∏–º, –û—Ä–µ—à–∫–∏–Ω, ,, –∫–æ—Ç–æ—Ä—ã–π, —Ä–∞–Ω–µ–µ, –∑–∞–Ω–∏–º–∞–ª, –ø–æ—Å—Ç, –∑–∞–º–µ—Å—Ç–∏—Ç–µ–ª—è, –º–∏–Ω–∏—Å—Ç—Ä–∞, —Ñ–∏–Ω–∞–Ω—Å–æ–≤, –º–∏–Ω–∏—Å—Ç—Ä–∞, —Ñ–∏–Ω–∞–Ω—Å–æ–≤, ., –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç, –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç, –†–§, –†–§, –í–ª–∞–¥–∏–º–∏—Ä, –ü—É—Ç–∏–Ω, —Å–ø—Ä–æ—Å–∏–ª, —É, –û—Ä–µ—à–∫–∏–Ω–∞, ,, —á—Ç–æ, –ø–æ, –µ–≥–æ, –º–Ω–µ–Ω–∏—é, —Å–∞–º–æ–µ, –≤–∞–∂–Ω–æ–µ, –≤, —Ä–∞–±–æ—Ç–µ, –≤–µ–¥–æ–º—Å—Ç–≤–∞,, –Ω–∞, —á—Ç–æ, —Ç–æ—Ç, –æ—Ç–≤–µ—Ç–∏–ª:, –°–∞–º–æ–µ, –ø–ª–æ—Ö–æ–µ, —É–∂–µ, –ø–æ–∑–∞–¥–∏, –∏, —Å–µ–π—á–∞—Å, –≤–∞–∂–Ω–æ, –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å, –∫–ª—é—á–µ–≤—ã–µ, –º–µ—Ä—ã,, –∫–æ—Ç–æ—Ä—ã–µ, –ø–æ–∑–≤–æ–ª—è—Ç, —Å–Ω—è—Ç—å, —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ, –ø—Ä–µ–≥—Ä–∞–¥—ã, –¥–ª—è, —Ä–æ—Å—Ç–∞, —Ä–æ—Å—Å–∏–π—Å–∫–æ–π, —ç–∫–æ–Ω–æ–º–∏–∫–∏., –ü–æ—Å–ª–µ, —á–µ–≥–æ, –ü—É—Ç–∏–Ω, –æ—Ç–≤–µ—Ç–∏–ª:, –ú–∞–∫—Å–∏–º, –°—Ç–∞–Ω–∏—Å–ª–∞–≤–æ–≤–∏—á, ,, –≤—ã, —á–µ–ª–æ–≤–µ–∫, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –º–æ–ª–æ–¥–æ–π,, –Ω–æ, –º–æ–ª–æ–¥—ã–º, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–º, –≤–∞—Å, —É–∂–µ, –Ω–µ, –Ω–∞–∑–æ–≤–µ—à—å., –í—ã, —á–µ–ª–æ–≤–µ–∫, –≥—Ä–∞–º–æ—Ç–Ω—ã–π, –∏, –∑—Ä–µ–ª—ã–π, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç,, ...]</td>\n",
       "      <td>[B-PERSON, I-PERSON, B-EVENT, B-PROFESSION, I-PROFESSION, I-PROFESSION, B-PERSON, I-PERSON, B-DATE, I-DATE, I-DATE, I-DATE, O, B-PROFESSION, I-PROFESSION, I-PROFESSION, B-PROFESSION, O, O, B-EVENT, B-AGE, B-PERSON, I-PERSON, O, O, O, O, O, B-PROFESSION, I-PROFESSION, I-PROFESSION, B-PROFESSION, I-PROFESSION, O, B-PROFESSION, B-PROFESSION, I-PROFESSION, B-COUNTRY, B-PERSON, I-PERSON, O, O, B-PERSON, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-COUNTRY, O, O, O, B-PERSON, O, B-PERSON, I-PERSON, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>–£–º–µ—Ä –æ–¥–∏–Ω –∏–∑ —Å–æ–∑–¥–∞—Ç–µ–ª–µ–π –º–∞–∑–µ—Ä–∞ –∏ –ª–∞–∑–µ—Ä–∞ –ß–∞—Ä–ª–∑ –¢–∞—É–Ω—Å\\n–ß–∞—Ä–ª–∑ –¢–∞—É–Ω—Å\\n–í –º–∏–Ω—É–≤—à–∏–π –≤—Ç–æ—Ä–Ω–∏–∫, 27 —è–Ω–≤–∞—Ä—è 2015 –≥–æ–¥–∞, –≤ –û–∫–ª–µ–Ω–¥–µ (—à—Ç–∞—Ç –ö–∞–ª–∏—Ñ–æ—Ä–Ω–∏—è, –°–®–ê) –≤ –≤–æ–∑—Ä–∞—Å—Ç–µ 99 –ª–µ—Ç —É–º–µ—Ä –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏–π —Ñ–∏–∑–∏–∫, –Ω–æ–±–µ–ª–µ–≤—Å–∫–∏–π –ª–∞—É—Ä–µ–∞—Ç –ß–∞—Ä–ª–∑ –¢–∞—É–Ω—Å, —á—å–∏ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è –ø—Ä–∏–≤–µ–ª–∏ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –º–∞–∑–µ—Ä–∞ –∏ –ª–∞–∑–µ—Ä–∞.\\n\\n–û—Å–Ω–æ–≤–Ω—ã–µ —Ç—Ä—É–¥—ã –¢–∞—É–Ω—Å–∞ –ø–æ—Å–≤—è—â–µ–Ω—ã —Ä–∞–¥–∏–æ—Å–ø–µ–∫—Ç—Ä–æ—Å–∫–æ–ø–∏–∏, –∫–≤–∞–Ω—Ç–æ–≤–æ–π —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–µ –∏ –µ—ë –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º, –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π –æ–ø—Ç–∏–∫–µ, —Ä–∞–¥–∏–æ–∞—Å—Ç—Ä–æ–Ω–æ–º–∏–∏. –ù–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –ü—Ä–æ—Ö–æ—Ä–æ–≤–∞ –∏ –ù–∏–∫–æ–ª–∞—è –ë–∞—Å–æ–≤–∞ –≤—ã–¥–≤–∏–Ω—É–ª –∏–¥–µ—é –Ω–æ–≤–æ–≥–æ –ø—Ä–∏–Ω—Ü–∏–ø–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —É—Å–∏–ª–µ–Ω–∏—è —ç–ª–µ–∫—Ç—Ä–æ–º–∞–≥–Ω–∏—Ç–Ω—ã—Ö –≤–æ–ª–Ω –∏ –Ω–∞ –µ–≥–æ –æ—Å–Ω–æ–≤–µ —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º–∏ —Å–æ–∑–¥–∞–ª –ø–µ—Ä–≤—ã–π –∫–≤–∞–Ω—Ç–æ–≤—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä ‚Äî –º–∞–∑–µ—Ä –Ω–∞ –∞–º–º–∏–∞–∫–µ (1954). –í 1958 –≥–æ–¥—É —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å –ê—Ä—Ç—É—Ä–æ–º –®–∞–≤–ª–æ–≤—ã–º –æ–±–æ—Å–Ω–æ–≤–∞–ª–∏ –∏ –∑–∞–ø–∞—Ç–µ–Ω—Ç–æ–≤–∞–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–æ–∑–¥–∞–Ω–∏—è –æ–ø—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ (–ª–∞–∑–µ—Ä–∞). –í 1964 –≥–æ–¥—É ¬´–∑–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–∞–±–æ—Ç—ã –≤ –æ–±–ª–∞—Å—Ç–∏ –∫–≤–∞–Ω—Ç–æ–≤–æ–π —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–≤–µ–ª–∏ –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∏–∑–ª—É—á–∞—Ç–µ–ª–µ–π –∏ —É—Å–∏–ª–∏—Ç–µ–ª–µ–π –Ω–∞ –ª–∞–∑–µ—Ä–Ω–æ-–º–∞–∑–µ—Ä–Ω–æ–º –ø—Ä–∏–Ω—Ü–∏–ø–µ¬ª, –¢–∞—É–Ω—Å —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å –ù–∏–∫–æ–ª–∞–µ–º –ë–∞—Å–æ–≤—ã–º –∏ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–º –ü—Ä–æ—Ö–æ—Ä–æ–≤—ã–º –±—ã–ª —É–¥–æ—Å—Ç–æ–µ–Ω –ù–æ–±–µ–ª–µ–≤—Å–∫–æ–π –ø—Ä–µ–º–∏–∏ –ø–æ —Ñ–∏–∑–∏–∫–µ.\\n\\n–°–æ–∑–¥–∞–Ω–Ω—ã–µ –ª–∞–∑–µ—Ä—ã –¢–∞—É–Ω—Å –ø—Ä–∏–º–µ–Ω–∏–ª –¥–ª—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ —Ç–µ–æ—Ä–∏–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –±–∏–æ–ª–æ–≥–∏–∏ –∏ –º–µ–¥–∏—Ü–∏–Ω—ã. –í –æ–±–ª–∞—Å—Ç–∏ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π –æ–ø—Ç–∏–∫–∏ –¢–∞—É–Ω—Å –æ–±–Ω–∞—Ä—É–∂–∏–ª –≤—ã–Ω—É–∂–¥–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å–µ—è–Ω–∏–µ –ú–∞–Ω–¥–µ–ª—å—à—Ç–∞–º–∞ ‚Äî –ë—Ä–∏–ª–ª—é—ç–Ω–∞, –≤–≤—ë–ª –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –º–æ—â–Ω–æ—Å—Ç–∏ –ø—É—á–∫–∞ —Å–≤–µ—Ç–∞ –∏ —è–≤–ª–µ–Ω–∏–∏ —Å–∞–º–æ—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ (1964).\\n\\n–¢–∞—É–Ω—Å –ø—Ä–∏–º–µ–Ω–∏–ª –º–µ—Ç–æ–¥—ã –∫–≤–∞–Ω—Ç–æ–≤–æ–π —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∏ –∏ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π –æ–ø—Ç–∏–∫–∏ –≤ –∞—Å—Ç—Ä–æ—Ñ–∏–∑–∏–∫–µ –∏ —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å –¥—Ä—É–≥–∏–º–∏ –≤ 1969 –æ—Ç–∫—Ä—ã–ª –º–∞–∑–µ—Ä–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç –≤ –∫–æ—Å–º–æ—Å–µ (–∏–∑–ª—É—á–µ–Ω–∏–µ –∫–æ—Å–º–∏—á–µ—Å–∫–∏—Ö –º–æ–ª–µ–∫—É–ª –≤–æ–¥—ã –Ω–∞ –¥–ª–∏–Ω–µ –≤–æ–ª–Ω—ã 1,35 —Å–º).\\n\\n–ß–∞—Ä–ª–∑ –•–∞—Ä–¥ –¢–∞—É–Ω—Å —Ä–æ–¥–∏–ª—Å—è –≤ –ì—Ä–∏–Ω–≤–∏–ª–ª–µ, —Ç–∞–º –∂–µ —É—á–∏–ª—Å—è, –≤ 1935 –≥–æ–¥—É –æ–∫–æ–Ω—á–∏–ª –§—É—Ä–º–∞–Ω—Å–∫–∏–π —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç, –≤ 1939 ‚Äî –≤ –ö–∞–ª–∏—Ñ–æ—Ä–Ω–∏–π—Å–∫–∏–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∏–Ω—Å—Ç–∏—Ç—É—Ç. –í 1939‚Äî1948 ‚Äî —Ä–∞–±–æ—Ç–∞–ª –≤ —Ñ–∏—Ä–º–µ ¬´–ë–µ–ª–ª —Ç–µ–ª–µ—Ñ–æ–Ω¬ª, –≤ 1948‚Äî1961 ‚Äî –≤ –ö–æ–ª—É–º–±–∏–π—Å–∫–æ–º —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–µ (—Å 1950 –≤ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ –ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–∞). –í 1961‚Äî1966 –¢–∞—É–Ω—Å —è–≤–ª—è–ª—Å—è –ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä–æ–º –∏ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–æ–º –ú–∞—Å—Å–∞—á—É—Å–µ—Ç—Å–∫–æ–≥–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Å—Ç–∏—Ç—É—Ç–∞, —Å 1967 –≤–æ–∑–≥–ª–∞–≤–ª—è–ª —Ñ–∏–∑–∏—á–µ—Å–∫–∏–π –æ—Ç–¥–µ–ª –ö–∞–ª–∏—Ñ–æ—Ä–Ω–∏–π—Å–∫–æ–≥–æ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞ (–ë–µ—Ä–∫–ª–∏). –í 1967 –≥–æ–¥—É –±—ã–ª –∏–∑–±—Ä–∞–Ω –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–æ–º –ê–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–≥–æ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –æ–±—â–µ—Å—Ç–≤–∞.\\n</td>\n",
       "      <td>[–£–º–µ—Ä, –æ–¥–∏–Ω, –∏–∑, —Å–æ–∑–¥–∞—Ç–µ–ª–µ–π, –º–∞–∑–µ—Ä–∞, –∏, –ª–∞–∑–µ—Ä–∞, –ß–∞—Ä–ª–∑, –¢–∞—É–Ω—Å, –ß–∞—Ä–ª–∑, –¢–∞—É–Ω—Å, –í, –º–∏–Ω—É–≤—à–∏–π, –≤—Ç–æ—Ä–Ω–∏–∫,, 27, —è–Ω–≤–∞—Ä—è, 2015, –≥–æ–¥–∞, ,, –≤, –û–∫–ª–µ–Ω–¥–µ, (—à—Ç–∞—Ç, –ö–∞–ª–∏—Ñ–æ—Ä–Ω–∏—è, ,, –°–®–ê, ), –≤, –≤–æ–∑—Ä–∞—Å—Ç–µ, 99, –ª–µ—Ç, —É–º–µ—Ä, –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏–π, —Ñ–∏–∑–∏–∫, ,, –Ω–æ–±–µ–ª–µ–≤—Å–∫–∏–π, –ª–∞—É—Ä–µ–∞—Ç, –ß–∞—Ä–ª–∑, –¢–∞—É–Ω—Å, ,, —á—å–∏, –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è, –ø—Ä–∏–≤–µ–ª–∏, –∫, —Å–æ–∑–¥–∞–Ω–∏—é, –º–∞–∑–µ—Ä–∞, –∏, –ª–∞–∑–µ—Ä–∞., –û—Å–Ω–æ–≤–Ω—ã–µ, —Ç—Ä—É–¥—ã, –¢–∞—É–Ω—Å–∞, –ø–æ—Å–≤—è—â–µ–Ω—ã, —Ä–∞–¥–∏–æ—Å–ø–µ–∫—Ç—Ä–æ—Å–∫–æ–ø–∏–∏,, –∫–≤–∞–Ω—Ç–æ–≤–æ–π, —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–µ, –∏, –µ—ë, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è–º,, –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π, –æ–ø—Ç–∏–∫–µ,, —Ä–∞–¥–∏–æ–∞—Å—Ç—Ä–æ–Ω–æ–º–∏–∏., –ù–µ–∑–∞–≤–∏—Å–∏–º–æ, –æ—Ç, –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞, –ü—Ä–æ—Ö–æ—Ä–æ–≤–∞, –∏, –ù–∏–∫–æ–ª–∞—è, –ë–∞—Å–æ–≤–∞, –≤—ã–¥–≤–∏–Ω—É–ª, –∏–¥–µ—é, –Ω–æ–≤–æ–≥–æ, –ø—Ä–∏–Ω—Ü–∏–ø–∞, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∏, —É—Å–∏–ª–µ–Ω–∏—è, —ç–ª–µ–∫—Ç—Ä–æ–º–∞–≥–Ω–∏—Ç–Ω—ã—Ö, –≤–æ–ª–Ω, –∏, –Ω–∞, –µ–≥–æ, –æ—Å–Ω–æ–≤–µ, —Å–æ–≤–º–µ—Å—Ç–Ω–æ, —Å, —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º–∏, —Å–æ–∑–¥–∞–ª, –ø–µ—Ä–≤—ã–π, –∫–≤–∞–Ω—Ç–æ–≤—ã–π, –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä, ‚Äî, –º–∞–∑–µ—Ä, –Ω–∞, –∞–º–º–∏–∞–∫–µ, (, 1954, )., –í, 1958, –≥–æ–¥—É, —Å–æ–≤–º–µ—Å—Ç–Ω–æ, —Å, –ê—Ä—Ç—É—Ä–æ–º, ...]</td>\n",
       "      <td>[B-EVENT, O, O, O, O, O, O, B-PERSON, I-PERSON, B-PERSON, I-PERSON, B-DATE, I-DATE, I-DATE, I-DATE, I-DATE, I-DATE, I-DATE, O, O, B-CITY, O, B-STATE_OR_PROVINCE, O, B-COUNTRY, O, O, O, B-AGE, I-AGE, O, B-NATIONALITY, B-PROFESSION, O, B-AWARD, O, B-PERSON, I-PERSON, O, O, O, O, O, O, O, O, O, O, O, B-PERSON, O, O, O, O, O, O, O, O, O, O, O, O, B-PERSON, I-PERSON, O, B-PERSON, I-PERSON, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORDINAL, O, O, O, O, O, O, O, B-DATE, O, B-DATE, I-DATE, I-DATE, O, O, B-PERSON, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>–ü—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å —Å–≤–æ–±–æ–¥—É –ø–µ—Ä–µ–¥–≤–∏–∂–µ–Ω–∏—è –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –õ–µ–±–µ–¥–µ–≤–∞\\n–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –õ–µ–±–µ–¥–µ–≤\\n–°–µ—Ä–≥–µ–π –ü–æ–ª–æ–Ω—Å–∫–∏–π\\n–í –ø—è—Ç–Ω–∏—Ü—É ‚Äî 28 –∏—é–Ω—è 2013 –≥–æ–¥–∞ —Ä–æ—Å—Å–∏–π—Å–∫–∞—è –ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞ –æ–±—Ä–∞—Ç–∏–ª–∞—Å—å –≤ –º–æ—Å–∫–æ–≤—Å–∫–∏–π —Å—É–¥ —Å –ø—Ä–æ—Å—å–±–æ–π –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å —Å–≤–æ–±–æ–¥—É –ø–µ—Ä–µ–¥–≤–∏–∂–µ–Ω–∏—è –º–µ–¥–∏–∞–º–∞–≥–Ω–∞—Ç–∞ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞ –õ–µ–±–µ–¥–µ–≤–∞ –Ω–∞ 21 –º–µ—Å—è—Ü –∑–∞ –∏–∑–±–∏–µ–Ω–∏–µ —Å–≤–æ–µ–≥–æ –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞ ‚Äî –°–µ—Ä–≥–µ—è –ü–æ–ª–æ–Ω—Å–∫–æ–≥–æ –Ω–∞ —Ç–µ–ª–µ—à–æ—É.\\n–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, —á—Ç–æ —Å–∞–º –ø–æ—Å—Ç—Ä–∞–¥–∞–≤—à–∏–π –ø–æ–ø—Ä–æ—Å–∏–ª —Å—É–¥ –Ω–µ –Ω–∞–∫–∞–∑—ã–≤–∞—Ç—å –æ–±–≤–∏–Ω—è–µ–º–æ–≥–æ.\\n\\n–ü—Ä–∏ —ç—Ç–æ–º –æ–±–≤–∏–Ω–µ–Ω–∏–µ –Ω–µ –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª–æ –¥–ª—è –õ–µ–±–µ–¥–µ–≤–∞ —Ç—é—Ä–µ–º–Ω–æ–≥–æ –∑–∞–∫–ª—é—á–µ–Ω–∏—è.\\n\\n–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –ø–æ–∫—Ä–æ–≤–∏—Ç–µ–ª—å –±—Ä–∏—Ç–∞–Ω—Å–∫–∏—Ö –≥–∞–∑–µ—Ç Independent –∏ London Evening Standard, –õ–µ–±–µ–¥–µ–≤ –≥–æ–≤–æ—Ä–∏—Ç, —á—Ç–æ —Å—É–¥–µ–±–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞–¥ –Ω–∏–º —è–≤–ª—è–µ—Ç—Å—è –º–µ—Å—Ç—å—é –ö—Ä–µ–º–ª—è –∫—Ä–∏—Ç–∏–∫—É –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –∏ —Å–æ–≤–ª–∞–¥–µ–ª—å—Ü—É —Ä–æ—Å—Å–∏–π—Å–∫–æ–π –≥–∞–∑–µ—Ç—ã, –∫–æ—Ç–æ—Ä–∞—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –í–ª–∞–¥–∏–º–∏—Ä–∞ –ü—É—Ç–∏–Ω–∞.\\n\\n–û–Ω —Ç–∞–∫–∂–µ –≤–∏–¥–∏—Ç –≤ —Ä–∞–∑–±–∏—Ä–∞—Ç–µ–ª—å—Å—Ç–≤–µ —Å–∏–≥–Ω–∞–ª –¥—Ä—É–≥–∏–º –º–∞–≥–Ω–∞—Ç–∞–º —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ –±–∏–∑–Ω–µ—Å–∞.\\n\\n–í –∑–∞–∫–ª—é—á–µ–Ω–∏–∏ –ø–æ –¥–µ–ª—É –õ–µ–±–µ–¥–µ–≤–∞ –ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞ –∑–∞–≤–∏–ª–∞, —á—Ç–æ –µ–≥–æ —Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–∑–Ω–∞—Ç—å –≤–∏–Ω–æ–≤–Ω—ã–º –≤ –ø—Ä–æ—è–≤–ª–µ–Ω–∏–∏ –Ω–µ–Ω–∞–≤–∏—Å—Ç–∏ –ø–æ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–º –º–æ—Ç–∏–≤–∞–º, –∑–∞–ø—Ä–µ—Ç–∏—Ç—å –º–µ–Ω—è—Ç—å –º–µ—Å—Ç–æ –∂–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –∏–ª–∏ —Ä–∞–±–æ—Ç—É –±–µ–∑ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –≤–ª–∞—Å—Ç–µ–π –∏ –ø—Ä–∏–Ω–∏–º–∞—Ç—å —É—á–∞—Å—Ç–∏–µ –≤ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –ø—É–±–ª–∏—á–Ω—ã—Ö –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π –≤ —Ç–µ—á–µ–Ω–∏–µ –≥–æ–¥–∞ –∏ –¥–µ–≤—è—Ç–∏ –º–µ—Å—è—Ü–µ–≤.\\n\\n–°—É–¥—å—è –∏–º–µ–µ—Ç –ø—Ä–∞–≤–æ –≤—ã–Ω–µ—Å—Ç–∏ –±–æ–ª–µ–µ —Å—É—Ä–æ–≤—ã–π –ø—Ä–∏–≥–æ–≤–æ—Ä, –µ—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–µ—Ç –õ–µ–±–µ–¥–µ–≤–∞ –≤–∏–Ω–æ–≤–Ω—ã–º –≤ —Ö—É–ª–∏–≥–∞–Ω—Å—Ç–≤–µ –∏ –Ω–∞–Ω–µ—Å–µ–Ω–∏–∏ –ø–æ–±–æ–µ–≤.\\n\\n–≠—Ç–∏ —Å—Ç–∞—Ç—å–∏ –ø—Ä–µ–¥—É—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –Ω–∞–∫–∞–∑–∞–Ω–∏–µ –¥–æ –ø—è—Ç–∏ –ª–µ—Ç —Ç—é—Ä–µ–º–Ω–æ–≥–æ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ, –Ω–æ –≤—ã–Ω–µ—Å–µ–Ω–∏–µ —Ç–∞–∫–æ–≥–æ –≤–µ—Ä–¥–∏–∫—Ç–∞ –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω–æ, —É—á–∏—Ç—ã–≤–∞—è –æ–±—Ä–∞—â–µ–Ω–∏–µ –ø—Ä–æ–∫—É—Ä–æ—Ä–∞.\\n\\n–†–µ—à–µ–Ω–∏–µ —Å—É–¥–∞ –æ–∂–∏–¥–∞–µ—Ç—Å—è –≤ –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫.\\n\\n–°—É–¥ –Ω–∞–¥ 53-–ª–µ—Ç–Ω–∏–º –õ–µ–±–µ–¥–µ–≤—ã–º –Ω–∞—á–∞–ª—Å—è –≤ –º–∞–µ –∏ –±—ã–ª —Å–≤—è–∑–∞–Ω —Å –¥—Ä–∞–∫–æ–π 2011 –≥–æ–¥–∞, –∫–æ–≥–¥–∞ –≤–æ –≤—Ä–µ–º—è –∑–∞–ø–∏—Å–∏ —Ç–µ–ª–µ–≤–∏–∑–∏–æ–Ω–Ω–æ–≥–æ —Ç–æ–∫-—à–æ—É –æ–Ω –≤—Å–∫–æ—á–∏–ª —Å–æ —Å—Ç—É–ª–∞ –∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞—Å —É–¥–∞—Ä–∏–ª –±–∏–∑–Ω–µ—Å–º–µ–Ω–∞ –°–µ—Ä–≥–µ—è –ü–æ–ª–æ–Ω—Å–∫–æ–≥–æ.\\n\\n–õ–µ–±–µ–¥–µ–≤ –ø—Ä–∏–∑–Ω–∞–ª —Å–≤–æ–µ —É—á–∞—Å—Ç–∏–µ –≤ –¥—Ä–∞–∫–µ, –Ω–æ –æ—Ç–≤–µ—Ä–≥ –æ–±–≤–∏–Ω–µ–Ω–∏—è –≤ —Ö—É–ª–∏–≥–∞–Ω—Å—Ç–≤–µ –∏ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–π –Ω–µ–Ω–∞–≤–∏—Å—Ç–∏. –í–æ –≤—Ä–µ–º—è –≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏—è –ø—Ä–æ–∫—É—Ä–æ—Ä–∞ –æ–Ω –º–æ–ª—á–∞ —Å–∏–¥–µ–ª, —Å–∫—Ä–µ—Å—Ç–∏–≤ –Ω–æ–≥–∏ –∏ —Ä–∞–±–æ—Ç–∞—è —Å–æ —Å–≤–æ–∏–º –ø–ª–∞–Ω—à–µ—Ç–Ω—ã–º –∫–æ–º–ø—å—é—Ç–µ—Ä–æ–º.\\n\\n–ì–ª–∞–≤–Ω–æ–π –∂–µ —Å–µ–Ω—Å–∞—Ü–∏–µ–π —Å–µ–≥–æ–¥–Ω—è—à–Ω–µ–≥–æ –∑–∞—Å–µ–¥–∞–Ω–∏—è —Å—Ç–∞–ª–æ –ø–æ—Å—Ç—É–ø–∏–≤—à–µ–µ –æ—Ç –ø–æ—Ç–µ—Ä–ø–µ–≤—à–µ–≥–æ –∑–∞—è–≤–ª–µ–Ω–∏–µ, –≤ –∫–æ—Ç–æ—Ä–æ–º –æ–Ω —è–∫–æ–±—ã –ø—Ä–æ—â–∞–µ—Ç –æ–±–≤–∏–Ω—è–µ–º–æ–≥–æ –∏ –ø—Ä–æ—Å–∏—Ç —Å—É–¥ ¬´–Ω–µ –≤—ã–Ω–æ—Å–∏—Ç—å –æ–±–≤–∏–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–≥–æ–≤–æ—Ä –õ–µ–±–µ–¥–µ–≤—É –≤ —Å–≤—è–∑–∏ —Å —Ç–µ–º, —á—Ç–æ —Ç—é—Ä—å–º—É –æ–Ω –Ω–µ –≤—ã–¥–µ—Ä–∂–∏—Ç, –∞ —à—Ç—Ä–∞—Ñ –¥–ª—è –Ω–µ–≥–æ –Ω–µ–∑–Ω–∞—á–∏–º–æ–µ –Ω–∞–∫–∞–∑–∞–Ω–∏–µ. –ü—Ä–æ—à—É –ø—Ä–æ—Å—Ç–∏—Ç—å –õ–µ–±–µ–¥–µ–≤–∞ –∑–∞ –µ–≥–æ –ø—Å–∏—Ö–∏—á–µ—Å–∫—É—é –Ω–µ—É—Ä–∞–≤–Ω–æ–≤–µ—à–µ–Ω–Ω–æ—Å—Ç—å¬ª.\\n</td>\n",
       "      <td>[–ü—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞, –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç, –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å, —Å–≤–æ–±–æ–¥—É, –ø–µ—Ä–µ–¥–≤–∏–∂–µ–Ω–∏—è, –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞, –õ–µ–±–µ–¥–µ–≤–∞, –ê–ª–µ–∫—Å–∞–Ω–¥—Ä, –õ–µ–±–µ–¥–µ–≤, –°–µ—Ä–≥–µ–π, –ü–æ–ª–æ–Ω—Å–∫–∏–π, –í, –ø—è—Ç–Ω–∏—Ü—É, ‚Äî, 28, –∏—é–Ω—è, 2013, –≥–æ–¥–∞, —Ä–æ—Å—Å–∏–π—Å–∫–∞—è, —Ä–æ—Å—Å–∏–π—Å–∫–∞—è, –ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞, –ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞, –æ–±—Ä–∞—Ç–∏–ª–∞—Å—å, –≤, –º–æ—Å–∫–æ–≤—Å–∫–∏–π, –º–æ—Å–∫–æ–≤—Å–∫–∏–π, —Å—É–¥, —Å, –ø—Ä–æ—Å—å–±–æ–π, –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å, —Å–≤–æ–±–æ–¥—É, –ø–µ—Ä–µ–¥–≤–∏–∂–µ–Ω–∏—è, –º–µ–¥–∏–∞–º–∞–≥–Ω–∞—Ç–∞, –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∞, –õ–µ–±–µ–¥–µ–≤–∞, –Ω–∞, 21, –º–µ—Å—è—Ü, –∑–∞, –∏–∑–±–∏–µ–Ω–∏–µ, —Å–≤–æ–µ–≥–æ, –æ–ø–ø–æ–Ω–µ–Ω—Ç–∞, ‚Äî, –°–µ—Ä–≥–µ—è, –ü–æ–ª–æ–Ω—Å–∫–æ–≥–æ, –Ω–∞, —Ç–µ–ª–µ—à–æ—É., –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ,, —á—Ç–æ, —Å–∞–º, –ø–æ—Å—Ç—Ä–∞–¥–∞–≤—à–∏–π, –ø–æ–ø—Ä–æ—Å–∏–ª, —Å—É–¥, –Ω–µ, –Ω–∞–∫–∞–∑—ã–≤–∞—Ç—å, –æ–±–≤–∏–Ω—è–µ–º–æ–≥–æ., –ü—Ä–∏, —ç—Ç–æ–º, –æ–±–≤–∏–Ω–µ–Ω–∏–µ, –Ω–µ, –ø–æ—Ç—Ä–µ–±–æ–≤–∞–ª–æ, –¥–ª—è, –õ–µ–±–µ–¥–µ–≤–∞, —Ç—é—Ä–µ–º–Ω–æ–≥–æ, –∑–∞–∫–ª—é—á–µ–Ω–∏—è, ., –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π, –ø–æ–∫—Ä–æ–≤–∏—Ç–µ–ª—å, –±—Ä–∏—Ç–∞–Ω—Å–∫–∏—Ö, –≥–∞–∑–µ—Ç, Independent, –∏, London, Evening, Standard, London, Evening, Standard,, –õ–µ–±–µ–¥–µ–≤, –≥–æ–≤–æ—Ä–∏—Ç,, —á—Ç–æ, —Å—É–¥–µ–±–Ω—ã–π, –ø—Ä–æ—Ü–µ—Å—Å, –Ω–∞–¥, –Ω–∏–º, —è–≤–ª—è–µ—Ç—Å—è, –º–µ—Å—Ç—å—é, –ö—Ä–µ–º–ª—è, –∫—Ä–∏—Ç–∏–∫—É, –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞, –∏, —Å–æ–≤–ª–∞–¥–µ–ª—å—Ü—É, —Ä–æ—Å—Å–∏–π—Å–∫–æ–π, –≥–∞–∑–µ—Ç—ã, —Ä–æ—Å—Å–∏–π—Å–∫–æ–π, –≥–∞–∑–µ—Ç—ã,, –∫–æ—Ç–æ—Ä–∞—è, –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞, –≤, ...]</td>\n",
       "      <td>[B-ORGANIZATION, O, B-PENALTY, I-PENALTY, I-PENALTY, B-PERSON, I-PERSON, B-PERSON, I-PERSON, B-PERSON, I-PERSON, B-DATE, I-DATE, I-DATE, I-DATE, I-DATE, I-DATE, I-DATE, B-COUNTRY, B-ORGANIZATION, I-ORGANIZATION, B-ORGANIZATION, O, O, B-CITY, B-ORGANIZATION, I-ORGANIZATION, O, O, B-PENALTY, I-PENALTY, I-PENALTY, B-PROFESSION, B-PERSON, I-PERSON, B-DATE, I-DATE, I-DATE, O, B-CRIME, O, O, O, B-PERSON, I-PERSON, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-PERSON, B-PENALTY, I-PENALTY, O, O, O, B-COUNTRY, O, B-ORGANIZATION, O, B-ORGANIZATION, I-ORGANIZATION, I-ORGANIZATION, B-CITY, O, O, B-PERSON, O, O, B-EVENT, I-EVENT, O, O, O, O, B-ORGANIZATION, O, B-ORGANIZATION, O, O, B-ORGANIZATION, I-ORGANIZATION, B-COUNTRY, O, O, O, O, O, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 268
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a ü§ó Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
    "\n",
    "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "- we download the vocabulary used when pretraining this specific checkpoint.\n",
    "\n",
    "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eXNLu_-nIrJI",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:15.711264Z",
     "start_time": "2024-04-10T09:22:14.648741Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ],
   "outputs": [],
   "execution_count": 269
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "The following assertion ensures that our tokenizer is a fast tokenizers (backed by Rust) from the ü§ó Tokenizers library. Those fast tokenizers are available for almost all models, and we will need some of the special features they have for our preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-n0_1lnuYoUo",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:15.735617Z",
     "start_time": "2024-04-10T09:22:15.722620Z"
    }
   },
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ],
   "outputs": [],
   "execution_count": 270
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nZHUROIYoUo"
   },
   "source": [
    "You can check which type of models have a fast tokenizer available and which don't on the [big table of models](https://huggingface.co/transformers/index.html#bigtable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rowT4iCLIrJK"
   },
   "source": [
    "You can directly call this tokenizer on one sentence:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a5hBlsrHIrJL",
    "outputId": "acdaa98a-a8cd-4a20-89b8-cc26437bbe90",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:16.150438Z",
     "start_time": "2024-04-10T09:22:16.137458Z"
    }
   },
   "source": [
    "tokenizer(\"Hello, this is one sentence!\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 67124, 70471, 121, 26802, 13218, 30046, 118080, 12894, 177, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 271
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i28NrDEPYoUt"
   },
   "source": [
    "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n",
    "\n",
    "If, as is the case here, your inputs have already been split into words, you should pass the list of words to your tokenzier with the argument `is_split_into_words=True`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pteRZjrYYoUt",
    "outputId": "0eb65824-32c6-47e6-faab-298f97ae1749",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:16.906183Z",
     "start_time": "2024-04-10T09:22:16.899938Z"
    }
   },
   "source": [
    "tokenizer([\"Hello\", \",\", \"this\", \"is\", \"one\", \"sentence\", \"split\", \"into\", \"words\", \".\"], is_split_into_words=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 67124, 70471, 121, 26802, 13218, 30046, 118080, 12894, 16994, 69821, 443, 42038, 119660, 454, 126, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 272
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gati9HKdYoUt"
   },
   "source": [
    "Note that transformers are often pretrained with subword tokenizers, meaning that even if your inputs have been split into words already, each of those words could be split again by the tokenizer. Let's look at an example of that:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O3HfxAqUYoUt",
    "outputId": "5404d8cc-5203-4b13-c9d7-0cb393201451",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:17.732955Z",
     "start_time": "2024-04-10T09:22:17.726517Z"
    }
   },
   "source": [
    "example = train_dataset[4]\n",
    "print(example[\"tokens\"])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facebook', '–Ω–∞—à–µ–ª', '–Ω–æ–≤–æ–≥–æ', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–∞', '–§–∏–Ω–∞–Ω—Å–æ–≤—ã–º', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º', '—Å–æ—Ü–∏–∞–ª—å–Ω–æ–π', '—Å–µ—Ç–∏', 'Facebook', '–Ω–∞–∑–Ω–∞—á–µ–Ω', '39-–ª–µ—Ç–Ω–∏–π', '–î—ç–≤–∏–¥', '–≠–±–µ—Ä—Å–º–∞–Ω', '(', 'David', 'Ebersman', '),', '—Å–æ–æ–±—â–∞–µ—Ç', 'The', 'Wall', 'Street', 'Journal', '.', '–ù–∞', '—Ä–∞–±–æ—Ç—É', '–≤', 'Facebook', '–æ–Ω', '–≤—ã–π–¥–µ—Ç', '–≤', '—Å–µ–Ω—Ç—è–±—Ä–µ', '.', '–†–∞–Ω–µ–µ', '–≠–±–µ—Ä—Å–º–∞–Ω', '–±—ã–ª', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º', '–±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π', '–∫–æ–º–ø–∞–Ω–∏–∏', 'Genentech', '.', '–≠–±–µ—Ä—Å–º–∞–Ω', '–ø–æ–¥—á–µ—Ä–∫–Ω—É–ª,', '—á—Ç–æ', '–≤–∏–¥–∏—Ç', '–º–Ω–æ–≥–æ', '–æ–±—â–µ–≥–æ', '–º–µ–∂–¥—É', 'Facebook', '–∏', 'Genentech', '.', '–í', '—á–∞—Å—Ç–Ω–æ—Å—Ç–∏,', '—ç—Ç–æ', '–¥–≤–µ', '–±—ã—Å—Ç—Ä–æ—Ä–∞—Å—Ç—É—â–∏–µ', '–∫–æ–º–ø–∞–Ω–∏–∏', '—Å', '—Å–∏–ª—å–Ω–æ–π', '–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π', '–∫—É–ª—å—Ç—É—Ä–æ–π.', '–¢–∞–∫–∂–µ', '–æ–Ω', '–∑–∞—è–≤–∏–ª,', '—á—Ç–æ', 'Facebook', '–æ–∂–∏–¥–∞–µ—Ç', '70-–ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ', '—É–≤–µ–ª–∏—á–µ–Ω–∏–µ', '–≤—ã—Ä—É—á–∫–∏', '–≤', '2009', '–≥–æ–¥—É', '.', '–í', '–∫–æ–º–ø–∞–Ω–∏–∏', 'Genentech', '–î—ç–≤–∏–¥', '–≠–±–µ—Ä—Å–º–∞–Ω', '–ø—Ä–æ—Ä–∞–±–æ—Ç–∞–ª', '15', '–ª–µ—Ç', '.', '–ï–µ', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º', '–æ–Ω', '—Å—Ç–∞–ª', '–≤', '2006', '–≥–æ–¥—É', '.', '–ù–∞', '—ç—Ç–æ–π', '–¥–æ–ª–∂–Ω–æ—Å—Ç–∏', '–≠–±–µ—Ä—Å–º–∞–Ω', '–ø—Ä–æ—Ä–∞–±–æ—Ç–∞–ª', '–¥–æ', '–∞–ø—Ä–µ–ª—è', '2009', '–≥–æ–¥–∞', ',', '–∫–æ–≥–¥–∞', 'Roche', 'Holding', '–∫—É–ø–∏–ª', 'Genentech', '.', '–ü–æ', '–¥–∞–Ω–Ω—ã–º', 'The', 'Wall', 'Street', 'Journal', ',', '–Ω–∞', '–¥–æ–ª–∂–Ω–æ—Å—Ç—å', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–∞', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–∞', 'Facebook', 'Facebook', '–ø—Ä–µ—Ç–µ–Ω–¥–æ–≤–∞–ª–∏', '11', '–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤.', '–ö–∞–∂–¥—ã–π', '–∏–∑', '–Ω–∏—Ö', '–ø—Ä–æ—à–µ–ª', '—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ', '—Å', '–æ—Å–Ω–æ–≤–∞—Ç–µ–ª–µ–º', 'Facebook', '–ú–∞—Ä–∫–æ–º', '–¶—É–∫–µ—Ä–±–µ—Ä–≥–æ–º', '(', 'Mark', 'Zuckerberg', ')', '–∏', '–¥—Ä—É–≥–∏–º–∏', '—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è–º–∏', '–∫–æ–º–ø–∞–Ω–∏–∏', '.', '–†–∞–Ω–µ–µ', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º', 'Facebook', 'Facebook', '–±—ã–ª', '–ì–∏–¥–µ–æ–Ω', '–Æ', '(', 'Gideon', 'Yu', ').', '–û–Ω', '–ø–æ–∫–∏–Ω—É–ª', '–∫–æ–º–ø–∞–Ω–∏—é', '—Ç—Ä–∏', '–º–µ—Å—è—Ü–∞', '–Ω–∞–∑–∞–¥', '.', '–í–º–µ—Å—Ç–æ', '–Ω–µ–≥–æ', '—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è', '—Å–µ—Ç—å', '—Ä–µ—à–∏–ª–∞', '–Ω–∞–Ω—è—Ç—å', '—Ç–æ–ø-–º–µ–Ω–µ–¥–∂–µ—Ä–∞', '—Å', '–æ–ø—ã—Ç–æ–º', '—Ä–∞–±–æ—Ç—ã', '–≤', '–ø—É–±–ª–∏—á–Ω–æ–π', '–∫–æ–º–ø–∞–Ω–∏–∏.', 'Facebook', '—è–≤–ª—è–µ—Ç—Å—è', '–∫—Ä—É–ø–Ω–µ–π—à–µ–π', '–∑–∞–ø–∞–¥–Ω–æ–π', '—Å–æ—Ü–∏–∞–ª—å–Ω–æ–π', '—Å–µ—Ç—å—é.', '–ó–∞', '–ø—è—Ç—å', '–ª–µ—Ç', '—Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è', '—ç—Ç–æ–≥–æ', '–ø—Ä–æ–µ–∫—Ç–∞', '–µ–≥–æ', '–∞—É–¥–∏—Ç–æ—Ä–∏—è', '–ø—Ä–µ–≤—ã—Å–∏–ª–∞', '200', '–º–∏–ª–ª–∏–æ–Ω–æ–≤', '—á–µ–ª–æ–≤–µ–∫.']\n"
     ]
    }
   ],
   "execution_count": 273
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MPiR1SNxYoUt",
    "outputId": "3249e0f4-957a-4013-e1ed-9f67146c0c2f",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:18.244377Z",
     "start_time": "2024-04-10T09:22:18.237036Z"
    }
   },
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'facebook', '–Ω–∞—à–µ–ª', '–Ω–æ–≤–æ–≥–æ', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–∞', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º', '—Å–æ—Ü–∏–∞–ª—å–Ω–æ', '##–∏', '—Å–µ—Ç–∏', 'facebook', '–Ω–∞–∑–Ω–∞—á–µ–Ω', '39', '-', '–ª–µ—Ç', '##–Ω–∏–∏', '–¥—ç', '##–≤–∏–¥', '—ç', '##–±–µ—Ä', '##—Å–º–∞–Ω', '(', 'dav', '##id', 'e', '##ber', '##sm', '##an', ')', ',', '—Å–æ–æ–±—â–∞–µ—Ç', 'the', 'w', '##all', 'str', '##ee', '##t', 'j', '##ournal', '.', '–Ω–∞', '—Ä–∞–±–æ—Ç—É', '–≤', 'facebook', '–æ–Ω', '–≤—ã–∏', '##–¥–µ—Ç', '–≤', '—Å–µ–Ω—Ç—è–±—Ä–µ', '.', '—Ä–∞–Ω–µ–µ', '—ç', '##–±–µ—Ä', '##—Å–º–∞–Ω', '–±—ã–ª', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º', '–±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏', '##—á–µ—Å–∫–æ', '##–∏', '–∫–æ–º–ø–∞–Ω–∏–∏', 'gen', '##ente', '##ch', '.', '—ç', '##–±–µ—Ä', '##—Å–º–∞–Ω', '–ø–æ–¥—á–µ—Ä–∫–Ω—É–ª', ',', '—á—Ç–æ', '–≤–∏–¥–∏—Ç', '–º–Ω–æ–≥–æ', '–æ–±—â–µ–≥–æ', '–º–µ–∂–¥—É', 'facebook', '–∏', 'gen', '##ente', '##ch', '.', '–≤', '—á–∞—Å—Ç–Ω–æ—Å—Ç–∏', ',', '—ç—Ç–æ', '–¥–≤–µ', '–±—ã—Å—Ç—Ä–æ—Ä–∞—Å—Ç—É', '##—â–∏–µ', '–∫–æ–º–ø–∞–Ω–∏–∏', '—Å', '—Å–∏–ª—å–Ω–æ', '##–∏', '–∫–æ—Ä–ø–æ—Ä–∞', '##—Ç–∏–≤–Ω–æ', '##–∏', '–∫—É–ª—å—Ç—É—Ä–æ', '##–∏', '.', '—Ç–∞–∫–∂–µ', '–æ–Ω', '–∑–∞—è–≤–∏–ª', ',', '—á—Ç–æ', 'facebook', '–æ–∂–∏–¥–∞–µ—Ç', '70', '-', '–ø—Ä–æ—Ü–µ–Ω—Ç', '##–Ω–æ–µ', '—É–≤–µ–ª–∏—á–µ–Ω–∏–µ', '–≤—ã—Ä—É—á–∫–∏', '–≤', '2009', '–≥–æ–¥—É', '.', '–≤', '–∫–æ–º–ø–∞–Ω–∏–∏', 'gen', '##ente', '##ch', '–¥—ç', '##–≤–∏–¥', '—ç', '##–±–µ—Ä', '##—Å–º–∞–Ω', '–ø—Ä–æ—Ä–∞–±–æ—Ç–∞–ª', '15', '–ª–µ—Ç', '.', '–µ–µ', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º', '–æ–Ω', '—Å—Ç–∞–ª', '–≤', '2006', '–≥–æ–¥—É', '.', '–Ω–∞', '—ç—Ç–æ', '##–∏', '–¥–æ–ª–∂–Ω–æ—Å—Ç–∏', '—ç', '##–±–µ—Ä', '##—Å–º–∞–Ω', '–ø—Ä–æ—Ä–∞–±–æ—Ç–∞–ª', '–¥–æ', '–∞–ø—Ä–µ–ª—è', '2009', '–≥–æ–¥–∞', ',', '–∫–æ–≥–¥–∞', 'ro', '##che', 'h', '##old', '##ing', '–∫—É–ø–∏–ª', 'gen', '##ente', '##ch', '.', '–ø–æ', '–¥–∞–Ω–Ω—ã–º', 'the', 'w', '##all', 'str', '##ee', '##t', 'j', '##ournal', ',', '–Ω–∞', '–¥–æ–ª–∂–Ω–æ—Å—Ç—å', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–∞', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–∞', 'facebook', 'facebook', '–ø—Ä–µ—Ç–µ–Ω–¥–æ–≤–∞–ª–∏', '11', '–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤', '.', '–∫–∞–∂–¥—ã', '##–∏', '–∏–∑', '–Ω–∏—Ö', '–ø—Ä–æ—à–µ–ª', '—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ', '—Å', '–æ—Å–Ω–æ–≤–∞—Ç–µ–ª–µ–º', 'facebook', '–º–∞—Ä–∫', '##–æ–º', '—Ü—É', '##–∫–µ—Ä', '##–±–µ—Ä–≥–æ–º', '(', 'mark', 'zu', '##ck', '##er', '##berg', ')', '–∏', '–¥—Ä—É–≥–∏–º–∏', '—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è–º–∏', '–∫–æ–º–ø–∞–Ω–∏–∏', '.', '—Ä–∞–Ω–µ–µ', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º', 'facebook', 'facebook', '–±—ã–ª', '–≥–∏–¥', '##–µ', '##–æ–Ω', '—é', '(', 'g', '##ideo', '##n', 'y', '##u', ')', '.', '–æ–Ω', '–ø–æ–∫–∏–Ω—É–ª', '–∫–æ–º–ø–∞–Ω–∏—é', '—Ç—Ä–∏', '–º–µ—Å—è—Ü–∞', '–Ω–∞–∑–∞–¥', '.', '–≤–º–µ—Å—Ç–æ', '–Ω–µ–≥–æ', '—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è', '—Å–µ—Ç—å', '—Ä–µ—à–∏–ª–∞', '–Ω–∞–Ω—è—Ç—å', '—Ç–æ–ø', '-', '–º–µ–Ω–µ–¥–∂–µ—Ä–∞', '—Å', '–æ–ø—ã—Ç–æ–º', '—Ä–∞–±–æ—Ç—ã', '–≤', '–ø—É–±–ª–∏—á–Ω–æ', '##–∏', '–∫–æ–º–ø–∞–Ω–∏–∏', '.', 'facebook', '—è–≤–ª—è–µ—Ç—Å—è', '–∫—Ä—É–ø', '##–Ω–µ', '##–∏', '##—à–µ', '##–∏', '–∑–∞–ø–∞–¥–Ω–æ', '##–∏', '—Å–æ—Ü–∏–∞–ª—å–Ω–æ', '##–∏', '—Å–µ—Ç—å—é', '.', '–∑–∞', '–ø—è—Ç—å', '–ª–µ—Ç', '—Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è', '—ç—Ç–æ–≥–æ', '–ø—Ä–æ–µ–∫—Ç–∞', '–µ–≥–æ', '–∞—É–¥–∏—Ç–æ—Ä–∏—è', '–ø—Ä–µ–≤—ã—Å–∏–ª–∞', '200', '–º–∏–ª–ª–∏–æ–Ω–æ–≤', '—á–µ–ª–æ–≤–µ–∫', '.', '[SEP]']\n"
     ]
    }
   ],
   "execution_count": 274
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPuTUJPhYoUt"
   },
   "source": [
    "Here the words \"Zwingmann\" and \"sheepmeat\" have been split in three subtokens.\n",
    "\n",
    "This means that we need to do some processing on our labels as the input ids returned by the tokenizer are longer than the lists of labels our dataset contain, first because some special tokens might be added (we can a `[CLS]` and a `[SEP]` above) and then because of those possible splits of words in multiple tokens:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P21pkSmiYoUt",
    "outputId": "aca85ec0-3119-4967-dda8-91e24e4a9fd8",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:19.216967Z",
     "start_time": "2024-04-10T09:22:19.212038Z"
    }
   },
   "source": [
    "len(example[f\"{task}_tags\"]), len(tokenized_input[\"input_ids\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199, 283)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 275
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdWxuHvoYoUu"
   },
   "source": [
    "Thankfully, the tokenizer returns outputs that have a `word_ids` method which can help us."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3FWUGBqQYoUu",
    "outputId": "7dbf269e-87d0-46c7-cd29-9244d43f08a7",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:20.530392Z",
     "start_time": "2024-04-10T09:22:20.519374Z"
    }
   },
   "source": [
    "print(tokenized_input.word_ids())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11, 11, 11, 12, 12, 13, 13, 13, 14, 15, 15, 16, 16, 16, 16, 17, 17, 18, 19, 20, 20, 21, 21, 21, 22, 22, 23, 24, 25, 26, 27, 28, 29, 29, 30, 31, 32, 33, 34, 34, 34, 35, 36, 37, 38, 38, 38, 39, 40, 40, 40, 41, 42, 42, 42, 43, 43, 44, 45, 46, 47, 48, 49, 50, 51, 51, 51, 52, 53, 54, 54, 55, 56, 57, 57, 58, 59, 60, 60, 61, 61, 61, 62, 62, 62, 63, 64, 65, 65, 66, 67, 68, 69, 69, 69, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 78, 78, 79, 79, 80, 80, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 95, 96, 97, 97, 97, 98, 99, 100, 101, 102, 103, 104, 105, 105, 106, 106, 106, 107, 108, 108, 108, 109, 110, 111, 112, 113, 113, 114, 114, 114, 115, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 127, 128, 128, 129, 130, 131, 132, 133, 134, 135, 136, 136, 137, 137, 137, 138, 139, 140, 140, 140, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 155, 155, 156, 157, 158, 158, 158, 159, 159, 160, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 174, 174, 175, 176, 177, 178, 179, 179, 180, 180, 181, 182, 183, 183, 183, 183, 183, 184, 184, 185, 185, 186, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 198, None]\n"
     ]
    }
   ],
   "execution_count": 276
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hX6jEEBzYoUu"
   },
   "source": [
    "As we can see, it returns a list with the same number of elements as our processed input ids, mapping special tokens to `None` and all other tokens to their respective word. This way, we can align the labels with the processed input ids."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XY0QUrK5YoUu",
    "outputId": "bddcceab-a963-4483-c48a-cd5394a46cb6",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:21.703751Z",
     "start_time": "2024-04-10T09:22:21.692054Z"
    }
   },
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[f\"{task}_tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))\n",
    "for i in range(len(aligned_labels)):\n",
    "    print(*tokenizer.convert_ids_to_tokens([tokenized_input[\"input_ids\"][i]]), aligned_labels[i])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283 283\n",
      "[CLS] -100\n",
      "facebook B-ORGANIZATION\n",
      "–Ω–∞—à–µ–ª O\n",
      "–Ω–æ–≤–æ–≥–æ O\n",
      "—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ B-PROFESSION\n",
      "–¥–∏—Ä–µ–∫—Ç–æ—Ä–∞ I-PROFESSION\n",
      "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º B-PROFESSION\n",
      "–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º I-PROFESSION\n",
      "—Å–æ—Ü–∏–∞–ª—å–Ω–æ O\n",
      "##–∏ O\n",
      "—Å–µ—Ç–∏ O\n",
      "facebook B-ORGANIZATION\n",
      "–Ω–∞–∑–Ω–∞—á–µ–Ω B-EVENT\n",
      "39 B-AGE\n",
      "- B-AGE\n",
      "–ª–µ—Ç B-AGE\n",
      "##–Ω–∏–∏ B-AGE\n",
      "–¥—ç B-PERSON\n",
      "##–≤–∏–¥ B-PERSON\n",
      "—ç I-PERSON\n",
      "##–±–µ—Ä I-PERSON\n",
      "##—Å–º–∞–Ω I-PERSON\n",
      "( O\n",
      "dav B-PERSON\n",
      "##id B-PERSON\n",
      "e I-PERSON\n",
      "##ber I-PERSON\n",
      "##sm I-PERSON\n",
      "##an I-PERSON\n",
      ") O\n",
      ", O\n",
      "—Å–æ–æ–±—â–∞–µ—Ç O\n",
      "the B-ORGANIZATION\n",
      "w I-ORGANIZATION\n",
      "##all I-ORGANIZATION\n",
      "str I-ORGANIZATION\n",
      "##ee I-ORGANIZATION\n",
      "##t I-ORGANIZATION\n",
      "j I-ORGANIZATION\n",
      "##ournal I-ORGANIZATION\n",
      ". O\n",
      "–Ω–∞ O\n",
      "—Ä–∞–±–æ—Ç—É O\n",
      "–≤ O\n",
      "facebook B-ORGANIZATION\n",
      "–æ–Ω O\n",
      "–≤—ã–∏ O\n",
      "##–¥–µ—Ç O\n",
      "–≤ B-DATE\n",
      "—Å–µ–Ω—Ç—è–±—Ä–µ I-DATE\n",
      ". O\n",
      "—Ä–∞–Ω–µ–µ O\n",
      "—ç B-PERSON\n",
      "##–±–µ—Ä B-PERSON\n",
      "##—Å–º–∞–Ω B-PERSON\n",
      "–±—ã–ª O\n",
      "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º B-PROFESSION\n",
      "–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º I-PROFESSION\n",
      "–±–∏–æ—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏ O\n",
      "##—á–µ—Å–∫–æ O\n",
      "##–∏ O\n",
      "–∫–æ–º–ø–∞–Ω–∏–∏ O\n",
      "gen B-ORGANIZATION\n",
      "##ente B-ORGANIZATION\n",
      "##ch B-ORGANIZATION\n",
      ". O\n",
      "—ç B-PERSON\n",
      "##–±–µ—Ä B-PERSON\n",
      "##—Å–º–∞–Ω B-PERSON\n",
      "–ø–æ–¥—á–µ—Ä–∫–Ω—É–ª O\n",
      ", O\n",
      "—á—Ç–æ O\n",
      "–≤–∏–¥–∏—Ç O\n",
      "–º–Ω–æ–≥–æ O\n",
      "–æ–±—â–µ–≥–æ O\n",
      "–º–µ–∂–¥—É O\n",
      "facebook B-ORGANIZATION\n",
      "–∏ O\n",
      "gen B-ORGANIZATION\n",
      "##ente B-ORGANIZATION\n",
      "##ch B-ORGANIZATION\n",
      ". O\n",
      "–≤ O\n",
      "—á–∞—Å—Ç–Ω–æ—Å—Ç–∏ O\n",
      ", O\n",
      "—ç—Ç–æ O\n",
      "–¥–≤–µ B-NUMBER\n",
      "–±—ã—Å—Ç—Ä–æ—Ä–∞—Å—Ç—É O\n",
      "##—â–∏–µ O\n",
      "–∫–æ–º–ø–∞–Ω–∏–∏ O\n",
      "—Å O\n",
      "—Å–∏–ª—å–Ω–æ O\n",
      "##–∏ O\n",
      "–∫–æ—Ä–ø–æ—Ä–∞ O\n",
      "##—Ç–∏–≤–Ω–æ O\n",
      "##–∏ O\n",
      "–∫—É–ª—å—Ç—É—Ä–æ O\n",
      "##–∏ O\n",
      ". O\n",
      "—Ç–∞–∫–∂–µ O\n",
      "–æ–Ω O\n",
      "–∑–∞—è–≤–∏–ª O\n",
      ", O\n",
      "—á—Ç–æ O\n",
      "facebook B-ORGANIZATION\n",
      "–æ–∂–∏–¥–∞–µ—Ç O\n",
      "70 B-PERCENT\n",
      "- B-PERCENT\n",
      "–ø—Ä–æ—Ü–µ–Ω—Ç B-PERCENT\n",
      "##–Ω–æ–µ B-PERCENT\n",
      "—É–≤–µ–ª–∏—á–µ–Ω–∏–µ O\n",
      "–≤—ã—Ä—É—á–∫–∏ O\n",
      "–≤ B-DATE\n",
      "2009 I-DATE\n",
      "–≥–æ–¥—É I-DATE\n",
      ". O\n",
      "–≤ O\n",
      "–∫–æ–º–ø–∞–Ω–∏–∏ O\n",
      "gen B-ORGANIZATION\n",
      "##ente B-ORGANIZATION\n",
      "##ch B-ORGANIZATION\n",
      "–¥—ç B-PERSON\n",
      "##–≤–∏–¥ B-PERSON\n",
      "—ç I-PERSON\n",
      "##–±–µ—Ä I-PERSON\n",
      "##—Å–º–∞–Ω I-PERSON\n",
      "–ø—Ä–æ—Ä–∞–±–æ—Ç–∞–ª O\n",
      "15 B-DATE\n",
      "–ª–µ—Ç I-DATE\n",
      ". O\n",
      "–µ–µ O\n",
      "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º B-PROFESSION\n",
      "–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º I-PROFESSION\n",
      "–æ–Ω O\n",
      "—Å—Ç–∞–ª O\n",
      "–≤ B-DATE\n",
      "2006 I-DATE\n",
      "–≥–æ–¥—É I-DATE\n",
      ". O\n",
      "–Ω–∞ O\n",
      "—ç—Ç–æ O\n",
      "##–∏ O\n",
      "–¥–æ–ª–∂–Ω–æ—Å—Ç–∏ O\n",
      "—ç B-PERSON\n",
      "##–±–µ—Ä B-PERSON\n",
      "##—Å–º–∞–Ω B-PERSON\n",
      "–ø—Ä–æ—Ä–∞–±–æ—Ç–∞–ª O\n",
      "–¥–æ B-DATE\n",
      "–∞–ø—Ä–µ–ª—è I-DATE\n",
      "2009 I-DATE\n",
      "–≥–æ–¥–∞ I-DATE\n",
      ", O\n",
      "–∫–æ–≥–¥–∞ O\n",
      "ro B-ORGANIZATION\n",
      "##che B-ORGANIZATION\n",
      "h I-ORGANIZATION\n",
      "##old I-ORGANIZATION\n",
      "##ing I-ORGANIZATION\n",
      "–∫—É–ø–∏–ª B-EVENT\n",
      "gen B-ORGANIZATION\n",
      "##ente B-ORGANIZATION\n",
      "##ch B-ORGANIZATION\n",
      ". O\n",
      "–ø–æ O\n",
      "–¥–∞–Ω–Ω—ã–º O\n",
      "the B-ORGANIZATION\n",
      "w I-ORGANIZATION\n",
      "##all I-ORGANIZATION\n",
      "str I-ORGANIZATION\n",
      "##ee I-ORGANIZATION\n",
      "##t I-ORGANIZATION\n",
      "j I-ORGANIZATION\n",
      "##ournal I-ORGANIZATION\n",
      ", O\n",
      "–Ω–∞ O\n",
      "–¥–æ–ª–∂–Ω–æ—Å—Ç—å O\n",
      "—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ B-PROFESSION\n",
      "–¥–∏—Ä–µ–∫—Ç–æ—Ä–∞ I-PROFESSION\n",
      "—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ B-PROFESSION\n",
      "–¥–∏—Ä–µ–∫—Ç–æ—Ä–∞ I-PROFESSION\n",
      "facebook I-PROFESSION\n",
      "facebook B-ORGANIZATION\n",
      "–ø—Ä–µ—Ç–µ–Ω–¥–æ–≤–∞–ª–∏ O\n",
      "11 B-NUMBER\n",
      "–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ O\n",
      ". O\n",
      "–∫–∞–∂–¥—ã O\n",
      "##–∏ O\n",
      "–∏–∑ O\n",
      "–Ω–∏—Ö O\n",
      "–ø—Ä–æ—à–µ–ª O\n",
      "—Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ O\n",
      "—Å O\n",
      "–æ—Å–Ω–æ–≤–∞—Ç–µ–ª–µ–º O\n",
      "facebook B-ORGANIZATION\n",
      "–º–∞—Ä–∫ B-PERSON\n",
      "##–æ–º B-PERSON\n",
      "—Ü—É I-PERSON\n",
      "##–∫–µ—Ä I-PERSON\n",
      "##–±–µ—Ä–≥–æ–º I-PERSON\n",
      "( O\n",
      "mark B-PERSON\n",
      "zu I-PERSON\n",
      "##ck I-PERSON\n",
      "##er I-PERSON\n",
      "##berg I-PERSON\n",
      ") O\n",
      "–∏ O\n",
      "–¥—Ä—É–≥–∏–º–∏ O\n",
      "—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è–º–∏ B-PROFESSION\n",
      "–∫–æ–º–ø–∞–Ω–∏–∏ I-PROFESSION\n",
      ". O\n",
      "—Ä–∞–Ω–µ–µ O\n",
      "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º B-PROFESSION\n",
      "–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º I-PROFESSION\n",
      "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º B-PROFESSION\n",
      "–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º I-PROFESSION\n",
      "facebook I-PROFESSION\n",
      "facebook B-ORGANIZATION\n",
      "–±—ã–ª O\n",
      "–≥–∏–¥ B-PERSON\n",
      "##–µ B-PERSON\n",
      "##–æ–Ω B-PERSON\n",
      "—é I-PERSON\n",
      "( O\n",
      "g B-PERSON\n",
      "##ideo B-PERSON\n",
      "##n B-PERSON\n",
      "y I-PERSON\n",
      "##u I-PERSON\n",
      ") O\n",
      ". O\n",
      "–æ–Ω O\n",
      "–ø–æ–∫–∏–Ω—É–ª B-EVENT\n",
      "–∫–æ–º–ø–∞–Ω–∏—é I-EVENT\n",
      "—Ç—Ä–∏ B-DATE\n",
      "–º–µ—Å—è—Ü–∞ I-DATE\n",
      "–Ω–∞–∑–∞–¥ I-DATE\n",
      ". O\n",
      "–≤–º–µ—Å—Ç–æ O\n",
      "–Ω–µ–≥–æ O\n",
      "—Å–æ—Ü–∏–∞–ª—å–Ω–∞—è O\n",
      "—Å–µ—Ç—å O\n",
      "—Ä–µ—à–∏–ª–∞ O\n",
      "–Ω–∞–Ω—è—Ç—å O\n",
      "—Ç–æ–ø B-PROFESSION\n",
      "- B-PROFESSION\n",
      "–º–µ–Ω–µ–¥–∂–µ—Ä–∞ B-PROFESSION\n",
      "—Å O\n",
      "–æ–ø—ã—Ç–æ–º O\n",
      "—Ä–∞–±–æ—Ç—ã O\n",
      "–≤ O\n",
      "–ø—É–±–ª–∏—á–Ω–æ O\n",
      "##–∏ O\n",
      "–∫–æ–º–ø–∞–Ω–∏–∏ O\n",
      ". O\n",
      "facebook B-ORGANIZATION\n",
      "—è–≤–ª—è–µ—Ç—Å—è O\n",
      "–∫—Ä—É–ø O\n",
      "##–Ω–µ O\n",
      "##–∏ O\n",
      "##—à–µ O\n",
      "##–∏ O\n",
      "–∑–∞–ø–∞–¥–Ω–æ B-LOCATION\n",
      "##–∏ B-LOCATION\n",
      "—Å–æ—Ü–∏–∞–ª—å–Ω–æ O\n",
      "##–∏ O\n",
      "—Å–µ—Ç—å—é O\n",
      ". O\n",
      "–∑–∞ B-DATE\n",
      "–ø—è—Ç—å I-DATE\n",
      "–ª–µ—Ç I-DATE\n",
      "—Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è O\n",
      "—ç—Ç–æ–≥–æ O\n",
      "–ø—Ä–æ–µ–∫—Ç–∞ O\n",
      "–µ–≥–æ O\n",
      "–∞—É–¥–∏—Ç–æ—Ä–∏—è O\n",
      "–ø—Ä–µ–≤—ã—Å–∏–ª–∞ O\n",
      "200 B-NUMBER\n",
      "–º–∏–ª–ª–∏–æ–Ω–æ–≤ I-NUMBER\n",
      "—á–µ–ª–æ–≤–µ–∫ O\n",
      ". O\n",
      "[SEP] -100\n"
     ]
    }
   ],
   "execution_count": 277
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nejP5Y5DYoUu"
   },
   "source": [
    "Here we set the labels of all special tokens to -100 (the index that is ignored by PyTorch) and the labels of all other tokens to the label of the word they come from. Another strategy is to set the label only on the first token obtained from a given word, and give a label of -100 to the other subtokens from the same word. We propose the two strategies here, just change the value of the following flag:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DSfs0DqCYoUu",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:22.977607Z",
     "start_time": "2024-04-10T09:22:22.974782Z"
    }
   },
   "source": [
    "label_all_tokens = True"
   ],
   "outputs": [],
   "execution_count": 278
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "We're now ready to write the function that will preprocess our samples. We feed them to the `tokenizer` with the argument `truncation=True` (to truncate texts that are bigger than the maximum size allowed by the model) and `is_split_into_words=True` (as seen above). Then we align the labels with the token ids using the strategy we picked:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:24.066217Z",
     "start_time": "2024-04-10T09:22:24.058013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label2id = {\n",
    "    'O': 0,\n",
    "    'B-AGE': 1,\n",
    "    'I-AGE': 2,\n",
    "    'B-AWARD': 3,\n",
    "    'I-AWARD': 4,\n",
    "    'B-CITY': 5,\n",
    "    'I-CITY': 6,\n",
    "    'B-COUNTRY': 7,\n",
    "    'I-COUNTRY': 8,\n",
    "    'B-CRIME': 9,\n",
    "    'I-CRIME': 10,\n",
    "    'B-DATE': 11,\n",
    "    'I-DATE': 12,\n",
    "    'B-DISEASE': 13,\n",
    "    'I-DISEASE': 14,\n",
    "    'B-DISTRICT': 15,\n",
    "    'I-DISTRICT': 16,\n",
    "    'B-EVENT': 17,\n",
    "    'I-EVENT': 18,\n",
    "    'B-FACILITY': 19,\n",
    "    'I-FACILITY': 20,\n",
    "    'B-FAMILY': 21,\n",
    "    'I-FAMILY': 22,\n",
    "    'B-IDEOLOGY': 23,\n",
    "    'I-IDEOLOGY': 24,\n",
    "    'B-LANGUAGE': 25,\n",
    "    'I-LANGUAGE': 26,\n",
    "    'B-LAW': 27,\n",
    "    'I-LAW': 28,\n",
    "    'B-LOCATION': 29,\n",
    "    'I-LOCATION': 30,\n",
    "    'B-MONEY': 31,\n",
    "    'I-MONEY': 32,\n",
    "    'B-NATIONALITY': 33,\n",
    "    'I-NATIONALITY': 34,\n",
    "    'B-NUMBER': 35,\n",
    "    'I-NUMBER': 36,\n",
    "    'B-ORDINAL': 37,\n",
    "    'I-ORDINAL': 38,\n",
    "    'B-ORGANIZATION': 39,\n",
    "    'I-ORGANIZATION': 40,\n",
    "    'B-PENALTY': 41,\n",
    "    'I-PENALTY': 42,\n",
    "    'B-PERCENT': 43,\n",
    "    'I-PERCENT': 44,\n",
    "    'B-PERSON': 45,\n",
    "    'I-PERSON': 46,\n",
    "    'B-PRODUCT': 47,\n",
    "    'I-PRODUCT': 48,\n",
    "    'B-PROFESSION': 49,\n",
    "    'I-PROFESSION': 50,\n",
    "    'B-RELIGION': 51,\n",
    "    'I-RELIGION': 52,\n",
    "    'B-STATE_OR_PROVINCE': 53,\n",
    "    'I-STATE_OR_PROVINCE': 54,\n",
    "    'B-TIME': 55,\n",
    "    'I-TIME': 56,\n",
    "    'B-WORK_OF_ART': 57,\n",
    "    'I-WORK_OF_ART': 58,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 279
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vc0BSBLIIrJQ",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:24.652702Z",
     "start_time": "2024-04-10T09:22:24.646651Z"
    }
   },
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    \n",
    "    transformed = {\"id\": [], \"text\": [], \"tokens\": [], \"ner_tags\": []}\n",
    "    \n",
    "    for idx, text, entities in zip(examples[\"id\"], examples[\"text\"], examples[\"entities\"]):\n",
    "        tokens, ner_tags = transform(text, transform_entities(entities))\n",
    "        transformed[\"id\"].append(idx)\n",
    "        transformed[\"text\"].append(text)\n",
    "        transformed[\"tokens\"].append(tokens)\n",
    "        transformed[\"ner_tags\"].append(ner_tags)\n",
    "    \n",
    "    tokenized_inputs = tokenizer(transformed[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(transformed[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label2id[label[word_idx]] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ],
   "outputs": [],
   "execution_count": 280
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lm8ozrJIrJR"
   },
   "source": [
    "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-b70jh26IrJS",
    "outputId": "acd3a42d-985b-44ee-9daa-af5d944ce1d9",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:25.784008Z",
     "start_time": "2024-04-10T09:22:25.750924Z"
    }
   },
   "source": "tokenize_and_align_labels(datasets[\"train\"][:5])",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 56081, 121, 37009, 107, 3594, 378, 1004, 1022, 29653, 36666, 113, 15568, 10060, 35797, 4308, 113, 1461, 4356, 809, 9381, 197, 8535, 197, 4216, 9381, 197, 8535, 197, 4216, 1355, 162, 3902, 1461, 4356, 385, 121, 167, 2684, 197, 5880, 133, 19361, 390, 19361, 390, 197, 126, 19893, 7528, 73689, 54896, 73689, 54896, 8201, 113, 4308, 121, 15568, 10060, 21578, 67899, 67899, 113, 1461, 4356, 809, 121, 56081, 121, 37009, 107, 3594, 378, 1004, 1022, 29653, 121, 1762, 703, 6697, 2262, 133, 4167, 115, 384, 387, 73689, 54896, 115, 384, 387, 115, 384, 387, 73689, 54896, 73689, 54896, 126, 152, 113, 2972, 5303, 14455, 133, 22294, 2008, 6824, 771, 89786, 6288, 152, 44732, 152, 113, 2257, 6569, 809, 120, 24042, 23009, 121, 113, 4308, 121, 94152, 9485, 133, 28225, 49857, 73689, 54896, 73689, 54896, 107, 15568, 10060, 21578, 67899, 67899, 121, 14204, 107, 36666, 162, 4439, 89209, 41096, 110, 7564, 3985, 52226, 107, 110, 62046, 26369, 26998, 121, 2900, 33131, 19842, 167, 121, 7889, 60827, 121, 1363, 13509, 11766, 377, 8283, 726, 381, 705, 741, 1586, 133, 695, 19842, 121, 2439, 30297, 700, 44489, 747, 34296, 121, 1363, 118216, 1560, 73822, 121, 2439, 3594, 378, 1004, 1022, 29653, 110, 7564, 3985, 52226, 73920, 4995, 121, 4439, 57028, 19842, 159, 34463, 8283, 14913, 666, 121, 2395, 57955, 152, 121, 133, 3165, 113, 4810, 115, 384, 387, 126, 2262, 133, 4167, 5432, 121, 693, 660, 2105, 39930, 152, 113, 4308, 6719, 1456, 5545, 14623, 67899, 14623, 67899, 67899, 121, 62688, 60629, 115268, 121, 107, 1355, 9804, 121, 5990, 39185, 734, 67899, 849, 5303, 111956, 377, 110, 13932, 66146, 23059, 734, 22672, 10760, 4960, 121, 9115, 1698, 113, 3514, 1012, 28345, 152, 126, 4741, 22049, 121, 113, 4308, 904, 16311, 107, 19888, 3414, 17928, 126, 152, 1363, 32815, 121, 74359, 415, 18270, 26531, 19842, 167, 121, 7889, 60827, 121, 15494, 44295, 18270, 26531, 19842, 159, 34463, 121, 1363, 32650, 26531, 19842, 2721, 34463, 121, 113, 8038, 16921, 18270, 121, 34824, 19842, 1586, 34463, 113, 13148, 103236, 18270, 121, 8283, 81898, 160, 119492, 15863, 158, 34824, 160, 16656, 4995, 158, 10365, 18270, 121, 9485, 18270, 38940, 15339, 700, 40208, 4825, 121, 8154, 18270, 38940, 15339, 700, 18260, 14913, 666, 152, 121, 133, 3165, 113, 2262, 133, 17228, 126, 2262, 133, 4167, 115, 384, 387, 115, 384, 387, 5553, 121, 693, 8621, 654, 15070, 7415, 10722, 12534, 12534, 1461, 4356, 657, 1461, 4356, 657, 126, 794, 86425, 381, 33846, 121, 4276, 1922, 7991, 40332, 376, 4937, 121, 3025, 1938, 377, 5880, 126, 4937, 121, 40332, 376, 118, 2382, 67899, 67899, 48956, 15565, 73689, 54896, 73689, 54896, 121, 14275, 113, 14804, 110, 47902, 3476, 73689, 54896, 73689, 54896, 121, 2422, 703, 6697, 4123, 2262, 133, 3629, 14623, 67899, 2262, 133, 3629, 14623, 67899, 14623, 67899, 67899, 126, 152, 780, 11286, 14275, 660, 2209, 73689, 54896, 110, 47902, 107, 10402, 73689, 917, 86774, 378, 73689, 917, 86774, 378, 152, 121, 133, 1239, 11653, 6218, 126, 24699, 107, 4937, 152, 8267, 113, 34840, 654, 37836, 3476, 110, 4655, 7975, 19217, 1874, 63147, 849, 40772, 152, 121, 33611, 29934, 29934, 101151, 14876, 101151, 14876, 126, 152, 3158, 107, 11286, 6719, 771, 1906, 104663, 73689, 917, 73689, 917, 86774, 378, 152, 121, 133, 5432, 2262, 133, 4167, 126, 22149, 67899, 67899, 3542, 38108, 152, 50229, 70301, 152, 107, 31334, 14142, 693, 152, 67899, 107, 73689, 79261, 3029, 707, 12258, 7641, 654, 40003, 19217, 1874, 1501, 988, 378, 73689, 54896, 152, 126, 2262, 133, 4167, 1907, 702, 378, 3838, 378, 9302, 1907, 702, 378, 1907, 702, 378, 3838, 378, 9302, 3108, 110870, 113, 17809, 11158, 152, 81452, 381, 152, 7291, 73689, 54896, 73689, 54896, 13790, 21530, 1260, 667, 378, 14092, 110, 8411, 7332, 4385, 126, 152, 5439, 18924, 121, 43415, 110, 2206, 28166, 121, 8758, 22149, 67899, 67899, 152, 121, 133, 3617, 1938, 377, 5880, 113, 2262, 133, 4713, 9302, 126, 102], [101, 32429, 666, 668, 690, 10902, 10239, 87352, 380, 21093, 2598, 2598, 80666, 54896, 80666, 54896, 32429, 666, 668, 690, 10902, 10239, 113, 3754, 110870, 768, 3636, 24503, 13820, 2467, 378, 4900, 126, 5171, 795, 53614, 25381, 15129, 70507, 4910, 121, 862, 378, 12502, 80666, 1533, 28839, 80666, 1533, 28839, 1748, 2271, 934, 60675, 378, 21178, 21178, 2202, 390, 36250, 27009, 97208, 121, 794, 86425, 381, 1835, 2358, 3866, 11412, 7508, 3495, 2662, 51771, 654, 32721, 378, 8849, 7508, 3495, 2662, 51771, 654, 32721, 378, 8849, 3495, 2662, 51771, 654, 32721, 378, 8849, 2662, 2662, 51771, 51771, 654, 32721, 378, 8849, 126, 60762, 17172, 70507, 4910, 121, 1981, 26563, 660, 2849, 113, 84626, 84626, 118, 4676, 118, 4676, 121, 1281, 672, 5488, 126, 785, 2422, 152, 192, 152, 2969, 2262, 133, 3629, 94070, 2662, 2262, 133, 3629, 94070, 2662, 94070, 94070, 2662, 2662, 110183, 8083, 32429, 668, 5426, 1005, 121, 5171, 32429, 666, 668, 690, 10902, 10239, 8447, 14958, 721, 17612, 4050, 2467, 378, 4900, 126, 6397, 133, 1049, 939, 25381, 5885, 70507, 883, 121, 2944, 1310, 946, 378, 113, 6109, 36286, 654, 9457, 16169, 7832, 121, 106, 2478, 1578, 73272, 23059, 734, 690, 378, 833, 748, 4900, 118, 4676, 118, 4676, 121, 8370, 1373, 3866, 113, 7660, 4949, 878, 126, 2875, 33963, 98493, 4776, 12020, 15965, 121, 7856, 946, 378, 5517, 21178, 27305, 4050, 4900, 2202, 390, 36250, 27009, 97208, 126, 1748, 2271, 113, 10118, 5115, 1480, 94070, 94070, 20620, 9620, 18687, 152, 36524, 6556, 6556, 152, 121, 7526, 3480, 17172, 36250, 27009, 97208, 113, 3170, 160, 4888, 8146, 36250, 27009, 10239, 905, 45503, 660, 2849, 113, 116721, 158, 126, 4430, 834, 19278, 378, 1073, 3170, 844, 14274, 660, 4681, 17172, 690, 10902, 97208, 152, 19595, 113847, 394, 1251, 19595, 113847, 394, 1251, 152, 121, 113, 2567, 795, 36451, 3462, 20900, 378, 4282, 107, 16592, 152, 27186, 102634, 102634, 152, 681, 52294, 863, 114, 7369, 386, 8741, 126, 785, 15968, 152, 192, 152, 180, 2840, 121, 5464, 9278, 14871, 4925, 3170, 29062, 7174, 1078, 377, 1005, 672, 32438, 1180, 10419, 32429, 666, 868, 690, 10902, 97208, 3913, 92855, 377, 126, 660, 1373, 768, 6304, 27801, 113, 80666, 1022, 1196, 672, 2575, 126, 2969, 28937, 2969, 28937, 152, 7114, 378, 44422, 152, 7114, 378, 44422, 44422, 152, 29062, 7174, 43467, 6035, 1689, 152, 192, 152, 121, 693, 118, 3170, 1177, 4654, 73272, 4988, 654, 4448, 21178, 4050, 80666, 702, 378, 4900, 80666, 702, 378, 80666, 702, 378, 4900, 126, 152, 736, 27603, 1715, 9247, 126, 795, 2526, 10654, 1818, 89467, 1841, 31394, 152, 121, 133, 1239, 795, 126, 6534, 73272, 3754, 110870, 21178, 5500, 152, 192, 152, 113, 80666, 702, 378, 80666, 702, 378, 4900, 5742, 7413, 118, 20620, 9362, 99540, 748, 5106, 917, 33799, 107, 24259, 113, 25670, 1591, 670, 126, 152, 934, 14837, 2202, 390, 36250, 27009, 97208, 14408, 6232, 102644, 121, 1003, 7023, 5148, 32243, 654, 5968, 110, 44368, 3476, 152, 121, 133, 3308, 11653, 152, 192, 152, 126, 654, 806, 1923, 121, 9495, 10866, 3168, 121, 693, 15987, 3816, 113, 80666, 702, 378, 8849, 1020, 4746, 4483, 703, 5900, 110, 690, 1181, 686, 29616, 118479, 121, 2224, 113, 4578, 7893, 121, 693, 2969, 2656, 27262, 152, 9964, 106886, 747, 12297, 121, 862, 378, 113, 4270, 3302, 7183, 700, 104521, 22632, 4050, 4900, 152, 126, 7979, 27300, 5089, 121, 693, 2541, 11476, 1011, 1716, 15478, 121, 785, 8146, 70507, 883, 121, 1293, 17506, 11246, 152, 15591, 2450, 152, 121, 934, 1093, 785, 14381, 7336, 3269, 17172, 690, 10902, 97208, 110, 31444, 10353, 126, 113, 49087, 702, 378, 49087, 702, 378, 18526, 152, 192, 152, 3617, 121, 693, 5171, 834, 8146, 70507, 883, 2702, 6336, 17817, 660, 3866, 32506, 15703, 32506, 15703, 4900, 118, 4676, 4900, 118, 4676, 118, 4676, 126, 3048, 104, 25887, 3445, 4050, 2467, 378, 4900, 32429, 666, 668, 690, 10902, 10239, 121, 654, 1923, 17172, 32429, 668, 5426, 2568, 121, 1281, 672, 8447, 126, 654, 66672, 2206, 152, 192, 152, 121, 113, 2847, 6118, 59052, 660, 1373, 3866, 20621, 22346, 22346, 104, 377, 375, 152, 80666, 14667, 152, 80666, 14667, 152, 116238, 378, 21817, 3463, 121, 7943, 755, 6520, 390, 113, 14804, 152, 44781, 17227, 57964, 6536, 152, 107, 5296, 17557, 33820, 5329, 110870, 17832, 73272, 126, 785, 3542, 6811, 81666, 4925, 1857, 110870, 10330, 3086, 75278, 110358, 823, 121, 1688, 15226, 13779, 152, 68036, 76769, 651, 152, 121, 107, 2969, 2656, 1079, 12734, 17172, 21817, 10029, 785, 1942, 16885, 50468, 55249, 660, 6594, 2662, 4960, 126, 152, 849, 13448, 752, 1688, 15226, 1079, 9667, 1184, 114845, 378, 121, 1009, 29616, 19559, 121, 12902, 1788, 707, 734, 806, 14495, 378, 26106, 107, 8129, 11414, 7535, 3495, 152, 121, 133, 1689, 8146, 110358, 823, 126, 49087, 727, 378, 22576, 2652, 12355, 28249, 2532, 10239, 121, 7880, 121, 3542, 121, 693, 3975, 3071, 672, 104, 60762, 376, 121, 106, 104, 152, 16582, 5398, 917, 44263, 152, 126, 654, 806, 1923, 121, 2598, 121, 15477, 1981, 110586, 779, 1458, 1748, 121, 17355, 4881, 2470, 5512, 11780, 7892, 375, 376, 1613, 113, 77601, 15319, 378, 39695, 133, 818, 834, 121, 785, 11569, 2944, 2048, 133, 5900, 115, 384, 387, 654, 80666, 54896, 115, 384, 387, 654, 80666, 54896, 80666, 54896, 690, 1799, 1181, 389, 96309, 17742, 121, 1981, 934, 21178, 734, 10399, 9146, 113, 14804, 26867, 5401, 133, 33402, 126, 102], [101, 6556, 105657, 19986, 53070, 1806, 113, 38158, 376, 21688, 10482, 13644, 1970, 133, 50417, 5260, 160, 139, 78967, 393, 158, 121, 2514, 2077, 51320, 53070, 736, 378, 3034, 126, 654, 2206, 152, 192, 152, 121, 1205, 2081, 2037, 21961, 133, 113, 6473, 13448, 661, 14432, 660, 19615, 8213, 133, 8074, 3034, 113, 38158, 376, 113, 9024, 660, 1818, 51607, 1942, 53070, 126, 2224, 681, 22319, 11239, 6556, 818, 107, 672, 103740, 110, 73703, 73272, 121, 693, 1079, 41543, 1003, 4988, 113, 3034, 126, 60530, 53070, 53070, 139, 78967, 393, 139, 78967, 393, 2077, 2037, 13108, 13967, 660, 3665, 39472, 736, 378, 3034, 113, 152, 89356, 10912, 385, 10912, 385, 152, 126, 5206, 755, 378, 5550, 113, 1019, 5952, 905, 15336, 660, 21479, 1111, 7193, 113, 110919, 121, 8619, 1981, 15949, 55871, 4001, 7906, 107, 6255, 14306, 3034, 126, 1882, 834, 904, 5429, 2335, 104, 8213, 133, 7822, 139, 78967, 393, 139, 78967, 393, 126, 6556, 6145, 24841, 5026, 64100, 133, 9663, 121, 750, 113, 2972, 12410, 76933, 10471, 5023, 10670, 665, 38158, 389, 126, 9271, 121, 785, 3308, 152, 192, 152, 1354, 734, 5414, 696, 378, 4417, 121, 6556, 28038, 25589, 100036, 2656, 711, 3814, 16437, 162, 44991, 6536, 44991, 6536, 139, 78967, 393, 139, 78967, 393, 2081, 2037, 12830, 51771, 51771, 126, 2686, 60530, 27133, 44991, 6536, 139, 78967, 393, 44991, 6536, 139, 78967, 393, 113, 2972, 18847, 10293, 904, 1861, 94504, 133, 110919, 9220, 1707, 2775, 680, 44113, 1847, 126, 2224, 736, 5032, 22014, 126, 2598, 2598, 51771, 51771, 116, 4851, 1030, 10133, 4171, 15958, 14385, 39601, 110, 73703, 73272, 114, 1470, 2574, 126, 4741, 62102, 13644, 121, 44991, 2081, 1202, 672, 34041, 4258, 1049, 121, 4821, 1470, 133, 1049, 939, 6166, 2218, 113, 33103, 73272, 10998, 107, 35034, 24272, 54896, 1313, 126, 785, 3308, 152, 192, 152, 5500, 113, 12007, 12007, 51771, 51771, 121, 1025, 113, 7125, 904, 14355, 51607, 660, 2241, 6047, 2048, 133, 4477, 13390, 42594, 390, 132, 829, 7128, 377, 121, 862, 378, 794, 86425, 381, 1733, 63520, 2403, 2662, 51771, 654, 37578, 80043, 389, 24609, 2662, 51771, 51771, 654, 37578, 80043, 389, 24609, 107, 6347, 113, 1991, 11525, 152, 2588, 44019, 152, 2588, 44019, 152, 126, 2224, 8146, 132, 829, 7128, 6444, 6613, 126, 654, 1923, 26740, 110, 1823, 19654, 121, 152, 6634, 3034, 849, 1263, 672, 2330, 52782, 152, 126, 5500, 152, 192, 152, 113, 12007, 6529, 17110, 2812, 654, 133, 7865, 162, 152, 780, 12855, 121, 693, 35442, 967, 794, 2314, 21563, 132, 829, 7128, 389, 6309, 121, 700, 1981, 672, 28001, 126, 750, 9486, 672, 25248, 152, 126, 21981, 110, 10260, 42594, 686, 132, 829, 7128, 49761, 152, 192, 152, 5171, 672, 2845, 126, 934, 1164, 121, 654, 1923, 20026, 152, 192, 152, 121, 16936, 1025, 2493, 34995, 162, 6463, 9324, 62110, 80069, 378, 3875, 152, 2588, 44019, 152, 9324, 62110, 80069, 378, 3875, 152, 2588, 44019, 152, 2588, 44019, 152, 2414, 6520, 11527, 26495, 832, 107, 2048, 133, 2969, 83963, 152, 2588, 61557, 152, 83963, 152, 2588, 61557, 152, 4520, 378, 60294, 121, 6848, 928, 378, 10400, 133, 19137, 36014, 24626, 33048, 15263, 26767, 20322, 13512, 23015, 486, 11580, 100752, 2530, 6003, 12947, 30269, 126, 152, 119, 2289, 121, 693, 904, 6117, 794, 1555, 660, 3653, 66536, 6891, 104, 1109, 121, 1015, 22340, 112192, 113, 38158, 389, 60294, 377, 967, 26495, 2007, 152, 121, 133, 8496, 152, 192, 152, 12891, 36199, 6236, 121, 20894, 378, 110, 52238, 55216, 126, 750, 113, 4578, 4984, 11050, 126, 16348, 26495, 832, 107, 60294, 672, 53121, 5143, 126, 113, 2411, 1355, 2840, 5401, 133, 3848, 51771, 5401, 133, 3848, 51771, 42594, 394, 794, 2314, 11369, 8264, 121, 693, 6556, 1281, 818, 107, 672, 103740, 110, 73703, 73272, 660, 3866, 4050, 139, 78967, 393, 139, 78967, 393, 126, 14615, 11990, 121, 654, 2206, 152, 192, 152, 121, 1073, 67211, 113, 2171, 2396, 384, 756, 378, 5098, 126, 785, 3308, 152, 192, 152, 5500, 113, 12007, 121, 152, 6065, 7508, 2235, 734, 71204, 805, 675, 4921, 15851, 18255, 113, 38158, 389, 121, 2224, 42594, 394, 794, 2314, 1239, 121, 693, 672, 9810, 121, 4349, 6797, 1373, 1266, 2526, 114, 47168, 121, 818, 693, 107, 1688, 53171, 89790, 152, 126, 113, 4578, 44422, 1075, 26391, 121, 2633, 40119, 3398, 13390, 3398, 13390, 51771, 51771, 1379, 1062, 378, 131, 5885, 650, 121, 7042, 121, 5335, 121, 25728, 113, 38158, 389, 1151, 34995, 660, 3866, 53070, 53070, 139, 78967, 393, 139, 78967, 393, 126, 9008, 654, 4648, 121, 13448, 661, 16181, 13892, 7349, 5200, 3034, 8651, 2136, 104, 44991, 376, 660, 8394, 7193, 126, 736, 8496, 152, 192, 152, 1354, 734, 5414, 4417, 113, 38158, 376, 126, 152, 118, 1226, 1114, 3003, 11272, 45645, 4004, 5296, 16252, 8213, 133, 8074, 121, 42164, 107, 2455, 152, 121, 133, 1239, 795, 126, 3982, 20900, 378, 13021, 26657, 6167, 53070, 1114, 126, 785, 19924, 152, 192, 152, 113, 8970, 14667, 121, 849, 806, 31395, 2404, 121, 1015, 121, 654, 1138, 91841, 378, 3675, 121, 2762, 5414, 139, 78967, 393, 41517, 3130, 7906, 104, 8698, 3034, 121, 107, 110, 5123, 63195, 5155, 7349, 5414, 721, 1019, 2846, 1913, 13669, 378, 126, 660, 3246, 2840, 78739, 1146, 6825, 654, 139, 78967, 393, 13087, 4439, 734, 1741, 1970, 133, 5200, 162, 67722, 382, 121, 15022, 121, 63691, 121, 19939, 79261, 121, 1463, 61811, 387, 107, 696, 831, 653, 121, 106, 1080, 6556, 126, 750, 11110, 64071, 673, 14808, 104, 56687, 10487, 1707, 51771, 121, 19939, 79261, 107, 1463, 61811, 387, 126, 43632, 834, 113, 13201, 7829, 8387, 121, 18922, 13699, 662, 121, 11449, 121, 52094, 1421, 107, 94835, 5656, 844, 59793, 91841, 390, 110, 78739, 83027, 378, 119325, 126, 1201, 672, 2482, 8619, 5394, 691, 378, 6711, 113, 38158, 376, 121, 5335, 121, 39184, 660, 60530, 53070, 126, 818, 121, 11449, 5171, 4549, 7198, 3653, 1942, 13085, 133, 1205, 1579, 5401, 133, 2598, 5401, 133, 2598, 11449, 702, 378, 23231, 378, 33103, 73272, 2185, 11449, 702, 378, 23231, 378, 33103, 73272, 2185, 11449, 702, 378, 23231, 378, 33103, 73272, 2185, 8171, 6016, 19736, 662, 18952, 12758, 24366, 387, 126, 711, 1019, 4123, 745, 37524, 377, 113, 10912, 385, 745, 37524, 377, 113, 10912, 385, 36951, 13714, 97823, 133, 23529, 1798, 680, 783, 5171, 1689, 121, 693, 5979, 53070, 53070, 3034, 984, 2194, 113, 7339, 1947, 126, 152, 1081, 8619, 4417, 672, 99853, 654, 7114, 378, 108177, 121, 696, 44991, 660, 12622, 377, 378, 1838, 1363, 878, 1020, 5488, 113, 57341, 870, 4803, 152, 121, 133, 1689, 795, 126, 113, 1019, 1947, 121, 44991, 6536, 44991, 6536, 139, 78967, 393, 139, 78967, 393, 1020, 12117, 700, 67722, 649, 126, 1081, 2241, 10025, 4184, 59588, 107, 2455, 8619, 5394, 691, 378, 4417, 121, 118, 44422, 8122, 1363, 3665, 126, 960, 2985, 1020, 2188, 9136, 700, 3759, 53070, 53070, 139, 78967, 393, 139, 78967, 393, 107, 12688, 4299, 22541, 4351, 849, 1231, 22221, 121, 2188, 51607, 113, 1388, 939, 2105, 3197, 5744, 133, 696, 13085, 126, 750, 13122, 32587, 667, 378, 113, 60945, 12117, 1079, 9667, 103887, 16312, 152, 89356, 10912, 385, 10912, 385, 152, 113, 47387, 849, 13448, 752, 7086, 126, 102], [101, 44422, 1022, 44958, 11050, 88395, 152, 89356, 10912, 385, 10912, 385, 152, 4483, 4483, 44422, 44422, 672, 50805, 2775, 680, 22221, 12818, 12689, 12818, 12689, 13644, 1970, 133, 50417, 5260, 13644, 1970, 133, 50417, 5260, 160, 139, 78967, 393, 158, 121, 862, 378, 113, 17415, 2828, 7232, 152, 33103, 73272, 10912, 385, 10912, 385, 152, 126, 721, 1019, 3660, 5935, 152, 6701, 7210, 152, 126, 654, 1003, 2206, 121, 780, 52215, 95224, 700, 44422, 11050, 700, 46134, 73272, 1205, 6709, 121, 2686, 6556, 32350, 660, 29092, 139, 78967, 393, 113, 38158, 376, 1151, 1942, 13085, 126, 152, 6701, 7210, 152, 5432, 121, 693, 28921, 28893, 3034, 2081, 905, 2037, 21961, 162, 721, 1019, 121, 654, 17771, 10065, 7610, 121, 1916, 12233, 113, 6473, 4216, 878, 113, 110919, 121, 1040, 6556, 14432, 660, 19615, 8213, 133, 8074, 139, 78967, 393, 113, 38158, 376, 126, 44422, 1362, 44422, 1362, 4483, 4483, 1882, 19866, 375, 14545, 8213, 133, 8724, 113, 64100, 133, 9663, 376, 121, 2224, 113, 2972, 10471, 22828, 1922, 38158, 377, 126, 9495, 113, 44422, 25013, 51607, 113, 29531, 660, 3866, 53070, 53070, 152, 33103, 73272, 10912, 385, 152, 33103, 73272, 10912, 385, 10912, 385, 152, 6665, 4477, 13390, 42594, 390, 132, 829, 7128, 377, 126, 794, 86425, 381, 795, 4209, 63520, 2403, 2662, 654, 37578, 80043, 389, 24609, 2662, 654, 37578, 80043, 389, 24609, 107, 3155, 734, 11525, 152, 2588, 44019, 152, 11525, 152, 2588, 44019, 152, 126, 654, 2206, 152, 6701, 25498, 152, 121, 132, 829, 7128, 6444, 6613, 700, 46134, 73272, 1211, 6709, 126, 2478, 12665, 18840, 1025, 2493, 34995, 162, 7508, 9324, 62110, 80069, 378, 3875, 152, 2588, 44019, 152, 7508, 9324, 62110, 80069, 378, 3875, 152, 2588, 44019, 152, 2414, 6520, 48526, 26495, 2007, 107, 6665, 4050, 152, 2588, 91225, 152, 2588, 91225, 152, 4520, 390, 60294, 377, 126, 4984, 991, 1080, 11050, 27796, 139, 78967, 393, 126, 4741, 62102, 139, 78967, 393, 121, 4891, 394, 667, 378, 5906, 3034, 2081, 35034, 24272, 54896, 1313, 11785, 121, 4821, 1470, 133, 1049, 939, 6166, 2218, 113, 10998, 107, 1202, 672, 34041, 4258, 1049, 126, 654, 2206, 152, 6701, 25498, 152, 121, 660, 17856, 113, 38158, 376, 6556, 33383, 19347, 5979, 12818, 12689, 660, 21248, 9103, 126, 2224, 2455, 1909, 965, 49434, 1841, 10074, 126, 818, 121, 113, 11449, 376, 5089, 121, 693, 44991, 6536, 1079, 2037, 5401, 133, 2598, 11449, 702, 378, 23231, 378, 33103, 73272, 2185, 5401, 133, 2598, 11449, 702, 378, 23231, 378, 33103, 73272, 2185, 11449, 702, 378, 23231, 378, 33103, 73272, 2185, 8171, 6016, 19736, 662, 18952, 12758, 24366, 387, 126, 113, 3359, 139, 78967, 393, 8619, 3034, 85780, 1662, 83027, 378, 721, 52837, 13367, 107, 17137, 5260, 126, 7023, 736, 378, 3034, 660, 6750, 654, 5968, 110, 10912, 385, 160, 2542, 83027, 378, 1970, 133, 50417, 6130, 158, 113768, 162, 139, 78967, 393, 3732, 672, 1079, 27066, 660, 5445, 660, 21646, 5562, 126, 1201, 672, 2482, 121, 113, 3498, 2771, 1688, 7700, 6595, 98818, 126, 113, 6473, 4216, 878, 3200, 139, 78967, 393, 660, 5131, 1111, 4302, 9680, 113, 64100, 133, 9663, 376, 126, 6556, 1733, 2143, 691, 378, 1979, 85940, 912, 5260, 113, 2814, 121, 2686, 960, 1079, 15289, 660, 2125, 378, 7399, 113, 3034, 126, 3283, 44422, 121, 113, 139, 78967, 393, 9750, 1080, 67722, 382, 121, 7829, 8387, 121, 26309, 691, 378, 121, 18922, 13699, 662, 121, 15022, 121, 42696, 4804, 121, 11449, 121, 63691, 121, 19939, 79261, 121, 26533, 1955, 79261, 121, 52094, 1421, 121, 104, 377, 407, 121, 16959, 888, 121, 1463, 61811, 387, 107, 696, 831, 653, 107, 94835, 5656, 844, 59793, 91841, 390, 126, 102], [101, 78307, 6502, 3445, 15789, 6795, 32399, 13048, 11255, 378, 5273, 78307, 5488, 6655, 133, 1049, 939, 58774, 2732, 136, 1542, 19112, 160, 91101, 2769, 156, 7334, 70869, 1321, 158, 121, 1762, 3251, 217, 6116, 119811, 15706, 443, 263, 16598, 126, 660, 2849, 113, 78307, 795, 3640, 865, 113, 6517, 126, 2944, 136, 1542, 19112, 905, 32399, 13048, 102637, 80069, 378, 2185, 66512, 99338, 3575, 126, 136, 1542, 19112, 4675, 121, 693, 8176, 1349, 6639, 1289, 78307, 107, 66512, 99338, 3575, 126, 113, 3325, 121, 736, 2493, 113020, 2430, 2185, 110, 3584, 378, 6662, 4041, 378, 96480, 378, 126, 1080, 795, 1689, 121, 693, 78307, 15837, 4156, 133, 17089, 845, 13139, 33445, 113, 4206, 979, 126, 113, 2185, 66512, 99338, 3575, 58774, 2732, 136, 1542, 19112, 35738, 1470, 1049, 126, 1003, 32399, 13048, 795, 1579, 113, 5093, 979, 126, 660, 736, 378, 6709, 136, 1542, 19112, 35738, 708, 2866, 4206, 878, 121, 1040, 27847, 11266, 195, 13232, 2804, 12206, 66512, 99338, 3575, 126, 654, 2206, 3251, 217, 6116, 119811, 15706, 443, 263, 16598, 121, 660, 6047, 15789, 6795, 15789, 6795, 78307, 78307, 75336, 1741, 10074, 126, 2563, 378, 734, 1303, 6568, 72802, 110, 42657, 78307, 52071, 6536, 30313, 5007, 45520, 160, 83900, 111964, 28658, 1119, 14636, 158, 107, 3993, 36365, 2185, 126, 2944, 32399, 13048, 32399, 13048, 78307, 78307, 905, 46614, 376, 833, 132, 160, 203, 50707, 446, 207, 459, 158, 126, 795, 10114, 10400, 1463, 4907, 2271, 126, 3961, 1263, 29598, 9282, 7042, 45610, 9803, 133, 31630, 110, 14330, 2218, 113, 17664, 378, 2185, 126, 78307, 1733, 2143, 691, 378, 756, 378, 40395, 378, 11255, 378, 35427, 126, 681, 2762, 1049, 10470, 1164, 4522, 806, 34796, 40730, 1241, 3815, 1266, 126, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 33, 0, 0, 5, 5, 5, 11, 11, 11, 11, 11, 55, 55, 55, 55, 55, 56, 56, 56, 5, 5, 5, 0, 11, 12, 0, 39, 39, 39, 39, 7, 7, 0, 0, 39, 40, 40, 40, 7, 7, 0, 0, 0, 0, 0, 0, 33, 34, 7, 0, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 12, 39, 39, 39, 40, 40, 40, 40, 40, 39, 39, 39, 39, 39, 39, 40, 40, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 5, 5, 5, 0, 0, 0, 0, 0, 1, 1, 1, 33, 34, 34, 7, 7, 0, 0, 0, 33, 34, 7, 0, 17, 18, 18, 0, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 0, 0, 35, 35, 35, 36, 0, 35, 0, 0, 0, 0, 47, 47, 47, 47, 35, 35, 35, 36, 0, 35, 0, 0, 0, 0, 0, 0, 35, 0, 0, 0, 0, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 0, 0, 35, 36, 0, 47, 47, 0, 35, 0, 0, 0, 0, 0, 0, 0, 39, 39, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 50, 50, 39, 40, 7, 0, 0, 0, 0, 0, 0, 35, 49, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 49, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 0, 0, 35, 35, 0, 0, 0, 35, 35, 35, 36, 0, 35, 35, 0, 0, 0, 35, 36, 0, 35, 0, 0, 0, 35, 36, 0, 0, 0, 35, 0, 0, 0, 0, 35, 36, 0, 0, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 0, 0, 35, 0, 0, 0, 0, 0, 0, 0, 35, 0, 0, 0, 0, 0, 47, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 39, 39, 40, 40, 40, 39, 39, 39, 0, 0, 0, 0, 0, 0, 0, 0, 39, 39, 40, 40, 40, 5, 5, 5, 0, 55, 55, 55, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 39, 40, 0, 0, 0, 0, 0, 0, 33, 34, 7, 39, 40, 40, 40, 7, 7, 0, 0, 0, 0, 0, 0, 39, 40, 40, 7, 7, 0, 0, 11, 12, 49, 50, 50, 50, 50, 50, 39, 39, 39, 40, 40, 39, 40, 7, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0, 0, 0, 0, 39, 39, 40, 40, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 0, 0, 0, 0, 0, 0, 0, 0, 39, 0, 0, 0, 0, 0, 0, 0, 39, 0, 0, 0, 7, 39, 40, 40, 39, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 39, 39, 40, 40, 0, 0, 0, 0, 39, 39, 39, 0, 39, 40, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0, 0, 39, 39, 39, 40, 40, 40, 40, 40, 40, 7, 7, 7, 39, 39, 39, 40, 40, 40, 0, 0, 0, 0, 0, 0, 19, 19, 0, 5, 6, 6, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 7, 49, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 40, 7, 0, 0, 0, 0, 39, 39, 40, 0, 0, 0, 0, 0, 0, -100], [-100, 45, 45, 45, 46, 46, 46, 0, 0, 0, 49, 49, 50, 50, 53, 53, 45, 45, 45, 46, 46, 46, 0, 0, 0, 0, 0, 0, 49, 50, 50, 50, 0, 11, 0, 17, 45, 45, 46, 46, 0, 0, 0, 0, 39, 39, 40, 53, 53, 0, 11, 12, 0, 17, 17, 18, 17, 45, 45, 46, 46, 46, 0, 0, 0, 0, 0, 0, 0, 49, 50, 50, 50, 50, 50, 50, 50, 50, 49, 50, 50, 50, 50, 50, 50, 50, 39, 40, 40, 40, 40, 40, 40, 49, 49, 50, 7, 0, 0, 0, 0, 0, 0, 0, 45, 45, 0, 0, 0, 0, 0, 0, 39, 39, 40, 40, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 39, 0, 49, 50, 50, 50, 50, 50, 39, 39, 39, 40, 40, 53, 49, 50, 49, 45, 45, 46, 46, 46, 46, 0, 11, 45, 45, 45, 46, 46, 46, 17, 18, 0, 17, 49, 0, 0, 0, 0, 1, 1, 1, 1, 45, 45, 46, 46, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 49, 50, 50, 50, 50, 50, 50, 50, 50, 50, 5, 5, 0, 17, 18, 18, 11, 12, 12, 12, 0, 0, 0, 0, 17, 17, 18, 0, 0, 0, 0, 0, 17, 0, 49, 50, 45, 45, 46, 46, 46, 0, 11, 12, 0, 0, 0, 0, 53, 49, 50, 0, 0, 0, 39, 40, 7, 0, 0, 0, 0, 0, 45, 45, 45, 0, 0, 0, 0, 0, 45, 45, 45, 0, 0, 0, 0, 0, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 45, 45, 45, 0, 39, 40, 40, 40, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 39, 39, 40, 0, 0, 0, 39, 40, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 0, 11, 12, 0, 0, 39, 40, 40, 40, 45, 45, 46, 46, 46, 0, 0, 0, 0, 45, 45, 45, 46, 46, 46, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 53, 53, 0, 0, 0, 0, 49, 50, 49, 50, 50, 50, 50, 50, 50, 39, 39, 40, 7, 0, 45, 45, 46, 46, 0, 0, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 49, 50, 50, 50, 50, 53, 53, 53, 39, 39, 39, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 0, 0, 53, 53, 53, 39, 39, 39, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 45, 45, 46, 46, 46, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 53, 53, 53, 0, 0, 0, 39, 0, 0, 0, 45, 45, 45, 46, 46, 0, 0, 0, 0, 0, 0, 0, 49, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 45, 45, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 45, 45, 45, 0, 39, 40, 0, 0, 5, 5, 5, 39, 39, 39, 40, 0, 39, 0, 0, 0, 0, 11, 0, 0, 45, 45, 0, 0, 17, 0, 0, 49, 50, 49, 50, 50, 50, 50, 39, 40, 40, 5, 5, 0, 0, 0, 0, 0, 49, 0, 0, 0, 45, 45, 45, 46, 46, 46, 0, 0, 0, 0, 45, 45, 45, 45, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 49, 50, 50, 50, 50, 50, 50, 50, 39, 39, 0, 45, 45, 46, 46, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 50, 50, 0, 0, 23, 45, 45, 46, 46, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 50, 0, 0, 0, 45, 45, 0, 0, 0, 0, 0, 0, 0, 49, 50, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 45, 45, 0, 0, 0, 0, 0, 0, 33, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 45, 45, 0, 5, 5, 5, 49, 45, 45, 46, 46, 46, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 0, 0, 0, 0, 0, 11, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 50, 50, 50, 50, 50, 50, 39, 39, 39, 40, 40, 40, 53, 53, 45, 45, 45, 45, 46, 46, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 49, 49, 0, -100], [-100, 7, 0, 0, 49, 11, 0, 5, 5, 0, 17, 39, 40, 40, 40, 40, 0, 39, 39, 39, 0, 0, 0, 0, 0, 49, 0, 0, 0, 0, 0, 0, 0, 39, 0, 0, 0, 0, 0, 33, 0, 11, 12, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 49, 0, 0, 11, 12, 12, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 49, 50, 50, 50, 39, 39, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 40, 40, 39, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 17, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 19, 19, 20, 20, 20, 39, 39, 39, 0, 7, 0, 0, 0, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 39, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 49, 49, 49, 49, 50, 50, 50, 39, 39, 39, 0, 0, 33, 34, 7, 0, 0, 0, 33, 49, 49, 50, 50, 50, 49, 49, 39, 39, 39, 0, 0, 11, 17, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 49, 50, 7, 45, 45, 45, 46, 46, 0, 39, 0, 0, 0, 0, 11, 12, 12, 0, 0, 0, 39, 0, 49, 0, 0, 1, 2, 2, 2, 0, 0, 11, 11, 11, 11, 0, 0, 0, 0, 0, 0, 0, 0, 25, 25, 25, 0, 0, 0, 0, 39, 0, 0, 0, 39, 39, 40, 7, 0, 0, 11, 12, 0, 0, 0, 0, 0, 0, 0, 0, 49, 50, 45, 45, 46, 46, 46, 46, 0, 0, 0, 0, 0, 0, 0, 49, 49, 50, 50, 50, 50, 50, 50, 50, 49, 50, 7, 0, 0, 0, 0, 0, 0, 0, 0, 39, 40, 40, 40, 40, 40, 39, 39, 0, 0, 0, 0, 45, 45, 45, 0, 0, 0, 0, 0, 0, 0, 0, 49, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 0, 0, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 45, 0, 45, 45, 0, 45, 45, 45, 45, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 45, 45, 46, 46, 46, 46, 0, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 0, 0, 0, 0, 35, 0, 0, 49, 50, 50, 50, 50, 50, 50, 50, 50, 50, 39, 40, 40, 40, 40, 40, 40, 40, 40, 39, 39, 0, 45, 45, 45, 46, 46, 0, 0, 0, 49, 50, 50, 50, 50, 50, 39, 40, 40, 40, 40, 45, 45, 46, 0, 0, 0, 0, 0, 0, 0, 0, 19, 19, 19, 19, 20, 39, 39, 39, 39, 40, 40, 40, 40, 40, 0, 0, 0, 0, 0, 0, 0, 0, 45, 45, 0, 0, 45, 45, 0, 0, 0, 0, 0, 0, 0, 5, 5, 45, 45, 0, 45, 45, 0, 0, 0, 0, 0, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 45, 45, 0, 45, 0, 0, 0, 0, 0, 0, 11, 12, 49, 49, 49, 50, 49, 49, 49, 7, 45, 45, 46, 46, 0, 49, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 50, 50, 50, 39, 39, 39, 0, 0, 0, 0, 0, 0, 0, 39, 0, 0, 0, 0, 11, 12, 12, 12, 12, 12, 12, 0, 0, 0, 0, 39, 0, 0, 0, 39, 0, 0, 49, 50, 0, 0, 0, 39, 39, 0, 0, 0, 0, 5, 5, 0, 0, 45, 45, 46, 46, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0, 0, 0, 0, 49, 50, 49, 50, 50, 7, 45, 45, 45, 46, 46, 46, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 49, 49, 50, 50, 50, 39, 39, 39, 0, 0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 49, 49, 0, 0, 0, 0, 0, 0, 0, 39, 0, 0, 0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 0, 0, 0, 0, 0, 39, 0, 0, 39, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 0, 39, 39, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 12, 12, 0, 11, 12, 12, 0, 0, 0, 0, 39, 39, 39, 0, 35, 0, 35, 0, 0, 0, 0, 7, 7, 0, 7, 0, 7, 0, 7, 7, 0, 7, 7, 7, 8, 8, 8, 8, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 7, 7, 0, 7, 7, 7, 0, 0, 0, 0, 0, 7, 7, 0, 7, 7, 7, 0, 7, 0, 7, 7, 0, 7, 7, 7, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 11, 17, 0, 5, 5, 0, 0, 0, 0, 0, 0, 49, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50, 50, 50, 50, 39, 39, 39, 40, 40, 40, 40, 40, 7, 7, 7, 0, 0, 0, 0, 0, 45, 45, 45, 45, 46, 46, 46, 46, 0, 0, 0, 49, 50, 50, 50, 50, 50, 50, 5, 5, 5, 0, 39, 39, 45, 45, 45, 45, 45, 46, 46, 46, 11, 0, 0, 0, 17, 18, 49, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 11, 12, 12, 12, 12, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 49, 49, 49, 50, 50, 50, 39, 39, 39, 0, 0, 0, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 49, 50, 50, 50, 39, 39, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 40, 40, 39, 39, 0, 0, 0, 0, 5, 5, 0, 0, -100], [-100, 7, 7, 49, 0, 0, 0, 39, 40, 40, 39, 39, 0, 39, 39, 40, 7, 0, 0, 0, 0, 0, 49, 50, 49, 50, 50, 50, 50, 50, 50, 39, 40, 40, 40, 40, 0, 39, 39, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 39, 40, 40, 39, 39, 0, 0, 0, 0, 0, 0, 0, 47, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 17, 39, 39, 39, 0, 5, 5, 0, 0, 0, 0, 0, 39, 39, 0, 0, 0, 0, 49, 50, 0, 0, 0, 0, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 12, 12, 12, 0, 5, 0, 0, 7, 0, 0, 0, 0, 0, 0, 39, 39, 39, 0, 5, 5, 0, 7, 7, 39, 39, 40, 39, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 49, 49, 50, 50, 50, 50, 50, 50, 39, 39, 40, 40, 39, 39, 0, 0, 49, 50, 45, 45, 46, 46, 46, 46, 0, 0, 0, 0, 0, 0, 49, 49, 50, 50, 50, 50, 50, 50, 49, 0, 0, 0, 0, 0, 0, 0, 0, 49, 50, 50, 50, 50, 49, 0, 39, 39, 0, 0, 0, 0, 0, 39, 39, 0, 0, 45, 45, 45, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 0, 0, 35, 0, 0, 49, 50, 50, 50, 50, 50, 50, 50, 50, 50, 49, 50, 0, 0, 0, 0, 0, 39, 39, 0, 45, 45, 45, 46, 46, 0, 0, 49, 50, 50, 50, 50, 39, 39, 0, 45, 45, 46, 46, 0, 0, 0, 0, 0, 0, 39, 39, 39, 0, 0, 0, 39, 39, 39, 0, 49, 49, 49, 49, 50, 0, 0, 0, 25, 25, 25, 0, 0, 0, 11, 11, 11, 11, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 39, 39, 0, 0, 0, 0, 0, 5, 5, 7, 0, 0, 0, 49, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0, 0, 0, 49, 49, 0, 0, 49, 49, 49, 50, 50, 50, 50, 50, 50, 50, 50, 49, 49, 49, 39, 39, 39, 40, 40, 40, 40, 40, 7, 7, 7, 0, 0, 0, 0, 0, 45, 45, 45, 45, 46, 46, 46, 46, 0, 0, 0, 39, 39, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 39, 0, 39, 39, 39, 40, 40, 40, 40, 0, 0, 0, 39, 39, 39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 12, 12, 0, 0, 0, 0, 0, 11, 12, 12, 12, 17, 39, 39, 39, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 39, 39, 39, 0, 0, 7, 7, 0, 7, 7, 0, 7, 7, 7, 0, 7, 7, 7, 0, 7, 0, 7, 7, 0, 7, 0, 7, 0, 7, 7, 0, 7, 7, 7, 0, 7, 7, 0, 7, 7, 7, 0, 7, 7, 0, 7, 7, 7, 8, 8, 8, 8, 0, 7, 7, 7, 8, 8, 8, 0, -100], [-100, 39, 0, 0, 49, 50, 49, 50, 0, 0, 0, 39, 17, 1, 1, 1, 1, 45, 45, 46, 46, 46, 0, 45, 45, 46, 46, 46, 46, 0, 0, 0, 39, 40, 40, 40, 40, 40, 40, 40, 0, 0, 0, 0, 39, 0, 0, 0, 11, 12, 0, 0, 45, 45, 45, 0, 49, 50, 0, 0, 0, 0, 39, 39, 39, 0, 45, 45, 45, 0, 0, 0, 0, 0, 0, 0, 39, 0, 39, 39, 39, 0, 0, 0, 0, 0, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 0, 43, 43, 43, 43, 0, 0, 11, 12, 12, 0, 0, 0, 39, 39, 39, 45, 45, 46, 46, 46, 0, 11, 12, 0, 0, 49, 50, 0, 0, 11, 12, 12, 0, 0, 0, 0, 0, 45, 45, 45, 0, 11, 12, 12, 12, 0, 0, 39, 39, 40, 40, 40, 17, 39, 39, 39, 0, 0, 0, 39, 40, 40, 40, 40, 40, 40, 40, 0, 0, 0, 49, 50, 49, 50, 50, 39, 0, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 39, 45, 45, 46, 46, 46, 0, 45, 46, 46, 46, 46, 0, 0, 0, 49, 50, 0, 0, 49, 50, 49, 50, 50, 39, 0, 45, 45, 45, 46, 0, 45, 45, 45, 46, 46, 0, 0, 0, 17, 18, 11, 12, 12, 0, 0, 0, 0, 0, 0, 0, 49, 49, 49, 0, 0, 0, 0, 0, 0, 0, 0, 39, 0, 0, 0, 0, 0, 0, 29, 29, 0, 0, 0, 0, 11, 12, 12, 0, 0, 0, 0, 0, 0, 35, 36, 0, 0, -100]]}"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 281
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:27.747903Z",
     "start_time": "2024-04-10T09:22:27.589394Z"
    }
   },
   "source": "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)",
   "outputs": [],
   "execution_count": 282
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voWiw8C7IrJV"
   },
   "source": [
    "Even better, the results are automatically cached by the ü§ó Datasets library to avoid spending time on this step the next time you run your notebook. The ü§ó Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. ü§ó Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n",
    "\n",
    "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our tasks are about token classification, we use the `AutoModelForTokenClassification` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us. The only thing we have to specify is the number of labels for our problem (which we can get from the features, as seen before):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:34.403889Z",
     "start_time": "2024-04-10T09:22:29.895825Z"
    }
   },
   "source": [
    "from transformers import BertForTokenClassification, AutoConfig, TrainingArguments, Trainer\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "\n",
    "id2label = {y: x for x, y in label2id.items()}\n",
    "config.label2id = label2id\n",
    "config.id2label = id2label\n",
    "config._num_labels = len(label2id)\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(model_checkpoint, config=config)"
   ],
   "execution_count": 283,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T09:22:57.211282Z",
     "start_time": "2024-04-10T09:22:57.204438Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(120138, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=59, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 284
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "The warning is telling us we are throwing away some weights (the `vocab_transform` and `vocab_layer_norm` layers) and randomly initializing some other (the `pre_classifier` and `classifier` layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "To instantiate a `Trainer`, we will need to define three more things. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bliy8zgjIrJY",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:23:00.599074Z",
     "start_time": "2024-04-10T09:23:00.581457Z"
    }
   },
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 285
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km3pGVdTIrJc"
   },
   "source": [
    "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay.\n",
    "\n",
    "The last argument to setup everything so we can push the model to the [Hub](https://huggingface.co/models) regularly during training. Remove it if you didn't follow the installation steps at the top of the notebook. If you want to save your model locally in a name that is different than the name of the repository it will be pushed, or if you want to push your model under an organization and not your name space, use the `hub_model_id` argument to set the repo name (it needs to be the full name, including your namespace: for instance `\"sgugger/bert-finetuned-ner\"` or `\"huggingface/bert-finetuned-ner\"`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3GyIIRKYoUv"
   },
   "source": [
    "Then we will need a data collator that will batch our processed examples together while applying padding to make them all the same size (each pad will be padded to the length of its longest example). There is a data collator for this task in the Transformers library, that not only pads the inputs, but also the labels:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Yo1EYOh8YoUv",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:23:02.857168Z",
     "start_time": "2024-04-10T09:23:02.851140Z"
    }
   },
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ],
   "outputs": [],
   "execution_count": 286
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgmxomZ0YoUv"
   },
   "source": [
    "The last thing to define for our `Trainer` is how to compute the metrics from the predictions. Here we will load the [`seqeval`](https://github.com/chakki-works/seqeval) metric (which is commonly used to evaluate results on the CONLL dataset) via the Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GZnZeIhlYoUv",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:23:06.445316Z",
     "start_time": "2024-04-10T09:23:05.164711Z"
    }
   },
   "source": [
    "metric = load_metric(\"seqeval\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 287
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqQx07N4YoUv"
   },
   "source": [
    "This metric takes list of labels for the predictions and references:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G74QYW2sYoUv",
    "outputId": "ceec1723-36c2-42be-fcf8-6e99c73ad48b",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:23:10.564881Z",
     "start_time": "2024-04-10T09:23:10.512455Z"
    }
   },
   "source": [
    "labels = [i for i in example[f\"{task}_tags\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AGE': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'DATE': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 7},\n",
       " 'EVENT': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 3},\n",
       " 'LOCATION': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'NUMBER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 3},\n",
       " 'ORGANIZATION': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 16},\n",
       " 'PERCENT': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'PERSON': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 10},\n",
       " 'PROFESSION': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 10},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 288
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sZOdRlRIrJd"
   },
   "source": [
    "So we will need to do a bit of post-processing on our predictions:\n",
    "- select the predicted index (with the maximum logit) for each token\n",
    "- convert it to its string label\n",
    "- ignore everywhere we set a label of -100\n",
    "\n",
    "The following function does all this post-processing on the result of `Trainer.evaluate` (which is a namedtuple containing predictions and labels) before applying the metric:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UmvbnJ9JIrJd",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:23:11.728400Z",
     "start_time": "2024-04-10T09:23:11.723176Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 289
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "Note that we drop the precision/recall/f1 computed for each category and only focus on the overall precision/recall/f1/accuracy.\n",
    "\n",
    "Then we just need to pass all of this along with our datasets to the `Trainer`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "imY1oC3SIrJf",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:23:16.145425Z",
     "start_time": "2024-04-10T09:23:13.285226Z"
    }
   },
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaystepanov/PycharmProjects/NLP/venv/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 290
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdzABDVcIrJg"
   },
   "source": [
    "We can now finetune our model by just calling the `train` method:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T09:24:13.364444Z",
     "start_time": "2024-04-10T09:24:13.348034Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(120138, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=59, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 291
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sxTtB9CIYoUw",
    "outputId": "f3f3d138-c5d7-4275-8533-5eeef7bb4286",
    "ExecuteTime": {
     "end_time": "2024-04-10T09:24:15.076584Z",
     "start_time": "2024-04-10T09:24:13.953813Z"
    }
   },
   "source": [
    "trainer.train()"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1480) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[292], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/venv/lib/python3.9/site-packages/transformers/trainer.py:1615\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1612\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1613\u001B[0m     \u001B[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001B[39;00m\n\u001B[1;32m   1614\u001B[0m     hf_hub_utils\u001B[38;5;241m.\u001B[39mdisable_progress_bars()\n\u001B[0;32m-> 1615\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1616\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1617\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1618\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1619\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1620\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1621\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1622\u001B[0m     hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/venv/lib/python3.9/site-packages/transformers/trainer.py:1961\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   1958\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[1;32m   1960\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[0;32m-> 1961\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1963\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1964\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   1965\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_tpu_available()\n\u001B[1;32m   1966\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   1967\u001B[0m ):\n\u001B[1;32m   1968\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   1969\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/venv/lib/python3.9/site-packages/transformers/trainer.py:2902\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   2899\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   2901\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 2902\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2904\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mn_gpu \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   2905\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()  \u001B[38;5;66;03m# mean() to average on multi-gpu parallel training\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/venv/lib/python3.9/site-packages/transformers/trainer.py:2925\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m   2923\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2924\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2925\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2926\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   2927\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   2928\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1758\u001B[0m, in \u001B[0;36mBertForTokenClassification.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1752\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[1;32m   1754\u001B[0m \u001B[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001B[39;00m\n\u001B[1;32m   1755\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1756\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1758\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1759\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1760\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1761\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1762\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1763\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1764\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1765\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1766\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1767\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1768\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1770\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1772\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(sequence_output)\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1006\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    999\u001B[0m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[1;32m   1000\u001B[0m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[1;32m   1001\u001B[0m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[1;32m   1002\u001B[0m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[1;32m   1003\u001B[0m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[1;32m   1004\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[0;32m-> 1006\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1007\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1008\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1009\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1010\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1011\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1012\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1013\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\n\u001B[1;32m   1014\u001B[0m     embedding_output,\n\u001B[1;32m   1015\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mextended_attention_mask,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1023\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[1;32m   1024\u001B[0m )\n\u001B[1;32m   1025\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/NLP/venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:238\u001B[0m, in \u001B[0;36mBertEmbeddings.forward\u001B[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001B[0m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embedding_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabsolute\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    237\u001B[0m     position_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embeddings(position_ids)\n\u001B[0;32m--> 238\u001B[0m     embeddings \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m position_embeddings\n\u001B[1;32m    239\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLayerNorm(embeddings)\n\u001B[1;32m    240\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(embeddings)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (1480) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "execution_count": 292
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKASz-2vIrJi"
   },
   "source": [
    "The `evaluate` method allows you to evaluate again on the evaluation dataset or on another dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOUcBkX8IrJi",
    "outputId": "de5b9dd6-9dc0-4702-cb43-55e9829fde25"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='408' max='204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [204/204 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.05934586375951767,\n",
       " 'eval_precision': 0.9292672127518264,\n",
       " 'eval_recall': 0.9391430808815304,\n",
       " 'eval_f1': 0.9341790463472988,\n",
       " 'eval_accuracy': 0.9842565968195466,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xV_q3s1tYoUw"
   },
   "source": [
    "To get the precision/recall/f1 computed for each category now that we have finished training, we can apply the same function as before on the result of the `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLltoXusYoUw",
    "outputId": "208bd802-bdf7-4729-e362-9958eb34704d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': {'precision': 0.949718574108818,\n",
       "  'recall': 0.966768525592055,\n",
       "  'f1': 0.9581677077418134,\n",
       "  'number': 2618},\n",
       " 'MISC': {'precision': 0.8132387706855791,\n",
       "  'recall': 0.8383428107229894,\n",
       "  'f1': 0.8255999999999999,\n",
       "  'number': 1231},\n",
       " 'ORG': {'precision': 0.9055232558139535,\n",
       "  'recall': 0.9090466926070039,\n",
       "  'f1': 0.9072815533980583,\n",
       "  'number': 2056},\n",
       " 'PER': {'precision': 0.9759552042160737,\n",
       "  'recall': 0.9765985497692815,\n",
       "  'f1': 0.9762767710049424,\n",
       "  'number': 3034},\n",
       " 'overall_precision': 0.9292672127518264,\n",
       " 'overall_recall': 0.9391430808815304,\n",
       " 'overall_f1': 0.9341790463472988,\n",
       " 'overall_accuracy': 0.9842565968195466}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLfJSn-KYoUw"
   },
   "source": [
    "You can now upload the result of the training to the Hub, just execute this instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWwDBfzNYoUw"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otWUEziQYoUw"
   },
   "source": [
    "You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"sgugger/my-awesome-model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XiRmYXjEYoUw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Token Classification",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
